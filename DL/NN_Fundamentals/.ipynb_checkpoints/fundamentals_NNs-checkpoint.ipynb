{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "EPSILON = 1e-8 # small constant to avoid underflow or divide per 0\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I - Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This time, the data will correspond to greyscale images. <br> Two different datasets can be used here:\n",
    "- The MNIST dataset, small 8*8 images, corresponding to handwritten digits &rightarrow; 10 classes\n",
    "- The Fashion MNIST dataset, medium 28*28 images, corresponding to clothes pictures &rightarrow; 10 classes\n",
    "\n",
    "#### Starting with the simple MNIST is recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"MNIST\"\n",
    "# dataset = \"FASHION_MNIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset='MNIST'):\n",
    "    if dataset == 'MNIST':\n",
    "        digits = load_digits()\n",
    "        X, Y = np.asarray(digits['data'], dtype='float32'), np.asarray(digits['target'], dtype='int32')\n",
    "        return X, Y\n",
    "    elif dataset == 'FASHION_MNIST':\n",
    "        import tensorflow as tf\n",
    "        fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "        (X, Y), (_, _) = fashion_mnist.load_data()\n",
    "        X = X.reshape((X.shape[0], X.shape[1] * X.shape[2]))\n",
    "        X, Y = np.asarray(X, dtype='float32'), np.asarray(Y, dtype='int32')\n",
    "        return X, Y\n",
    "X, Y = load_data(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(np.unique(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 1797\n",
      "Input dimension: 64\n",
      "Number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples: {:d}'.format(X.shape[0]))\n",
    "print('Input dimension: {:d}'.format(X.shape[1]))  # images 8x8 or 28*28 actually\n",
    "print('Number of classes: {:d}'.format(n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range max-min of greyscale pixel values: (16.0, 0.0)\n",
      "First image sample:\n",
      "[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.\n",
      " 15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.\n",
      "  0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.\n",
      "  0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]\n",
      "First image label: 0\n",
      "Input design matrix shape: (1797, 64)\n"
     ]
    }
   ],
   "source": [
    "print(\"Range max-min of greyscale pixel values: ({0:.1f}, {1:.1f})\".format(np.max(X), np.min(X)))\n",
    "print(\"First image sample:\\n{0}\".format(X[0]))\n",
    "print(\"First image label: {0}\".format(Y[0]))\n",
    "print(\"Input design matrix shape: {0}\".format(X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does the data look like?\n",
    "Each image in the dataset consists of a 8 x 8 (or 28 x 28) matrix, of greyscale pixels. For the MNIST dataset, the values are between 0 and 16 where 0 represents white, 16 represents black and there are many shades of grey in-between. For the Fashion MNIST dataset, the values are between 0 and 255.<br>Each image is assigned a corresponding numerical label, so the image in ```X[i]``` has its corresponding label stored in ```Y[i]```.\n",
    "\n",
    "The next cells below demonstrate how to visualise the input data. Make sure you understand what's happening, particularly how the indices correspond to individual items in the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data_sample(X, Y, nrows=2, ncols=2):\n",
    "    fig, ax = plt.subplots(nrows, ncols)\n",
    "    for row in ax:\n",
    "        for col in row:\n",
    "            index = random.randint(0, X.shape[0])\n",
    "            dim = np.sqrt(X.shape[1]).astype(int)\n",
    "            col.imshow(X[index].reshape((dim, dim)), cmap=plt.cm.gray_r)\n",
    "            col.set_title(\"image label: %d\" % Y[index])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_data_sample(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â II - Multiclass classification MLP with Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II a) - Problem definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/mlp_mnist.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task here will be to implement \"from scratch\" a Multilayer Perceptron for classification.\n",
    "\n",
    "We will define the formal categorical cross entropy loss as follows:\n",
    "$$\n",
    "l(\\mathbf{\\Theta}, \\mathbf{X}, \\mathbf{Y}) = - \\frac{1}{n} \\sum_{i=1}^n \\log \\mathbf{f}(\\mathbf{x}_i ; \\mathbf{\\Theta})^\\top y_i\n",
    "$$\n",
    "<center>with $y_i$ being the one-hot encoded true label for the sample $i$, and $\\Theta = (\\mathbf{W}^h; \\mathbf{b}^h; \\mathbf{W}^o; \\mathbf{b}^o)$</center>\n",
    "<center>In addition, $\\mathbf{f}(\\mathbf{x}) = softmax(\\mathbf{z^o}(\\mathbf{x})) = softmax(\\mathbf{W}^o\\mathbf{h}(\\mathbf{x}) + \\mathbf{b}^o)$</center>\n",
    "<center>and $\\mathbf{h}(\\mathbf{x}) = g(\\mathbf{z^h}(\\mathbf{x})) = g(\\mathbf{W}^h\\mathbf{x} + \\mathbf{b}^h)$, $g$ being the activation function and could be implemented with $sigmoid$ or $relu$</center>\n",
    "\n",
    "## Objectives:\n",
    "- Write the categorical cross entropy loss function\n",
    "- Write the activation functions with their associated gradient\n",
    "- Write the softmax function that is going to be used to output the predicted probabilities\n",
    "- Implement the forward pass through the neural network\n",
    "- Implement the backpropagation according to the used loss: progagate the gradients using the chain rule and return $(\\mathbf{\\nabla_{W^h}}l ; \\mathbf{\\nabla_{b^h}}l ; \\mathbf{\\nabla_{W^o}}l ; \\mathbf{\\nabla_{b^o}}l)$\n",
    "- Implement dropout regularization in the forward pass: be careful to consider both training and prediction cases\n",
    "- Implement the SGD optimization algorithm, and improve it with simple momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple graph function to let you have a global overview:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/function_graph.png\" style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) You may find numpy outer products useful: <br>\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.outer.html <br>\n",
    "We have: $outer(u, v) = u \\cdot v^T$, with $u, v$ two vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, v = np.random.normal(size=(5,)), np.random.normal(size=(10,))\n",
    "assert np.array_equal(\n",
    "    np.outer(u, v),\n",
    "    np.dot(np.reshape(u, (u.size, 1)), np.reshape(v, (1, v.size)))\n",
    ")\n",
    "assert np.outer(u, v).shape == (5, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) You also may find numpy matmul function useful: <br>\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.matmul.html <br>\n",
    "It can be used to perform matrix products along one fixed dimension (i.e. the batch size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, B = np.random.randint(0, 100, size=(64, 5, 10)), np.random.randint(0, 100, size=(64, 10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.array_equal(\n",
    "    np.stack([np.dot(A_i, B_i) for A_i, B_i in zip(A, B)]),\n",
    "    np.matmul(A, B)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â II b) - Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron():\n",
    "    \"\"\"MLP with one hidden layer having a hidden activation,\n",
    "    and one output layer having a softmax activation\"\"\"\n",
    "    def __init__(self, X, Y, hidden_size, activation='relu',\n",
    "                 initialization='uniform', dropout=False, dropout_rate=1):\n",
    "        # input, hidden, and output dimensions on the MLP based on X, Y\n",
    "        self.input_size, self.output_size = X.shape[1], len(np.unique(Y))\n",
    "        self.hidden_size = hidden_size\n",
    "        # initialization strategies: avoid a full-0 initialization of the weight matricest\n",
    "        if initialization == 'uniform':\n",
    "            self.W_h = np.random.uniform(size=(self.hidden_size, self.input_size), high=0.01, low=-0.01)\n",
    "            self.W_o = np.random.uniform(size=(self.output_size, self.hidden_size), high=0.01, low=-0.01)\n",
    "        elif initialization == 'normal':\n",
    "            self.W_h = np.random.normal(size=(self.hidden_size, self.input_size), loc=0, scale=0.01)\n",
    "            self.W_o = np.random.normal(size=(self.output_size, self.hidden_size), loc=0, scale=0.01)\n",
    "        # the bias could be initializated to 0 or a random low constant\n",
    "        self.b_h = np.zeros(self.hidden_size)\n",
    "        self.b_o = np.zeros(self.output_size)\n",
    "        # our namedtuple structure of gradients\n",
    "        self.Grads = namedtuple('Grads', ['W_h', 'b_h', 'W_o', 'b_o'])\n",
    "        # and the velocities associated which are going to be useful for the momentum\n",
    "        self.velocities = {'W_h': 0., 'b_h': 0., 'W_o': 0., 'b_o': 0.}\n",
    "        # the hidden activation function used\n",
    "        self.activation = activation\n",
    "        # arrays to track back the losses and accuracies evolution\n",
    "        self.training_losses_history = []\n",
    "        self.validation_losses_history = []\n",
    "        self.training_acc_history = []\n",
    "        self.validation_acc_history = []\n",
    "        # train val split and normalization of the features\n",
    "        self.X_tr, self.X_val, self.Y_tr, self.Y_val = self.split_train_validation(X, Y)\n",
    "        self.scaler = MinMaxScaler(feature_range=(0, 1), copy=False)\n",
    "        self.X_tr = self.scaler.fit_transform(self.X_tr)\n",
    "        self.X_val = self.scaler.transform(self.X_val)\n",
    "        # dropout parameters\n",
    "        self.dropout = dropout\n",
    "        self.dropout_rate = dropout_rate\n",
    "        # step used for the optimization algorithm and setted later (learning rate)\n",
    "        self.step = 0\n",
    "    \n",
    "    # One-hot encoding of the target\n",
    "    # Transform the integer represensation to a sparse one\n",
    "    @staticmethod\n",
    "    def one_hot(n_classes, Y):\n",
    "        return np.eye(n_classes)[Y]\n",
    "    \n",
    "    # Reverse one-hot encoding of the target\n",
    "    # Recover the former integer representation\n",
    "    # ex: from (0,0,1,0) to 2\n",
    "    @staticmethod\n",
    "    def reverse_one_hot(Y_one_hot):\n",
    "        return np.asarray(np.where(Y_one_hot==1)[1], dtype='int32')\n",
    "    \n",
    "    \"\"\"\n",
    "    Activation functions and their gradient\n",
    "    \"\"\"\n",
    "    # In implementations below X is a matrix of shape (n_samples, p)\n",
    "    \n",
    "    # A max_value value is indicated for the relu and grad_relu functions\n",
    "    # Make sure to clip the output to it to prevent numerical overflow (exploding gradient)\n",
    "    # Make it so the max value reachable is max_value\n",
    "    @staticmethod\n",
    "    def relu(X, max_value=20):\n",
    "        assert max_value > 0\n",
    "        # TODO:\n",
    "        return np.clip(X,0,max_value)\n",
    "    \n",
    "    # Make it so the gradient becomes 0 when X becomes greater than max_value\n",
    "    @staticmethod\n",
    "    def grad_relu(X, max_value=20):\n",
    "        assert max_value > 0\n",
    "        # TODO:\n",
    "        grad_logic = (X >= 0)*(X < max_value)\n",
    "        return grad_logic.astype(int)\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(X):\n",
    "        # TODO:\n",
    "        return 1/(1+np.exp(-X))\n",
    "    \n",
    "    @staticmethod\n",
    "    def grad_sigmoid(X):\n",
    "        # TODO:\n",
    "        return self.sigmoid(X)*(1-self.sigmoid(X))\n",
    "    \n",
    "    # Softmax function to output probabilities\n",
    "    @staticmethod\n",
    "    def softmax(X):\n",
    "        # TODO:\n",
    "        return np.exp(X)/np.sum(np.exp(X))\n",
    "    \n",
    "    # Loss function\n",
    "    # Consider using EPSILON to prevent numerical issues (log(0) is undefined)\n",
    "    # Y_true and Y_pred are of shape (n_samples,n_classes)\n",
    "    @staticmethod\n",
    "    def categorical_cross_entropy(Y_true, Y_pred):\n",
    "        # TODO:\n",
    "        #n = Y_true.shape[0]\n",
    "        return -np.mean(np.log(Y_pred)*Y_true)\n",
    "    \n",
    "    @staticmethod\n",
    "    def split_train_validation(X, Y, test_size=0.25, seed=False):\n",
    "        random_state = 42 if seed else np.random.randint(1e3)\n",
    "        X_tr, X_val, Y_tr, Y_val = train_test_split(X, Y, test_size=test_size, random_state=random_state)\n",
    "        return X_tr, X_val, Y_tr, Y_val\n",
    "    \n",
    "    # Sample random batch in (X, Y) with a given batch_size for SGD\n",
    "    @staticmethod\n",
    "    def get_random_batch(X, Y, batch_size):\n",
    "        indexes = np.random.choice(X.shape[0], size=batch_size, replace=False)\n",
    "        return X[indexes], Y[indexes]\n",
    "        \n",
    "    # Forward pass: compute f(x) as y, and return optionally the hidden states h(x) and z_h(x) for compute_grads\n",
    "    def forward(self, X, return_activation=False, training=False):\n",
    "        if self.activation == 'relu':\n",
    "            g_activation = self.relu\n",
    "        elif self.activation == 'sigmoid':\n",
    "            g_activation = self.sigmoid\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "            \n",
    "        #z_h = np.zeros((X.shape[0], self.hidden_size)) if len(X.shape) > 1 else np.zeros(self.hidden_size)\n",
    "        #h = np.zeros((X.shape[0], self.hidden_size)) if len(X.shape) > 1 else np.zeros(self.hidden_size)\n",
    "        # TODO:\n",
    "        #print(\"Parameters shape : \", self.W_h.shape, X.shape, self.b_h.shape)\n",
    "        \n",
    "        \n",
    "        z_h = np.dot(self.W_h,X.T).T + self.b_h\n",
    "        #print(\"z_h forward shape : \",z_h.shape)\n",
    "        h = g_activation(z_h)\n",
    "        #print(\"H forward shape : \", h.shape)\n",
    "        if self.dropout:\n",
    "            if training:\n",
    "                # TODO:\n",
    "                #pass\n",
    "                dropout_vec = np.random.binomial(1,1-self.dropout_rate,h.shape)\n",
    "                h = h*dropout_vec\n",
    "            else:\n",
    "                # TODO:\n",
    "                #pass\n",
    "                h = h*(self.dropout_rate)\n",
    "        # TODO:\n",
    "        #y = np.zeros((X.shape[0], self.output_size)) if len(X.shape) > 1 else np.zeros(self.output_size)        \n",
    "        z_o = np.dot(self.W_o,h.T).T + self.b_o\n",
    "        #print(\"Z_o forward shape\",z_o.shape)\n",
    "        y = self.softmax(z_o)\n",
    "        #print(\"f(x) forward shape : \",y.shape)\n",
    "        if return_activation:\n",
    "            return y, h, z_h\n",
    "        else:\n",
    "            return y\n",
    "    \n",
    "    # Backpropagation: return an instantiation of self.Grads that contains the average gradients for the given batch\n",
    "    def compute_grads(self, X, Y_true, vectorized=False):\n",
    "        if self.activation == 'relu':\n",
    "            g_grad = self.grad_relu\n",
    "        elif self.activation == 'sigmoid':\n",
    "            g_grad = self.grad_sigmoid\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        if len(X.shape) == 1:\n",
    "            X = X.reshape((1,) + X.shape)\n",
    "        \n",
    "        if not vectorized:\n",
    "            n = X.shape[0]\n",
    "            grad_W_h = np.zeros((self.hidden_size, self.input_size))\n",
    "            grad_b_h = np.zeros((self.hidden_size, )) \n",
    "            grad_W_o = np.zeros((self.output_size, self.hidden_size))\n",
    "            grad_b_o = np.zeros((self.output_size, ))\n",
    "           \n",
    "            for x, y_true in zip(X, Y_true):\n",
    "                y_pred, h, z_h = self.forward(x, return_activation=True, training=True)\n",
    "                #compute activation gradients\n",
    "                grad_z_o = y_pred - self.one_hot(self.output_size, y_true)\n",
    "                #print(\"Grad z_o shape : \", grad_z_o.shape)\n",
    "                #computer last layer parameters gradients \n",
    "                grad_W_o += np.outer(grad_z_o, h.T)\n",
    "              \n",
    "                #print(\"Grad W_o shape : \",grad_W_o.shape)\n",
    "                grad_b_o += grad_z_o\n",
    "                #print(\"Grad b_o shape : \",grad_b_o.shape)\n",
    "                #compute first layer activation gradients\n",
    "                grad_h = np.dot(self.W_o.T,grad_z_o)\n",
    "                #print(\"Grad h shape : \",grad_h.shape)\n",
    "\n",
    "                grad_z_h = np.multiply(grad_h,g_grad(z_h))\n",
    "                #print(\"Grad z_h shape : \",grad_z_h.shape)\n",
    "\n",
    "                #compute first layer parameters gradients\n",
    "                grad_W_h += np.outer(grad_z_h,x.T)\n",
    "                #print(\"Grad W_h shape : \",grad_W_h.shape)\n",
    "                grad_b_h += grad_z_h\n",
    "                #print(\"Grad b_h shape : \",grad_b_h.shape)\n",
    "                # TODO:\n",
    "                \n",
    "            grads = self.Grads(grad_W_h/n, grad_b_h/n, grad_W_o/n, grad_b_o/n)\n",
    "            \n",
    "        else: \n",
    "            Y_pred, h, z_h = self.forward(X, return_activation=True, training=True)\n",
    "\n",
    "            # TODO (optional), try to do the backprop without Python loops in a vectorized way:\n",
    "            \n",
    "            grad_W_h = np.zeros((X.shape[0], self.hidden_size, self.input_size))\n",
    "            grad_b_h = np.zeros((X.shape[0], self.hidden_size, )) \n",
    "            grad_W_o = np.zeros((X.shape[0], self.output_size, self.hidden_size))\n",
    "            grad_b_o = np.zeros((X.shape[0], self.output_size, ))\n",
    "            \n",
    "            grads = self.Grads(\n",
    "                np.mean(grad_W_h, axis=0), \n",
    "                np.mean(grad_b_h, axis=0), \n",
    "                np.mean(grad_W_o, axis=0), \n",
    "                np.mean(grad_b_o, axis=0)\n",
    "            )\n",
    "            \n",
    "        return grads\n",
    "    \n",
    "    # Perform the update of the parameters (W_h, b_h, W_o, b_o) based of their gradient\n",
    "    def optimizer_step(self, optimizer='gd', momentum=False, momentum_alpha=0.9, \n",
    "                       batch_size=None, vectorized=True):\n",
    "        if optimizer == 'gd':\n",
    "            grads = self.compute_grads(self.X_tr, self.Y_tr, vectorized=vectorized)\n",
    "        elif optimizer == 'sgd':\n",
    "            batch_X_tr, batch_Y_tr = self.get_random_batch(self.X_tr, self.Y_tr, batch_size)\n",
    "            grads = self.compute_grads(batch_X_tr, batch_Y_tr, vectorized=vectorized)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        if not momentum:\n",
    "            # TODO:\n",
    "            #print(\"Shape step and shape grad W_h\",self.step,grads.W_h.shape)\n",
    "            self.W_h = self.W_h - self.step * grads.W_h\n",
    "            self.b_h = self.b_h - self.step * grads.b_h\n",
    "            self.W_o = self.W_o - self.step * grads.W_o\n",
    "            self.b_o = self.b_o - self.step * grads.b_o\n",
    "        else:\n",
    "            # remember: use the stored velocities\n",
    "            # TODO:\n",
    "            # compute velocities update for momentum\n",
    "            self.velocities['W_h'] = momentum_alpha * self.velocities['W_h'] - self.step * grads.W_h\n",
    "            self.velocities['W_o'] = momentum_alpha * self.velocities['W_o'] - self.step * grads.W_o\n",
    "            self.velocities['b_h'] = momentum_alpha * self.velocities['b_h'] - self.step * grads.b_h\n",
    "            self.velocities['b_o'] = momentum_alpha * self.velocities['b_o'] - self.step * grads.b_o\n",
    "            # update parameters\n",
    "            self.W_h += self.velocities['W_h']\n",
    "            self.b_h += self.velocities['b_h']\n",
    "            self.W_o += self.velocities['W_o']\n",
    "            self.b_o += self.velocities['b_o']\n",
    "            \n",
    "            \n",
    "            pass\n",
    "    \n",
    "    # Loss wrapper\n",
    "    def loss(self, Y_true, Y_pred):\n",
    "        return self.categorical_cross_entropy(self.one_hot(self.output_size, Y_true), Y_pred)\n",
    "    \n",
    "    def loss_history_flush(self):\n",
    "        self.training_losses_history = []\n",
    "        self.validation_losses_history = []\n",
    "        \n",
    "    # Main function that trains the MLP with a design matrix X and a target vector Y\n",
    "    def train(self, optimizer='sgd', momentum=False, min_iterations=500, max_iterations=5000, initial_step=1e-1,\n",
    "              batch_size=64, early_stopping=True, early_stopping_lookbehind=100, early_stopping_delta=1e-4, \n",
    "              vectorized=False, flush_history=True, verbose=True):\n",
    "        if flush_history:\n",
    "            self.loss_history_flush()\n",
    "        cpt_patience, best_validation_loss = 0, np.inf\n",
    "        iteration_number = 0\n",
    "        self.step = initial_step\n",
    "        while len(self.training_losses_history) < max_iterations:\n",
    "            iteration_number += 1\n",
    "            self.optimizer_step(\n",
    "                optimizer=optimizer, momentum=momentum, batch_size=batch_size, vectorized=vectorized\n",
    "            )\n",
    "            \n",
    "            training_loss = self.loss(self.Y_tr, self.forward(self.X_tr))\n",
    "            self.training_losses_history.append(training_loss)\n",
    "            training_accuracy = self.accuracy_on_train()\n",
    "            self.training_acc_history.append(training_accuracy)\n",
    "            validation_loss = self.loss(self.Y_val, self.forward(self.X_val))\n",
    "            self.validation_losses_history.append(validation_loss)\n",
    "            validation_accuracy = self.accuracy_on_validation()\n",
    "            self.validation_acc_history.append(validation_accuracy)\n",
    "            if iteration_number > min_iterations and early_stopping:\n",
    "                if validation_loss + early_stopping_delta < best_validation_loss:\n",
    "                    best_validation_loss = validation_loss\n",
    "                    cpt_patience = 0\n",
    "                else:\n",
    "                    cpt_patience += 1\n",
    "            if verbose:\n",
    "                msg = \"iteration number: {0}\\t training loss: {1:.4f}\\t\" + \\\n",
    "                \"validation loss: {2:.4f}\\t validation accuracy: {3:.4f}\"\n",
    "                print(msg.format(iteration_number, \n",
    "                                 training_loss, \n",
    "                                 validation_loss,\n",
    "                                 validation_accuracy))\n",
    "            if cpt_patience >= early_stopping_lookbehind:\n",
    "                break\n",
    "    \n",
    "    # Return the predicted class once the MLP has been trained\n",
    "    def predict(self, X, normalize=True):\n",
    "        if normalize:\n",
    "            X = self.scaler.transform(X)\n",
    "        if len(X.shape) == 1:\n",
    "            return np.argmax(self.forward(X))\n",
    "        else:\n",
    "            return np.argmax(self.forward(X), axis=1)\n",
    "        \n",
    "    \"\"\"\n",
    "    Metrics and plots\n",
    "    \"\"\"\n",
    "    def accuracy_on_train(self):\n",
    "        return (self.predict(self.X_tr, normalize=False) == self.Y_tr).mean()\n",
    "\n",
    "    def accuracy_on_validation(self):\n",
    "        return (self.predict(self.X_val, normalize=False) == self.Y_val).mean()\n",
    "\n",
    "    def plot_loss_history(self, add_to_title=None):\n",
    "        import warnings\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.plot(range(len(self.training_losses_history)), \n",
    "                 self.training_losses_history, label='Training loss evolution')\n",
    "        plt.plot(range(len(self.validation_losses_history)), \n",
    "                 self.validation_losses_history, label='Validation loss evolution')\n",
    "        plt.legend(fontsize=15)\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel(\"iteration number\", fontsize=15)\n",
    "        plt.ylabel(\"Cross entropy loss\", fontsize=15)\n",
    "        base_title = \"Cross entropy loss evolution during training\"\n",
    "        if not self.dropout:\n",
    "            base_title += \", no dropout penalization\"\n",
    "        else:\n",
    "            base_title += \", {:.1f} dropout penalization\"\n",
    "            base_title = base_title.format(self.dropout_rate)\n",
    "        title = base_title + \", \" + add_to_title if add_to_title else base_title\n",
    "        plt.title(title, fontsize=20)\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_validation_prediction(self, sample_id):\n",
    "        fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "        classes = np.unique(self.Y_tr)\n",
    "        dim = np.sqrt(self.X_val.shape[1]).astype(int)\n",
    "        ax0.imshow(self.scaler.inverse_transform([self.X_val[sample_id]]).reshape(dim, dim), cmap=plt.cm.gray_r,\n",
    "                   interpolation='nearest')\n",
    "        ax0.set_title(\"True image label: %d\" % self.Y_val[sample_id]);\n",
    "\n",
    "        ax1.bar(classes, self.one_hot(len(classes), self.Y_val[sample_id]), label='true')\n",
    "        ax1.bar(classes, self.forward(self.X_val[sample_id]), label='prediction', color=\"red\")\n",
    "        ax1.set_xticks(classes)\n",
    "        prediction = self.predict(self.X_val[sample_id], normalize=False)\n",
    "        ax1.set_title('Output probabilities (prediction: %d)' % prediction)\n",
    "        ax1.set_xlabel('Digit class')\n",
    "        ax1.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1\t training loss: 0.9508\tvalidation loss: 0.8412\t validation accuracy: 0.0978\n",
      "iteration number: 2\t training loss: 0.9508\tvalidation loss: 0.8412\t validation accuracy: 0.0978\n",
      "iteration number: 3\t training loss: 0.9508\tvalidation loss: 0.8412\t validation accuracy: 0.0978\n",
      "iteration number: 4\t training loss: 0.9508\tvalidation loss: 0.8412\t validation accuracy: 0.1000\n",
      "iteration number: 5\t training loss: 0.9508\tvalidation loss: 0.8412\t validation accuracy: 0.1000\n",
      "iteration number: 6\t training loss: 0.9508\tvalidation loss: 0.8412\t validation accuracy: 0.0867\n",
      "iteration number: 7\t training loss: 0.9508\tvalidation loss: 0.8412\t validation accuracy: 0.0867\n",
      "iteration number: 8\t training loss: 0.9508\tvalidation loss: 0.8412\t validation accuracy: 0.1022\n",
      "iteration number: 9\t training loss: 0.9508\tvalidation loss: 0.8412\t validation accuracy: 0.1756\n",
      "iteration number: 10\t training loss: 0.9508\tvalidation loss: 0.8412\t validation accuracy: 0.0867\n",
      "iteration number: 11\t training loss: 0.9508\tvalidation loss: 0.8412\t validation accuracy: 0.1422\n",
      "iteration number: 12\t training loss: 0.9508\tvalidation loss: 0.8412\t validation accuracy: 0.0911\n",
      "iteration number: 13\t training loss: 0.9508\tvalidation loss: 0.8412\t validation accuracy: 0.0911\n",
      "iteration number: 14\t training loss: 0.9508\tvalidation loss: 0.8412\t validation accuracy: 0.1111\n",
      "iteration number: 15\t training loss: 0.9508\tvalidation loss: 0.8412\t validation accuracy: 0.1089\n",
      "iteration number: 16\t training loss: 0.9508\tvalidation loss: 0.8412\t validation accuracy: 0.0911\n",
      "iteration number: 17\t training loss: 0.9507\tvalidation loss: 0.8412\t validation accuracy: 0.0911\n",
      "iteration number: 18\t training loss: 0.9507\tvalidation loss: 0.8412\t validation accuracy: 0.0933\n",
      "iteration number: 19\t training loss: 0.9507\tvalidation loss: 0.8412\t validation accuracy: 0.1378\n",
      "iteration number: 20\t training loss: 0.9507\tvalidation loss: 0.8412\t validation accuracy: 0.1489\n",
      "iteration number: 21\t training loss: 0.9507\tvalidation loss: 0.8412\t validation accuracy: 0.0867\n",
      "iteration number: 22\t training loss: 0.9507\tvalidation loss: 0.8412\t validation accuracy: 0.0867\n",
      "iteration number: 23\t training loss: 0.9507\tvalidation loss: 0.8413\t validation accuracy: 0.0867\n",
      "iteration number: 24\t training loss: 0.9507\tvalidation loss: 0.8413\t validation accuracy: 0.0867\n",
      "iteration number: 25\t training loss: 0.9507\tvalidation loss: 0.8413\t validation accuracy: 0.0867\n",
      "iteration number: 26\t training loss: 0.9507\tvalidation loss: 0.8413\t validation accuracy: 0.0867\n",
      "iteration number: 27\t training loss: 0.9507\tvalidation loss: 0.8413\t validation accuracy: 0.0867\n",
      "iteration number: 28\t training loss: 0.9507\tvalidation loss: 0.8413\t validation accuracy: 0.0867\n",
      "iteration number: 29\t training loss: 0.9507\tvalidation loss: 0.8413\t validation accuracy: 0.0867\n",
      "iteration number: 30\t training loss: 0.9507\tvalidation loss: 0.8413\t validation accuracy: 0.0867\n",
      "iteration number: 31\t training loss: 0.9507\tvalidation loss: 0.8413\t validation accuracy: 0.0867\n",
      "iteration number: 32\t training loss: 0.9506\tvalidation loss: 0.8413\t validation accuracy: 0.0867\n",
      "iteration number: 33\t training loss: 0.9506\tvalidation loss: 0.8412\t validation accuracy: 0.0867\n",
      "iteration number: 34\t training loss: 0.9506\tvalidation loss: 0.8412\t validation accuracy: 0.0867\n",
      "iteration number: 35\t training loss: 0.9506\tvalidation loss: 0.8412\t validation accuracy: 0.0867\n",
      "iteration number: 36\t training loss: 0.9506\tvalidation loss: 0.8412\t validation accuracy: 0.0867\n",
      "iteration number: 37\t training loss: 0.9506\tvalidation loss: 0.8412\t validation accuracy: 0.0867\n",
      "iteration number: 38\t training loss: 0.9506\tvalidation loss: 0.8413\t validation accuracy: 0.0867\n",
      "iteration number: 39\t training loss: 0.9506\tvalidation loss: 0.8412\t validation accuracy: 0.0867\n",
      "iteration number: 40\t training loss: 0.9506\tvalidation loss: 0.8412\t validation accuracy: 0.0867\n",
      "iteration number: 41\t training loss: 0.9506\tvalidation loss: 0.8412\t validation accuracy: 0.0867\n",
      "iteration number: 42\t training loss: 0.9506\tvalidation loss: 0.8412\t validation accuracy: 0.0867\n",
      "iteration number: 43\t training loss: 0.9506\tvalidation loss: 0.8412\t validation accuracy: 0.0867\n",
      "iteration number: 44\t training loss: 0.9506\tvalidation loss: 0.8412\t validation accuracy: 0.0867\n",
      "iteration number: 45\t training loss: 0.9506\tvalidation loss: 0.8412\t validation accuracy: 0.0867\n",
      "iteration number: 46\t training loss: 0.9506\tvalidation loss: 0.8412\t validation accuracy: 0.0867\n",
      "iteration number: 47\t training loss: 0.9505\tvalidation loss: 0.8412\t validation accuracy: 0.0867\n",
      "iteration number: 48\t training loss: 0.9505\tvalidation loss: 0.8412\t validation accuracy: 0.0867\n",
      "iteration number: 49\t training loss: 0.9505\tvalidation loss: 0.8412\t validation accuracy: 0.0867\n",
      "iteration number: 50\t training loss: 0.9505\tvalidation loss: 0.8412\t validation accuracy: 0.0867\n",
      "iteration number: 51\t training loss: 0.9505\tvalidation loss: 0.8412\t validation accuracy: 0.0867\n",
      "iteration number: 52\t training loss: 0.9505\tvalidation loss: 0.8412\t validation accuracy: 0.0867\n",
      "iteration number: 53\t training loss: 0.9505\tvalidation loss: 0.8412\t validation accuracy: 0.0867\n",
      "iteration number: 54\t training loss: 0.9504\tvalidation loss: 0.8412\t validation accuracy: 0.0867\n",
      "iteration number: 55\t training loss: 0.9504\tvalidation loss: 0.8412\t validation accuracy: 0.0867\n",
      "iteration number: 56\t training loss: 0.9504\tvalidation loss: 0.8412\t validation accuracy: 0.0867\n",
      "iteration number: 57\t training loss: 0.9504\tvalidation loss: 0.8412\t validation accuracy: 0.0867\n",
      "iteration number: 58\t training loss: 0.9504\tvalidation loss: 0.8411\t validation accuracy: 0.0867\n",
      "iteration number: 59\t training loss: 0.9504\tvalidation loss: 0.8411\t validation accuracy: 0.0867\n",
      "iteration number: 60\t training loss: 0.9504\tvalidation loss: 0.8411\t validation accuracy: 0.0867\n",
      "iteration number: 61\t training loss: 0.9503\tvalidation loss: 0.8411\t validation accuracy: 0.0867\n",
      "iteration number: 62\t training loss: 0.9503\tvalidation loss: 0.8410\t validation accuracy: 0.0867\n",
      "iteration number: 63\t training loss: 0.9503\tvalidation loss: 0.8410\t validation accuracy: 0.0867\n",
      "iteration number: 64\t training loss: 0.9503\tvalidation loss: 0.8410\t validation accuracy: 0.0867\n",
      "iteration number: 65\t training loss: 0.9503\tvalidation loss: 0.8410\t validation accuracy: 0.0867\n",
      "iteration number: 66\t training loss: 0.9503\tvalidation loss: 0.8410\t validation accuracy: 0.0867\n",
      "iteration number: 67\t training loss: 0.9502\tvalidation loss: 0.8410\t validation accuracy: 0.0867\n",
      "iteration number: 68\t training loss: 0.9502\tvalidation loss: 0.8410\t validation accuracy: 0.0867\n",
      "iteration number: 69\t training loss: 0.9502\tvalidation loss: 0.8410\t validation accuracy: 0.0867\n",
      "iteration number: 70\t training loss: 0.9502\tvalidation loss: 0.8409\t validation accuracy: 0.0867\n",
      "iteration number: 71\t training loss: 0.9501\tvalidation loss: 0.8409\t validation accuracy: 0.0867\n",
      "iteration number: 72\t training loss: 0.9501\tvalidation loss: 0.8409\t validation accuracy: 0.0867\n",
      "iteration number: 73\t training loss: 0.9501\tvalidation loss: 0.8409\t validation accuracy: 0.0867\n",
      "iteration number: 74\t training loss: 0.9501\tvalidation loss: 0.8409\t validation accuracy: 0.0867\n",
      "iteration number: 75\t training loss: 0.9500\tvalidation loss: 0.8409\t validation accuracy: 0.0867\n",
      "iteration number: 76\t training loss: 0.9500\tvalidation loss: 0.8409\t validation accuracy: 0.0867\n",
      "iteration number: 77\t training loss: 0.9500\tvalidation loss: 0.8409\t validation accuracy: 0.0867\n",
      "iteration number: 78\t training loss: 0.9499\tvalidation loss: 0.8409\t validation accuracy: 0.0867\n",
      "iteration number: 79\t training loss: 0.9499\tvalidation loss: 0.8408\t validation accuracy: 0.0867\n",
      "iteration number: 80\t training loss: 0.9499\tvalidation loss: 0.8408\t validation accuracy: 0.0867\n",
      "iteration number: 81\t training loss: 0.9499\tvalidation loss: 0.8408\t validation accuracy: 0.0867\n",
      "iteration number: 82\t training loss: 0.9498\tvalidation loss: 0.8408\t validation accuracy: 0.0867\n",
      "iteration number: 83\t training loss: 0.9498\tvalidation loss: 0.8408\t validation accuracy: 0.1556\n",
      "iteration number: 84\t training loss: 0.9497\tvalidation loss: 0.8408\t validation accuracy: 0.1089\n",
      "iteration number: 85\t training loss: 0.9497\tvalidation loss: 0.8408\t validation accuracy: 0.1267\n",
      "iteration number: 86\t training loss: 0.9497\tvalidation loss: 0.8407\t validation accuracy: 0.1511\n",
      "iteration number: 87\t training loss: 0.9496\tvalidation loss: 0.8407\t validation accuracy: 0.2156\n",
      "iteration number: 88\t training loss: 0.9496\tvalidation loss: 0.8407\t validation accuracy: 0.1889\n",
      "iteration number: 89\t training loss: 0.9495\tvalidation loss: 0.8407\t validation accuracy: 0.2378\n",
      "iteration number: 90\t training loss: 0.9495\tvalidation loss: 0.8407\t validation accuracy: 0.2600\n",
      "iteration number: 91\t training loss: 0.9494\tvalidation loss: 0.8407\t validation accuracy: 0.2311\n",
      "iteration number: 92\t training loss: 0.9494\tvalidation loss: 0.8406\t validation accuracy: 0.1933\n",
      "iteration number: 93\t training loss: 0.9493\tvalidation loss: 0.8406\t validation accuracy: 0.2200\n",
      "iteration number: 94\t training loss: 0.9493\tvalidation loss: 0.8405\t validation accuracy: 0.2267\n",
      "iteration number: 95\t training loss: 0.9492\tvalidation loss: 0.8404\t validation accuracy: 0.1467\n",
      "iteration number: 96\t training loss: 0.9492\tvalidation loss: 0.8404\t validation accuracy: 0.2111\n",
      "iteration number: 97\t training loss: 0.9491\tvalidation loss: 0.8403\t validation accuracy: 0.2200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 98\t training loss: 0.9490\tvalidation loss: 0.8403\t validation accuracy: 0.1778\n",
      "iteration number: 99\t training loss: 0.9490\tvalidation loss: 0.8402\t validation accuracy: 0.2200\n",
      "iteration number: 100\t training loss: 0.9489\tvalidation loss: 0.8401\t validation accuracy: 0.1889\n",
      "iteration number: 101\t training loss: 0.9488\tvalidation loss: 0.8400\t validation accuracy: 0.1689\n",
      "iteration number: 102\t training loss: 0.9488\tvalidation loss: 0.8400\t validation accuracy: 0.1889\n",
      "iteration number: 103\t training loss: 0.9487\tvalidation loss: 0.8399\t validation accuracy: 0.1600\n",
      "iteration number: 104\t training loss: 0.9486\tvalidation loss: 0.8398\t validation accuracy: 0.2600\n",
      "iteration number: 105\t training loss: 0.9485\tvalidation loss: 0.8398\t validation accuracy: 0.2333\n",
      "iteration number: 106\t training loss: 0.9484\tvalidation loss: 0.8397\t validation accuracy: 0.2867\n",
      "iteration number: 107\t training loss: 0.9484\tvalidation loss: 0.8397\t validation accuracy: 0.2867\n",
      "iteration number: 108\t training loss: 0.9483\tvalidation loss: 0.8396\t validation accuracy: 0.2622\n",
      "iteration number: 109\t training loss: 0.9482\tvalidation loss: 0.8396\t validation accuracy: 0.2556\n",
      "iteration number: 110\t training loss: 0.9481\tvalidation loss: 0.8395\t validation accuracy: 0.2600\n",
      "iteration number: 111\t training loss: 0.9480\tvalidation loss: 0.8394\t validation accuracy: 0.2222\n",
      "iteration number: 112\t training loss: 0.9479\tvalidation loss: 0.8393\t validation accuracy: 0.2044\n",
      "iteration number: 113\t training loss: 0.9478\tvalidation loss: 0.8392\t validation accuracy: 0.2333\n",
      "iteration number: 114\t training loss: 0.9477\tvalidation loss: 0.8391\t validation accuracy: 0.2511\n",
      "iteration number: 115\t training loss: 0.9476\tvalidation loss: 0.8390\t validation accuracy: 0.2667\n",
      "iteration number: 116\t training loss: 0.9474\tvalidation loss: 0.8388\t validation accuracy: 0.2556\n",
      "iteration number: 117\t training loss: 0.9473\tvalidation loss: 0.8387\t validation accuracy: 0.2178\n",
      "iteration number: 118\t training loss: 0.9472\tvalidation loss: 0.8386\t validation accuracy: 0.2044\n",
      "iteration number: 119\t training loss: 0.9471\tvalidation loss: 0.8385\t validation accuracy: 0.2289\n",
      "iteration number: 120\t training loss: 0.9470\tvalidation loss: 0.8385\t validation accuracy: 0.2156\n",
      "iteration number: 121\t training loss: 0.9468\tvalidation loss: 0.8383\t validation accuracy: 0.3022\n",
      "iteration number: 122\t training loss: 0.9467\tvalidation loss: 0.8382\t validation accuracy: 0.2667\n",
      "iteration number: 123\t training loss: 0.9465\tvalidation loss: 0.8381\t validation accuracy: 0.2289\n",
      "iteration number: 124\t training loss: 0.9463\tvalidation loss: 0.8379\t validation accuracy: 0.2822\n",
      "iteration number: 125\t training loss: 0.9462\tvalidation loss: 0.8377\t validation accuracy: 0.2578\n",
      "iteration number: 126\t training loss: 0.9460\tvalidation loss: 0.8375\t validation accuracy: 0.2644\n",
      "iteration number: 127\t training loss: 0.9458\tvalidation loss: 0.8374\t validation accuracy: 0.2356\n",
      "iteration number: 128\t training loss: 0.9457\tvalidation loss: 0.8373\t validation accuracy: 0.1978\n",
      "iteration number: 129\t training loss: 0.9455\tvalidation loss: 0.8372\t validation accuracy: 0.2156\n",
      "iteration number: 130\t training loss: 0.9453\tvalidation loss: 0.8369\t validation accuracy: 0.2244\n",
      "iteration number: 131\t training loss: 0.9450\tvalidation loss: 0.8366\t validation accuracy: 0.2644\n",
      "iteration number: 132\t training loss: 0.9448\tvalidation loss: 0.8364\t validation accuracy: 0.3000\n",
      "iteration number: 133\t training loss: 0.9446\tvalidation loss: 0.8362\t validation accuracy: 0.2844\n",
      "iteration number: 134\t training loss: 0.9444\tvalidation loss: 0.8360\t validation accuracy: 0.2622\n",
      "iteration number: 135\t training loss: 0.9441\tvalidation loss: 0.8358\t validation accuracy: 0.3067\n",
      "iteration number: 136\t training loss: 0.9439\tvalidation loss: 0.8356\t validation accuracy: 0.3400\n",
      "iteration number: 137\t training loss: 0.9436\tvalidation loss: 0.8353\t validation accuracy: 0.3178\n",
      "iteration number: 138\t training loss: 0.9434\tvalidation loss: 0.8351\t validation accuracy: 0.3267\n",
      "iteration number: 139\t training loss: 0.9430\tvalidation loss: 0.8347\t validation accuracy: 0.3044\n",
      "iteration number: 140\t training loss: 0.9428\tvalidation loss: 0.8344\t validation accuracy: 0.3000\n",
      "iteration number: 141\t training loss: 0.9425\tvalidation loss: 0.8342\t validation accuracy: 0.3022\n",
      "iteration number: 142\t training loss: 0.9422\tvalidation loss: 0.8339\t validation accuracy: 0.2867\n",
      "iteration number: 143\t training loss: 0.9419\tvalidation loss: 0.8336\t validation accuracy: 0.2911\n",
      "iteration number: 144\t training loss: 0.9416\tvalidation loss: 0.8334\t validation accuracy: 0.3000\n",
      "iteration number: 145\t training loss: 0.9413\tvalidation loss: 0.8332\t validation accuracy: 0.3533\n",
      "iteration number: 146\t training loss: 0.9409\tvalidation loss: 0.8328\t validation accuracy: 0.3578\n",
      "iteration number: 147\t training loss: 0.9406\tvalidation loss: 0.8324\t validation accuracy: 0.3556\n",
      "iteration number: 148\t training loss: 0.9402\tvalidation loss: 0.8319\t validation accuracy: 0.3667\n",
      "iteration number: 149\t training loss: 0.9398\tvalidation loss: 0.8315\t validation accuracy: 0.3933\n",
      "iteration number: 150\t training loss: 0.9395\tvalidation loss: 0.8311\t validation accuracy: 0.3800\n",
      "iteration number: 151\t training loss: 0.9391\tvalidation loss: 0.8307\t validation accuracy: 0.4022\n",
      "iteration number: 152\t training loss: 0.9388\tvalidation loss: 0.8304\t validation accuracy: 0.4222\n",
      "iteration number: 153\t training loss: 0.9383\tvalidation loss: 0.8299\t validation accuracy: 0.4422\n",
      "iteration number: 154\t training loss: 0.9378\tvalidation loss: 0.8296\t validation accuracy: 0.4178\n",
      "iteration number: 155\t training loss: 0.9374\tvalidation loss: 0.8291\t validation accuracy: 0.3600\n",
      "iteration number: 156\t training loss: 0.9369\tvalidation loss: 0.8285\t validation accuracy: 0.3822\n",
      "iteration number: 157\t training loss: 0.9364\tvalidation loss: 0.8280\t validation accuracy: 0.3911\n",
      "iteration number: 158\t training loss: 0.9359\tvalidation loss: 0.8276\t validation accuracy: 0.4756\n",
      "iteration number: 159\t training loss: 0.9356\tvalidation loss: 0.8274\t validation accuracy: 0.4933\n",
      "iteration number: 160\t training loss: 0.9351\tvalidation loss: 0.8269\t validation accuracy: 0.4911\n",
      "iteration number: 161\t training loss: 0.9345\tvalidation loss: 0.8264\t validation accuracy: 0.4267\n",
      "iteration number: 162\t training loss: 0.9340\tvalidation loss: 0.8259\t validation accuracy: 0.4311\n",
      "iteration number: 163\t training loss: 0.9335\tvalidation loss: 0.8254\t validation accuracy: 0.3689\n",
      "iteration number: 164\t training loss: 0.9329\tvalidation loss: 0.8248\t validation accuracy: 0.3511\n",
      "iteration number: 165\t training loss: 0.9323\tvalidation loss: 0.8241\t validation accuracy: 0.3956\n",
      "iteration number: 166\t training loss: 0.9317\tvalidation loss: 0.8236\t validation accuracy: 0.4133\n",
      "iteration number: 167\t training loss: 0.9311\tvalidation loss: 0.8230\t validation accuracy: 0.4578\n",
      "iteration number: 168\t training loss: 0.9304\tvalidation loss: 0.8224\t validation accuracy: 0.4533\n",
      "iteration number: 169\t training loss: 0.9297\tvalidation loss: 0.8218\t validation accuracy: 0.4489\n",
      "iteration number: 170\t training loss: 0.9291\tvalidation loss: 0.8210\t validation accuracy: 0.4356\n",
      "iteration number: 171\t training loss: 0.9284\tvalidation loss: 0.8203\t validation accuracy: 0.4178\n",
      "iteration number: 172\t training loss: 0.9274\tvalidation loss: 0.8193\t validation accuracy: 0.4222\n",
      "iteration number: 173\t training loss: 0.9266\tvalidation loss: 0.8185\t validation accuracy: 0.4533\n",
      "iteration number: 174\t training loss: 0.9258\tvalidation loss: 0.8177\t validation accuracy: 0.4711\n",
      "iteration number: 175\t training loss: 0.9250\tvalidation loss: 0.8170\t validation accuracy: 0.4822\n",
      "iteration number: 176\t training loss: 0.9242\tvalidation loss: 0.8162\t validation accuracy: 0.4889\n",
      "iteration number: 177\t training loss: 0.9232\tvalidation loss: 0.8151\t validation accuracy: 0.5044\n",
      "iteration number: 178\t training loss: 0.9224\tvalidation loss: 0.8143\t validation accuracy: 0.5267\n",
      "iteration number: 179\t training loss: 0.9215\tvalidation loss: 0.8134\t validation accuracy: 0.5289\n",
      "iteration number: 180\t training loss: 0.9206\tvalidation loss: 0.8125\t validation accuracy: 0.5689\n",
      "iteration number: 181\t training loss: 0.9198\tvalidation loss: 0.8116\t validation accuracy: 0.5711\n",
      "iteration number: 182\t training loss: 0.9189\tvalidation loss: 0.8107\t validation accuracy: 0.5800\n",
      "iteration number: 183\t training loss: 0.9179\tvalidation loss: 0.8096\t validation accuracy: 0.5600\n",
      "iteration number: 184\t training loss: 0.9169\tvalidation loss: 0.8086\t validation accuracy: 0.5622\n",
      "iteration number: 185\t training loss: 0.9160\tvalidation loss: 0.8079\t validation accuracy: 0.5444\n",
      "iteration number: 186\t training loss: 0.9150\tvalidation loss: 0.8070\t validation accuracy: 0.5289\n",
      "iteration number: 187\t training loss: 0.9141\tvalidation loss: 0.8061\t validation accuracy: 0.5400\n",
      "iteration number: 188\t training loss: 0.9131\tvalidation loss: 0.8052\t validation accuracy: 0.5667\n",
      "iteration number: 189\t training loss: 0.9121\tvalidation loss: 0.8042\t validation accuracy: 0.5889\n",
      "iteration number: 190\t training loss: 0.9111\tvalidation loss: 0.8031\t validation accuracy: 0.5778\n",
      "iteration number: 191\t training loss: 0.9101\tvalidation loss: 0.8022\t validation accuracy: 0.5933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 192\t training loss: 0.9090\tvalidation loss: 0.8011\t validation accuracy: 0.5911\n",
      "iteration number: 193\t training loss: 0.9082\tvalidation loss: 0.8004\t validation accuracy: 0.5956\n",
      "iteration number: 194\t training loss: 0.9071\tvalidation loss: 0.7992\t validation accuracy: 0.6222\n",
      "iteration number: 195\t training loss: 0.9060\tvalidation loss: 0.7982\t validation accuracy: 0.6133\n",
      "iteration number: 196\t training loss: 0.9048\tvalidation loss: 0.7971\t validation accuracy: 0.6289\n",
      "iteration number: 197\t training loss: 0.9039\tvalidation loss: 0.7964\t validation accuracy: 0.6356\n",
      "iteration number: 198\t training loss: 0.9025\tvalidation loss: 0.7949\t validation accuracy: 0.6378\n",
      "iteration number: 199\t training loss: 0.9015\tvalidation loss: 0.7943\t validation accuracy: 0.5978\n",
      "iteration number: 200\t training loss: 0.9003\tvalidation loss: 0.7931\t validation accuracy: 0.6178\n",
      "iteration number: 201\t training loss: 0.8992\tvalidation loss: 0.7920\t validation accuracy: 0.6267\n",
      "iteration number: 202\t training loss: 0.8979\tvalidation loss: 0.7907\t validation accuracy: 0.6311\n",
      "iteration number: 203\t training loss: 0.8967\tvalidation loss: 0.7894\t validation accuracy: 0.6156\n",
      "iteration number: 204\t training loss: 0.8955\tvalidation loss: 0.7879\t validation accuracy: 0.6244\n",
      "iteration number: 205\t training loss: 0.8943\tvalidation loss: 0.7867\t validation accuracy: 0.6422\n",
      "iteration number: 206\t training loss: 0.8932\tvalidation loss: 0.7855\t validation accuracy: 0.6511\n",
      "iteration number: 207\t training loss: 0.8920\tvalidation loss: 0.7845\t validation accuracy: 0.6444\n",
      "iteration number: 208\t training loss: 0.8907\tvalidation loss: 0.7831\t validation accuracy: 0.6489\n",
      "iteration number: 209\t training loss: 0.8896\tvalidation loss: 0.7821\t validation accuracy: 0.6600\n",
      "iteration number: 210\t training loss: 0.8885\tvalidation loss: 0.7810\t validation accuracy: 0.6333\n",
      "iteration number: 211\t training loss: 0.8875\tvalidation loss: 0.7803\t validation accuracy: 0.6267\n",
      "iteration number: 212\t training loss: 0.8861\tvalidation loss: 0.7786\t validation accuracy: 0.6400\n",
      "iteration number: 213\t training loss: 0.8850\tvalidation loss: 0.7772\t validation accuracy: 0.6400\n",
      "iteration number: 214\t training loss: 0.8838\tvalidation loss: 0.7759\t validation accuracy: 0.6244\n",
      "iteration number: 215\t training loss: 0.8828\tvalidation loss: 0.7750\t validation accuracy: 0.6156\n",
      "iteration number: 216\t training loss: 0.8818\tvalidation loss: 0.7744\t validation accuracy: 0.6400\n",
      "iteration number: 217\t training loss: 0.8809\tvalidation loss: 0.7733\t validation accuracy: 0.6133\n",
      "iteration number: 218\t training loss: 0.8794\tvalidation loss: 0.7721\t validation accuracy: 0.6489\n",
      "iteration number: 219\t training loss: 0.8781\tvalidation loss: 0.7711\t validation accuracy: 0.6644\n",
      "iteration number: 220\t training loss: 0.8770\tvalidation loss: 0.7698\t validation accuracy: 0.6644\n",
      "iteration number: 221\t training loss: 0.8758\tvalidation loss: 0.7688\t validation accuracy: 0.6756\n",
      "iteration number: 222\t training loss: 0.8748\tvalidation loss: 0.7677\t validation accuracy: 0.6800\n",
      "iteration number: 223\t training loss: 0.8740\tvalidation loss: 0.7668\t validation accuracy: 0.6422\n",
      "iteration number: 224\t training loss: 0.8726\tvalidation loss: 0.7654\t validation accuracy: 0.6822\n",
      "iteration number: 225\t training loss: 0.8714\tvalidation loss: 0.7643\t validation accuracy: 0.6889\n",
      "iteration number: 226\t training loss: 0.8704\tvalidation loss: 0.7635\t validation accuracy: 0.6978\n",
      "iteration number: 227\t training loss: 0.8692\tvalidation loss: 0.7622\t validation accuracy: 0.7133\n",
      "iteration number: 228\t training loss: 0.8682\tvalidation loss: 0.7612\t validation accuracy: 0.7200\n",
      "iteration number: 229\t training loss: 0.8673\tvalidation loss: 0.7601\t validation accuracy: 0.6889\n",
      "iteration number: 230\t training loss: 0.8662\tvalidation loss: 0.7585\t validation accuracy: 0.7111\n",
      "iteration number: 231\t training loss: 0.8653\tvalidation loss: 0.7577\t validation accuracy: 0.7044\n",
      "iteration number: 232\t training loss: 0.8643\tvalidation loss: 0.7567\t validation accuracy: 0.7178\n",
      "iteration number: 233\t training loss: 0.8634\tvalidation loss: 0.7562\t validation accuracy: 0.7133\n",
      "iteration number: 234\t training loss: 0.8625\tvalidation loss: 0.7556\t validation accuracy: 0.7000\n",
      "iteration number: 235\t training loss: 0.8618\tvalidation loss: 0.7550\t validation accuracy: 0.7111\n",
      "iteration number: 236\t training loss: 0.8608\tvalidation loss: 0.7537\t validation accuracy: 0.7067\n",
      "iteration number: 237\t training loss: 0.8600\tvalidation loss: 0.7530\t validation accuracy: 0.6933\n",
      "iteration number: 238\t training loss: 0.8589\tvalidation loss: 0.7518\t validation accuracy: 0.6889\n",
      "iteration number: 239\t training loss: 0.8578\tvalidation loss: 0.7505\t validation accuracy: 0.7133\n",
      "iteration number: 240\t training loss: 0.8571\tvalidation loss: 0.7500\t validation accuracy: 0.7067\n",
      "iteration number: 241\t training loss: 0.8564\tvalidation loss: 0.7492\t validation accuracy: 0.7244\n",
      "iteration number: 242\t training loss: 0.8562\tvalidation loss: 0.7487\t validation accuracy: 0.7000\n",
      "iteration number: 243\t training loss: 0.8548\tvalidation loss: 0.7476\t validation accuracy: 0.7022\n",
      "iteration number: 244\t training loss: 0.8541\tvalidation loss: 0.7469\t validation accuracy: 0.7044\n",
      "iteration number: 245\t training loss: 0.8535\tvalidation loss: 0.7463\t validation accuracy: 0.7089\n",
      "iteration number: 246\t training loss: 0.8524\tvalidation loss: 0.7453\t validation accuracy: 0.7067\n",
      "iteration number: 247\t training loss: 0.8517\tvalidation loss: 0.7446\t validation accuracy: 0.7178\n",
      "iteration number: 248\t training loss: 0.8507\tvalidation loss: 0.7434\t validation accuracy: 0.7289\n",
      "iteration number: 249\t training loss: 0.8495\tvalidation loss: 0.7424\t validation accuracy: 0.7400\n",
      "iteration number: 250\t training loss: 0.8489\tvalidation loss: 0.7419\t validation accuracy: 0.7511\n",
      "iteration number: 251\t training loss: 0.8487\tvalidation loss: 0.7417\t validation accuracy: 0.7267\n",
      "iteration number: 252\t training loss: 0.8478\tvalidation loss: 0.7409\t validation accuracy: 0.7222\n",
      "iteration number: 253\t training loss: 0.8478\tvalidation loss: 0.7408\t validation accuracy: 0.7156\n",
      "iteration number: 254\t training loss: 0.8477\tvalidation loss: 0.7408\t validation accuracy: 0.7000\n",
      "iteration number: 255\t training loss: 0.8463\tvalidation loss: 0.7393\t validation accuracy: 0.7089\n",
      "iteration number: 256\t training loss: 0.8455\tvalidation loss: 0.7385\t validation accuracy: 0.7222\n",
      "iteration number: 257\t training loss: 0.8448\tvalidation loss: 0.7374\t validation accuracy: 0.7289\n",
      "iteration number: 258\t training loss: 0.8444\tvalidation loss: 0.7370\t validation accuracy: 0.7511\n",
      "iteration number: 259\t training loss: 0.8440\tvalidation loss: 0.7367\t validation accuracy: 0.7444\n",
      "iteration number: 260\t training loss: 0.8434\tvalidation loss: 0.7361\t validation accuracy: 0.7511\n",
      "iteration number: 261\t training loss: 0.8421\tvalidation loss: 0.7351\t validation accuracy: 0.7444\n",
      "iteration number: 262\t training loss: 0.8419\tvalidation loss: 0.7348\t validation accuracy: 0.7556\n",
      "iteration number: 263\t training loss: 0.8414\tvalidation loss: 0.7342\t validation accuracy: 0.7489\n",
      "iteration number: 264\t training loss: 0.8402\tvalidation loss: 0.7328\t validation accuracy: 0.7622\n",
      "iteration number: 265\t training loss: 0.8403\tvalidation loss: 0.7330\t validation accuracy: 0.7622\n",
      "iteration number: 266\t training loss: 0.8393\tvalidation loss: 0.7325\t validation accuracy: 0.7644\n",
      "iteration number: 267\t training loss: 0.8375\tvalidation loss: 0.7311\t validation accuracy: 0.7578\n",
      "iteration number: 268\t training loss: 0.8368\tvalidation loss: 0.7302\t validation accuracy: 0.7800\n",
      "iteration number: 269\t training loss: 0.8359\tvalidation loss: 0.7291\t validation accuracy: 0.7889\n",
      "iteration number: 270\t training loss: 0.8358\tvalidation loss: 0.7288\t validation accuracy: 0.7822\n",
      "iteration number: 271\t training loss: 0.8354\tvalidation loss: 0.7284\t validation accuracy: 0.7889\n",
      "iteration number: 272\t training loss: 0.8345\tvalidation loss: 0.7278\t validation accuracy: 0.7778\n",
      "iteration number: 273\t training loss: 0.8344\tvalidation loss: 0.7278\t validation accuracy: 0.7778\n",
      "iteration number: 274\t training loss: 0.8336\tvalidation loss: 0.7272\t validation accuracy: 0.7622\n",
      "iteration number: 275\t training loss: 0.8340\tvalidation loss: 0.7275\t validation accuracy: 0.7689\n",
      "iteration number: 276\t training loss: 0.8335\tvalidation loss: 0.7275\t validation accuracy: 0.7533\n",
      "iteration number: 277\t training loss: 0.8329\tvalidation loss: 0.7268\t validation accuracy: 0.7733\n",
      "iteration number: 278\t training loss: 0.8338\tvalidation loss: 0.7271\t validation accuracy: 0.7911\n",
      "iteration number: 279\t training loss: 0.8324\tvalidation loss: 0.7260\t validation accuracy: 0.7844\n",
      "iteration number: 280\t training loss: 0.8333\tvalidation loss: 0.7267\t validation accuracy: 0.7889\n",
      "iteration number: 281\t training loss: 0.8312\tvalidation loss: 0.7248\t validation accuracy: 0.7933\n",
      "iteration number: 282\t training loss: 0.8308\tvalidation loss: 0.7244\t validation accuracy: 0.7911\n",
      "iteration number: 283\t training loss: 0.8320\tvalidation loss: 0.7252\t validation accuracy: 0.7911\n",
      "iteration number: 284\t training loss: 0.8328\tvalidation loss: 0.7255\t validation accuracy: 0.8022\n",
      "iteration number: 285\t training loss: 0.8320\tvalidation loss: 0.7252\t validation accuracy: 0.7867\n",
      "iteration number: 286\t training loss: 0.8305\tvalidation loss: 0.7239\t validation accuracy: 0.7844\n",
      "iteration number: 287\t training loss: 0.8296\tvalidation loss: 0.7235\t validation accuracy: 0.7733\n",
      "iteration number: 288\t training loss: 0.8295\tvalidation loss: 0.7229\t validation accuracy: 0.8000\n",
      "iteration number: 289\t training loss: 0.8298\tvalidation loss: 0.7229\t validation accuracy: 0.8222\n",
      "iteration number: 290\t training loss: 0.8304\tvalidation loss: 0.7235\t validation accuracy: 0.8067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 291\t training loss: 0.8309\tvalidation loss: 0.7241\t validation accuracy: 0.8000\n",
      "iteration number: 292\t training loss: 0.8285\tvalidation loss: 0.7222\t validation accuracy: 0.7978\n",
      "iteration number: 293\t training loss: 0.8278\tvalidation loss: 0.7212\t validation accuracy: 0.8200\n",
      "iteration number: 294\t training loss: 0.8280\tvalidation loss: 0.7213\t validation accuracy: 0.7978\n",
      "iteration number: 295\t training loss: 0.8272\tvalidation loss: 0.7208\t validation accuracy: 0.8022\n",
      "iteration number: 296\t training loss: 0.8264\tvalidation loss: 0.7198\t validation accuracy: 0.8111\n",
      "iteration number: 297\t training loss: 0.8260\tvalidation loss: 0.7196\t validation accuracy: 0.8156\n",
      "iteration number: 298\t training loss: 0.8250\tvalidation loss: 0.7190\t validation accuracy: 0.8156\n",
      "iteration number: 299\t training loss: 0.8242\tvalidation loss: 0.7183\t validation accuracy: 0.8133\n",
      "iteration number: 300\t training loss: 0.8262\tvalidation loss: 0.7203\t validation accuracy: 0.8133\n",
      "iteration number: 301\t training loss: 0.8278\tvalidation loss: 0.7213\t validation accuracy: 0.8311\n",
      "iteration number: 302\t training loss: 0.8265\tvalidation loss: 0.7206\t validation accuracy: 0.8044\n",
      "iteration number: 303\t training loss: 0.8266\tvalidation loss: 0.7210\t validation accuracy: 0.7889\n",
      "iteration number: 304\t training loss: 0.8262\tvalidation loss: 0.7209\t validation accuracy: 0.8022\n",
      "iteration number: 305\t training loss: 0.8269\tvalidation loss: 0.7210\t validation accuracy: 0.8133\n",
      "iteration number: 306\t training loss: 0.8297\tvalidation loss: 0.7234\t validation accuracy: 0.8022\n",
      "iteration number: 307\t training loss: 0.8296\tvalidation loss: 0.7233\t validation accuracy: 0.8133\n",
      "iteration number: 308\t training loss: 0.8295\tvalidation loss: 0.7231\t validation accuracy: 0.8356\n",
      "iteration number: 309\t training loss: 0.8284\tvalidation loss: 0.7223\t validation accuracy: 0.8267\n",
      "iteration number: 310\t training loss: 0.8267\tvalidation loss: 0.7211\t validation accuracy: 0.8178\n",
      "iteration number: 311\t training loss: 0.8255\tvalidation loss: 0.7205\t validation accuracy: 0.7978\n",
      "iteration number: 312\t training loss: 0.8261\tvalidation loss: 0.7214\t validation accuracy: 0.7844\n",
      "iteration number: 313\t training loss: 0.8261\tvalidation loss: 0.7213\t validation accuracy: 0.7844\n",
      "iteration number: 314\t training loss: 0.8268\tvalidation loss: 0.7218\t validation accuracy: 0.8000\n",
      "iteration number: 315\t training loss: 0.8259\tvalidation loss: 0.7211\t validation accuracy: 0.8067\n",
      "iteration number: 316\t training loss: 0.8249\tvalidation loss: 0.7199\t validation accuracy: 0.8156\n",
      "iteration number: 317\t training loss: 0.8263\tvalidation loss: 0.7217\t validation accuracy: 0.7978\n",
      "iteration number: 318\t training loss: 0.8264\tvalidation loss: 0.7214\t validation accuracy: 0.8022\n",
      "iteration number: 319\t training loss: 0.8251\tvalidation loss: 0.7198\t validation accuracy: 0.8333\n",
      "iteration number: 320\t training loss: 0.8261\tvalidation loss: 0.7206\t validation accuracy: 0.8222\n",
      "iteration number: 321\t training loss: 0.8257\tvalidation loss: 0.7203\t validation accuracy: 0.8111\n",
      "iteration number: 322\t training loss: 0.8258\tvalidation loss: 0.7201\t validation accuracy: 0.8222\n",
      "iteration number: 323\t training loss: 0.8265\tvalidation loss: 0.7217\t validation accuracy: 0.8044\n",
      "iteration number: 324\t training loss: 0.8262\tvalidation loss: 0.7212\t validation accuracy: 0.8244\n",
      "iteration number: 325\t training loss: 0.8247\tvalidation loss: 0.7194\t validation accuracy: 0.8356\n",
      "iteration number: 326\t training loss: 0.8227\tvalidation loss: 0.7178\t validation accuracy: 0.8356\n",
      "iteration number: 327\t training loss: 0.8232\tvalidation loss: 0.7178\t validation accuracy: 0.8444\n",
      "iteration number: 328\t training loss: 0.8239\tvalidation loss: 0.7188\t validation accuracy: 0.8533\n",
      "iteration number: 329\t training loss: 0.8248\tvalidation loss: 0.7191\t validation accuracy: 0.8489\n",
      "iteration number: 330\t training loss: 0.8253\tvalidation loss: 0.7197\t validation accuracy: 0.8489\n",
      "iteration number: 331\t training loss: 0.8262\tvalidation loss: 0.7200\t validation accuracy: 0.8533\n",
      "iteration number: 332\t training loss: 0.8255\tvalidation loss: 0.7200\t validation accuracy: 0.8467\n",
      "iteration number: 333\t training loss: 0.8245\tvalidation loss: 0.7189\t validation accuracy: 0.8489\n",
      "iteration number: 334\t training loss: 0.8268\tvalidation loss: 0.7211\t validation accuracy: 0.8511\n",
      "iteration number: 335\t training loss: 0.8255\tvalidation loss: 0.7205\t validation accuracy: 0.8333\n",
      "iteration number: 336\t training loss: 0.8239\tvalidation loss: 0.7185\t validation accuracy: 0.8489\n",
      "iteration number: 337\t training loss: 0.8237\tvalidation loss: 0.7184\t validation accuracy: 0.8533\n",
      "iteration number: 338\t training loss: 0.8241\tvalidation loss: 0.7188\t validation accuracy: 0.8556\n",
      "iteration number: 339\t training loss: 0.8253\tvalidation loss: 0.7205\t validation accuracy: 0.8511\n",
      "iteration number: 340\t training loss: 0.8237\tvalidation loss: 0.7198\t validation accuracy: 0.8489\n",
      "iteration number: 341\t training loss: 0.8228\tvalidation loss: 0.7193\t validation accuracy: 0.8467\n",
      "iteration number: 342\t training loss: 0.8236\tvalidation loss: 0.7198\t validation accuracy: 0.8444\n",
      "iteration number: 343\t training loss: 0.8253\tvalidation loss: 0.7212\t validation accuracy: 0.8422\n",
      "iteration number: 344\t training loss: 0.8252\tvalidation loss: 0.7217\t validation accuracy: 0.8378\n",
      "iteration number: 345\t training loss: 0.8254\tvalidation loss: 0.7216\t validation accuracy: 0.8422\n",
      "iteration number: 346\t training loss: 0.8254\tvalidation loss: 0.7213\t validation accuracy: 0.8578\n",
      "iteration number: 347\t training loss: 0.8249\tvalidation loss: 0.7210\t validation accuracy: 0.8400\n",
      "iteration number: 348\t training loss: 0.8243\tvalidation loss: 0.7202\t validation accuracy: 0.8489\n",
      "iteration number: 349\t training loss: 0.8219\tvalidation loss: 0.7176\t validation accuracy: 0.8489\n",
      "iteration number: 350\t training loss: 0.8227\tvalidation loss: 0.7181\t validation accuracy: 0.8622\n",
      "iteration number: 351\t training loss: 0.8242\tvalidation loss: 0.7199\t validation accuracy: 0.8556\n",
      "iteration number: 352\t training loss: 0.8223\tvalidation loss: 0.7188\t validation accuracy: 0.8578\n",
      "iteration number: 353\t training loss: 0.8250\tvalidation loss: 0.7217\t validation accuracy: 0.8556\n",
      "iteration number: 354\t training loss: 0.8268\tvalidation loss: 0.7241\t validation accuracy: 0.8444\n",
      "iteration number: 355\t training loss: 0.8263\tvalidation loss: 0.7229\t validation accuracy: 0.8644\n",
      "iteration number: 356\t training loss: 0.8276\tvalidation loss: 0.7243\t validation accuracy: 0.8489\n",
      "iteration number: 357\t training loss: 0.8264\tvalidation loss: 0.7233\t validation accuracy: 0.8556\n",
      "iteration number: 358\t training loss: 0.8256\tvalidation loss: 0.7212\t validation accuracy: 0.8622\n",
      "iteration number: 359\t training loss: 0.8261\tvalidation loss: 0.7215\t validation accuracy: 0.8644\n",
      "iteration number: 360\t training loss: 0.8270\tvalidation loss: 0.7228\t validation accuracy: 0.8644\n",
      "iteration number: 361\t training loss: 0.8283\tvalidation loss: 0.7249\t validation accuracy: 0.8644\n",
      "iteration number: 362\t training loss: 0.8260\tvalidation loss: 0.7228\t validation accuracy: 0.8578\n",
      "iteration number: 363\t training loss: 0.8243\tvalidation loss: 0.7212\t validation accuracy: 0.8578\n",
      "iteration number: 364\t training loss: 0.8239\tvalidation loss: 0.7208\t validation accuracy: 0.8533\n",
      "iteration number: 365\t training loss: 0.8232\tvalidation loss: 0.7204\t validation accuracy: 0.8578\n",
      "iteration number: 366\t training loss: 0.8239\tvalidation loss: 0.7212\t validation accuracy: 0.8444\n",
      "iteration number: 367\t training loss: 0.8232\tvalidation loss: 0.7206\t validation accuracy: 0.8556\n",
      "iteration number: 368\t training loss: 0.8239\tvalidation loss: 0.7207\t validation accuracy: 0.8422\n",
      "iteration number: 369\t training loss: 0.8248\tvalidation loss: 0.7220\t validation accuracy: 0.8356\n",
      "iteration number: 370\t training loss: 0.8247\tvalidation loss: 0.7214\t validation accuracy: 0.8444\n",
      "iteration number: 371\t training loss: 0.8265\tvalidation loss: 0.7216\t validation accuracy: 0.8533\n",
      "iteration number: 372\t training loss: 0.8262\tvalidation loss: 0.7221\t validation accuracy: 0.8489\n",
      "iteration number: 373\t training loss: 0.8281\tvalidation loss: 0.7231\t validation accuracy: 0.8533\n",
      "iteration number: 374\t training loss: 0.8280\tvalidation loss: 0.7229\t validation accuracy: 0.8644\n",
      "iteration number: 375\t training loss: 0.8313\tvalidation loss: 0.7274\t validation accuracy: 0.8644\n",
      "iteration number: 376\t training loss: 0.8291\tvalidation loss: 0.7253\t validation accuracy: 0.8489\n",
      "iteration number: 377\t training loss: 0.8297\tvalidation loss: 0.7255\t validation accuracy: 0.8511\n",
      "iteration number: 378\t training loss: 0.8320\tvalidation loss: 0.7279\t validation accuracy: 0.8644\n",
      "iteration number: 379\t training loss: 0.8322\tvalidation loss: 0.7277\t validation accuracy: 0.8622\n",
      "iteration number: 380\t training loss: 0.8323\tvalidation loss: 0.7282\t validation accuracy: 0.8511\n",
      "iteration number: 381\t training loss: 0.8346\tvalidation loss: 0.7293\t validation accuracy: 0.8600\n",
      "iteration number: 382\t training loss: 0.8317\tvalidation loss: 0.7274\t validation accuracy: 0.8556\n",
      "iteration number: 383\t training loss: 0.8318\tvalidation loss: 0.7271\t validation accuracy: 0.8556\n",
      "iteration number: 384\t training loss: 0.8318\tvalidation loss: 0.7275\t validation accuracy: 0.8622\n",
      "iteration number: 385\t training loss: 0.8325\tvalidation loss: 0.7281\t validation accuracy: 0.8600\n",
      "iteration number: 386\t training loss: 0.8348\tvalidation loss: 0.7296\t validation accuracy: 0.8578\n",
      "iteration number: 387\t training loss: 0.8321\tvalidation loss: 0.7276\t validation accuracy: 0.8556\n",
      "iteration number: 388\t training loss: 0.8291\tvalidation loss: 0.7256\t validation accuracy: 0.8511\n",
      "iteration number: 389\t training loss: 0.8309\tvalidation loss: 0.7280\t validation accuracy: 0.8467\n",
      "iteration number: 390\t training loss: 0.8289\tvalidation loss: 0.7251\t validation accuracy: 0.8533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 391\t training loss: 0.8282\tvalidation loss: 0.7249\t validation accuracy: 0.8644\n",
      "iteration number: 392\t training loss: 0.8273\tvalidation loss: 0.7226\t validation accuracy: 0.8644\n",
      "iteration number: 393\t training loss: 0.8279\tvalidation loss: 0.7238\t validation accuracy: 0.8711\n",
      "iteration number: 394\t training loss: 0.8272\tvalidation loss: 0.7238\t validation accuracy: 0.8667\n",
      "iteration number: 395\t training loss: 0.8257\tvalidation loss: 0.7224\t validation accuracy: 0.8644\n",
      "iteration number: 396\t training loss: 0.8252\tvalidation loss: 0.7223\t validation accuracy: 0.8600\n",
      "iteration number: 397\t training loss: 0.8265\tvalidation loss: 0.7229\t validation accuracy: 0.8622\n",
      "iteration number: 398\t training loss: 0.8230\tvalidation loss: 0.7192\t validation accuracy: 0.8644\n",
      "iteration number: 399\t training loss: 0.8236\tvalidation loss: 0.7202\t validation accuracy: 0.8667\n",
      "iteration number: 400\t training loss: 0.8248\tvalidation loss: 0.7218\t validation accuracy: 0.8733\n",
      "iteration number: 401\t training loss: 0.8257\tvalidation loss: 0.7226\t validation accuracy: 0.8733\n",
      "iteration number: 402\t training loss: 0.8272\tvalidation loss: 0.7242\t validation accuracy: 0.8733\n",
      "iteration number: 403\t training loss: 0.8292\tvalidation loss: 0.7261\t validation accuracy: 0.8756\n",
      "iteration number: 404\t training loss: 0.8309\tvalidation loss: 0.7271\t validation accuracy: 0.8756\n",
      "iteration number: 405\t training loss: 0.8357\tvalidation loss: 0.7307\t validation accuracy: 0.8711\n",
      "iteration number: 406\t training loss: 0.8350\tvalidation loss: 0.7303\t validation accuracy: 0.8733\n",
      "iteration number: 407\t training loss: 0.8353\tvalidation loss: 0.7298\t validation accuracy: 0.8711\n",
      "iteration number: 408\t training loss: 0.8339\tvalidation loss: 0.7287\t validation accuracy: 0.8778\n",
      "iteration number: 409\t training loss: 0.8344\tvalidation loss: 0.7295\t validation accuracy: 0.8756\n",
      "iteration number: 410\t training loss: 0.8337\tvalidation loss: 0.7294\t validation accuracy: 0.8733\n",
      "iteration number: 411\t training loss: 0.8312\tvalidation loss: 0.7270\t validation accuracy: 0.8733\n",
      "iteration number: 412\t training loss: 0.8317\tvalidation loss: 0.7271\t validation accuracy: 0.8733\n",
      "iteration number: 413\t training loss: 0.8295\tvalidation loss: 0.7261\t validation accuracy: 0.8733\n",
      "iteration number: 414\t training loss: 0.8283\tvalidation loss: 0.7254\t validation accuracy: 0.8733\n",
      "iteration number: 415\t training loss: 0.8269\tvalidation loss: 0.7237\t validation accuracy: 0.8711\n",
      "iteration number: 416\t training loss: 0.8283\tvalidation loss: 0.7259\t validation accuracy: 0.8622\n",
      "iteration number: 417\t training loss: 0.8290\tvalidation loss: 0.7259\t validation accuracy: 0.8622\n",
      "iteration number: 418\t training loss: 0.8290\tvalidation loss: 0.7250\t validation accuracy: 0.8689\n",
      "iteration number: 419\t training loss: 0.8305\tvalidation loss: 0.7261\t validation accuracy: 0.8733\n",
      "iteration number: 420\t training loss: 0.8316\tvalidation loss: 0.7272\t validation accuracy: 0.8778\n",
      "iteration number: 421\t training loss: 0.8310\tvalidation loss: 0.7271\t validation accuracy: 0.8756\n",
      "iteration number: 422\t training loss: 0.8340\tvalidation loss: 0.7296\t validation accuracy: 0.8756\n",
      "iteration number: 423\t training loss: 0.8336\tvalidation loss: 0.7297\t validation accuracy: 0.8622\n",
      "iteration number: 424\t training loss: 0.8339\tvalidation loss: 0.7300\t validation accuracy: 0.8622\n",
      "iteration number: 425\t training loss: 0.8363\tvalidation loss: 0.7318\t validation accuracy: 0.8756\n",
      "iteration number: 426\t training loss: 0.8356\tvalidation loss: 0.7305\t validation accuracy: 0.8822\n",
      "iteration number: 427\t training loss: 0.8364\tvalidation loss: 0.7311\t validation accuracy: 0.8778\n",
      "iteration number: 428\t training loss: 0.8343\tvalidation loss: 0.7303\t validation accuracy: 0.8867\n",
      "iteration number: 429\t training loss: 0.8335\tvalidation loss: 0.7302\t validation accuracy: 0.8778\n",
      "iteration number: 430\t training loss: 0.8332\tvalidation loss: 0.7295\t validation accuracy: 0.8822\n",
      "iteration number: 431\t training loss: 0.8355\tvalidation loss: 0.7306\t validation accuracy: 0.8889\n",
      "iteration number: 432\t training loss: 0.8375\tvalidation loss: 0.7331\t validation accuracy: 0.8822\n",
      "iteration number: 433\t training loss: 0.8361\tvalidation loss: 0.7317\t validation accuracy: 0.8822\n",
      "iteration number: 434\t training loss: 0.8340\tvalidation loss: 0.7308\t validation accuracy: 0.8778\n",
      "iteration number: 435\t training loss: 0.8342\tvalidation loss: 0.7315\t validation accuracy: 0.8667\n",
      "iteration number: 436\t training loss: 0.8400\tvalidation loss: 0.7365\t validation accuracy: 0.8667\n",
      "iteration number: 437\t training loss: 0.8390\tvalidation loss: 0.7352\t validation accuracy: 0.8800\n",
      "iteration number: 438\t training loss: 0.8427\tvalidation loss: 0.7382\t validation accuracy: 0.8778\n",
      "iteration number: 439\t training loss: 0.8435\tvalidation loss: 0.7388\t validation accuracy: 0.8778\n",
      "iteration number: 440\t training loss: 0.8419\tvalidation loss: 0.7383\t validation accuracy: 0.8778\n",
      "iteration number: 441\t training loss: 0.8414\tvalidation loss: 0.7383\t validation accuracy: 0.8756\n",
      "iteration number: 442\t training loss: 0.8433\tvalidation loss: 0.7393\t validation accuracy: 0.8844\n",
      "iteration number: 443\t training loss: 0.8426\tvalidation loss: 0.7389\t validation accuracy: 0.8844\n",
      "iteration number: 444\t training loss: 0.8410\tvalidation loss: 0.7379\t validation accuracy: 0.8778\n",
      "iteration number: 445\t training loss: 0.8426\tvalidation loss: 0.7403\t validation accuracy: 0.8756\n",
      "iteration number: 446\t training loss: 0.8434\tvalidation loss: 0.7413\t validation accuracy: 0.8889\n",
      "iteration number: 447\t training loss: 0.8461\tvalidation loss: 0.7438\t validation accuracy: 0.8822\n",
      "iteration number: 448\t training loss: 0.8467\tvalidation loss: 0.7442\t validation accuracy: 0.8889\n",
      "iteration number: 449\t training loss: 0.8488\tvalidation loss: 0.7458\t validation accuracy: 0.8889\n",
      "iteration number: 450\t training loss: 0.8480\tvalidation loss: 0.7459\t validation accuracy: 0.8867\n",
      "iteration number: 451\t training loss: 0.8458\tvalidation loss: 0.7431\t validation accuracy: 0.8800\n",
      "iteration number: 452\t training loss: 0.8457\tvalidation loss: 0.7430\t validation accuracy: 0.8867\n",
      "iteration number: 453\t training loss: 0.8471\tvalidation loss: 0.7445\t validation accuracy: 0.8800\n",
      "iteration number: 454\t training loss: 0.8467\tvalidation loss: 0.7438\t validation accuracy: 0.8800\n",
      "iteration number: 455\t training loss: 0.8489\tvalidation loss: 0.7453\t validation accuracy: 0.8844\n",
      "iteration number: 456\t training loss: 0.8474\tvalidation loss: 0.7440\t validation accuracy: 0.8867\n",
      "iteration number: 457\t training loss: 0.8477\tvalidation loss: 0.7437\t validation accuracy: 0.8911\n",
      "iteration number: 458\t training loss: 0.8495\tvalidation loss: 0.7450\t validation accuracy: 0.8889\n",
      "iteration number: 459\t training loss: 0.8508\tvalidation loss: 0.7459\t validation accuracy: 0.8911\n",
      "iteration number: 460\t training loss: 0.8522\tvalidation loss: 0.7476\t validation accuracy: 0.8800\n",
      "iteration number: 461\t training loss: 0.8512\tvalidation loss: 0.7473\t validation accuracy: 0.8889\n",
      "iteration number: 462\t training loss: 0.8481\tvalidation loss: 0.7451\t validation accuracy: 0.8867\n",
      "iteration number: 463\t training loss: 0.8491\tvalidation loss: 0.7469\t validation accuracy: 0.8867\n",
      "iteration number: 464\t training loss: 0.8507\tvalidation loss: 0.7493\t validation accuracy: 0.8800\n",
      "iteration number: 465\t training loss: 0.8511\tvalidation loss: 0.7490\t validation accuracy: 0.8844\n",
      "iteration number: 466\t training loss: 0.8501\tvalidation loss: 0.7481\t validation accuracy: 0.8867\n",
      "iteration number: 467\t training loss: 0.8500\tvalidation loss: 0.7480\t validation accuracy: 0.8800\n",
      "iteration number: 468\t training loss: 0.8482\tvalidation loss: 0.7458\t validation accuracy: 0.8778\n",
      "iteration number: 469\t training loss: 0.8465\tvalidation loss: 0.7433\t validation accuracy: 0.8822\n",
      "iteration number: 470\t training loss: 0.8446\tvalidation loss: 0.7421\t validation accuracy: 0.8778\n",
      "iteration number: 471\t training loss: 0.8421\tvalidation loss: 0.7403\t validation accuracy: 0.8733\n",
      "iteration number: 472\t training loss: 0.8440\tvalidation loss: 0.7423\t validation accuracy: 0.8733\n",
      "iteration number: 473\t training loss: 0.8449\tvalidation loss: 0.7426\t validation accuracy: 0.8844\n",
      "iteration number: 474\t training loss: 0.8472\tvalidation loss: 0.7450\t validation accuracy: 0.8822\n",
      "iteration number: 475\t training loss: 0.8470\tvalidation loss: 0.7452\t validation accuracy: 0.8756\n",
      "iteration number: 476\t training loss: 0.8459\tvalidation loss: 0.7427\t validation accuracy: 0.8800\n",
      "iteration number: 477\t training loss: 0.8462\tvalidation loss: 0.7434\t validation accuracy: 0.8867\n",
      "iteration number: 478\t training loss: 0.8437\tvalidation loss: 0.7416\t validation accuracy: 0.8867\n",
      "iteration number: 479\t training loss: 0.8433\tvalidation loss: 0.7409\t validation accuracy: 0.8844\n",
      "iteration number: 480\t training loss: 0.8414\tvalidation loss: 0.7396\t validation accuracy: 0.8778\n",
      "iteration number: 481\t training loss: 0.8467\tvalidation loss: 0.7446\t validation accuracy: 0.8800\n",
      "iteration number: 482\t training loss: 0.8482\tvalidation loss: 0.7458\t validation accuracy: 0.8822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 483\t training loss: 0.8466\tvalidation loss: 0.7442\t validation accuracy: 0.8822\n",
      "iteration number: 484\t training loss: 0.8522\tvalidation loss: 0.7501\t validation accuracy: 0.8822\n",
      "iteration number: 485\t training loss: 0.8525\tvalidation loss: 0.7509\t validation accuracy: 0.8844\n",
      "iteration number: 486\t training loss: 0.8531\tvalidation loss: 0.7515\t validation accuracy: 0.8733\n",
      "iteration number: 487\t training loss: 0.8498\tvalidation loss: 0.7482\t validation accuracy: 0.8778\n",
      "iteration number: 488\t training loss: 0.8508\tvalidation loss: 0.7477\t validation accuracy: 0.8800\n",
      "iteration number: 489\t training loss: 0.8493\tvalidation loss: 0.7465\t validation accuracy: 0.8867\n",
      "iteration number: 490\t training loss: 0.8550\tvalidation loss: 0.7508\t validation accuracy: 0.8911\n",
      "iteration number: 491\t training loss: 0.8522\tvalidation loss: 0.7490\t validation accuracy: 0.8867\n",
      "iteration number: 492\t training loss: 0.8556\tvalidation loss: 0.7538\t validation accuracy: 0.8800\n",
      "iteration number: 493\t training loss: 0.8553\tvalidation loss: 0.7531\t validation accuracy: 0.8844\n",
      "iteration number: 494\t training loss: 0.8579\tvalidation loss: 0.7554\t validation accuracy: 0.8911\n",
      "iteration number: 495\t training loss: 0.8563\tvalidation loss: 0.7543\t validation accuracy: 0.8800\n",
      "iteration number: 496\t training loss: 0.8572\tvalidation loss: 0.7554\t validation accuracy: 0.8889\n",
      "iteration number: 497\t training loss: 0.8564\tvalidation loss: 0.7551\t validation accuracy: 0.8733\n",
      "iteration number: 498\t training loss: 0.8582\tvalidation loss: 0.7561\t validation accuracy: 0.8822\n",
      "iteration number: 499\t training loss: 0.8562\tvalidation loss: 0.7537\t validation accuracy: 0.8867\n",
      "iteration number: 500\t training loss: 0.8538\tvalidation loss: 0.7507\t validation accuracy: 0.8911\n",
      "iteration number: 501\t training loss: 0.8580\tvalidation loss: 0.7549\t validation accuracy: 0.8911\n",
      "iteration number: 502\t training loss: 0.8537\tvalidation loss: 0.7516\t validation accuracy: 0.8978\n",
      "iteration number: 503\t training loss: 0.8505\tvalidation loss: 0.7497\t validation accuracy: 0.8911\n",
      "iteration number: 504\t training loss: 0.8552\tvalidation loss: 0.7541\t validation accuracy: 0.8978\n",
      "iteration number: 505\t training loss: 0.8535\tvalidation loss: 0.7522\t validation accuracy: 0.8844\n",
      "iteration number: 506\t training loss: 0.8516\tvalidation loss: 0.7509\t validation accuracy: 0.8933\n",
      "iteration number: 507\t training loss: 0.8545\tvalidation loss: 0.7529\t validation accuracy: 0.8911\n",
      "iteration number: 508\t training loss: 0.8541\tvalidation loss: 0.7522\t validation accuracy: 0.8978\n",
      "iteration number: 509\t training loss: 0.8506\tvalidation loss: 0.7503\t validation accuracy: 0.8889\n",
      "iteration number: 510\t training loss: 0.8507\tvalidation loss: 0.7498\t validation accuracy: 0.9044\n",
      "iteration number: 511\t training loss: 0.8482\tvalidation loss: 0.7471\t validation accuracy: 0.8956\n",
      "iteration number: 512\t training loss: 0.8514\tvalidation loss: 0.7501\t validation accuracy: 0.8956\n",
      "iteration number: 513\t training loss: 0.8532\tvalidation loss: 0.7525\t validation accuracy: 0.8889\n",
      "iteration number: 514\t training loss: 0.8547\tvalidation loss: 0.7535\t validation accuracy: 0.8911\n",
      "iteration number: 515\t training loss: 0.8529\tvalidation loss: 0.7515\t validation accuracy: 0.8956\n",
      "iteration number: 516\t training loss: 0.8515\tvalidation loss: 0.7508\t validation accuracy: 0.8844\n",
      "iteration number: 517\t training loss: 0.8528\tvalidation loss: 0.7523\t validation accuracy: 0.8756\n",
      "iteration number: 518\t training loss: 0.8513\tvalidation loss: 0.7514\t validation accuracy: 0.8756\n",
      "iteration number: 519\t training loss: 0.8504\tvalidation loss: 0.7504\t validation accuracy: 0.8822\n",
      "iteration number: 520\t training loss: 0.8532\tvalidation loss: 0.7518\t validation accuracy: 0.8933\n",
      "iteration number: 521\t training loss: 0.8566\tvalidation loss: 0.7540\t validation accuracy: 0.9022\n",
      "iteration number: 522\t training loss: 0.8542\tvalidation loss: 0.7517\t validation accuracy: 0.8956\n",
      "iteration number: 523\t training loss: 0.8505\tvalidation loss: 0.7488\t validation accuracy: 0.8867\n",
      "iteration number: 524\t training loss: 0.8563\tvalidation loss: 0.7531\t validation accuracy: 0.8978\n",
      "iteration number: 525\t training loss: 0.8526\tvalidation loss: 0.7500\t validation accuracy: 0.8956\n",
      "iteration number: 526\t training loss: 0.8502\tvalidation loss: 0.7481\t validation accuracy: 0.9044\n",
      "iteration number: 527\t training loss: 0.8513\tvalidation loss: 0.7485\t validation accuracy: 0.8933\n",
      "iteration number: 528\t training loss: 0.8533\tvalidation loss: 0.7504\t validation accuracy: 0.8933\n",
      "iteration number: 529\t training loss: 0.8571\tvalidation loss: 0.7542\t validation accuracy: 0.8933\n",
      "iteration number: 530\t training loss: 0.8589\tvalidation loss: 0.7561\t validation accuracy: 0.8933\n",
      "iteration number: 531\t training loss: 0.8601\tvalidation loss: 0.7581\t validation accuracy: 0.8933\n",
      "iteration number: 532\t training loss: 0.8581\tvalidation loss: 0.7557\t validation accuracy: 0.8933\n",
      "iteration number: 533\t training loss: 0.8588\tvalidation loss: 0.7561\t validation accuracy: 0.8889\n",
      "iteration number: 534\t training loss: 0.8575\tvalidation loss: 0.7556\t validation accuracy: 0.8889\n",
      "iteration number: 535\t training loss: 0.8577\tvalidation loss: 0.7551\t validation accuracy: 0.8933\n",
      "iteration number: 536\t training loss: 0.8570\tvalidation loss: 0.7555\t validation accuracy: 0.8911\n",
      "iteration number: 537\t training loss: 0.8592\tvalidation loss: 0.7563\t validation accuracy: 0.8978\n",
      "iteration number: 538\t training loss: 0.8590\tvalidation loss: 0.7556\t validation accuracy: 0.8956\n",
      "iteration number: 539\t training loss: 0.8640\tvalidation loss: 0.7588\t validation accuracy: 0.8956\n",
      "iteration number: 540\t training loss: 0.8586\tvalidation loss: 0.7548\t validation accuracy: 0.9000\n",
      "iteration number: 541\t training loss: 0.8575\tvalidation loss: 0.7548\t validation accuracy: 0.8933\n",
      "iteration number: 542\t training loss: 0.8563\tvalidation loss: 0.7537\t validation accuracy: 0.8933\n",
      "iteration number: 543\t training loss: 0.8594\tvalidation loss: 0.7567\t validation accuracy: 0.8844\n",
      "iteration number: 544\t training loss: 0.8594\tvalidation loss: 0.7565\t validation accuracy: 0.8867\n",
      "iteration number: 545\t training loss: 0.8592\tvalidation loss: 0.7562\t validation accuracy: 0.8889\n",
      "iteration number: 546\t training loss: 0.8596\tvalidation loss: 0.7553\t validation accuracy: 0.9044\n",
      "iteration number: 547\t training loss: 0.8608\tvalidation loss: 0.7569\t validation accuracy: 0.8911\n",
      "iteration number: 548\t training loss: 0.8629\tvalidation loss: 0.7593\t validation accuracy: 0.8978\n",
      "iteration number: 549\t training loss: 0.8629\tvalidation loss: 0.7590\t validation accuracy: 0.8978\n",
      "iteration number: 550\t training loss: 0.8625\tvalidation loss: 0.7591\t validation accuracy: 0.8956\n",
      "iteration number: 551\t training loss: 0.8672\tvalidation loss: 0.7632\t validation accuracy: 0.9089\n",
      "iteration number: 552\t training loss: 0.8641\tvalidation loss: 0.7604\t validation accuracy: 0.9000\n",
      "iteration number: 553\t training loss: 0.8625\tvalidation loss: 0.7586\t validation accuracy: 0.8933\n",
      "iteration number: 554\t training loss: 0.8626\tvalidation loss: 0.7579\t validation accuracy: 0.8933\n",
      "iteration number: 555\t training loss: 0.8597\tvalidation loss: 0.7566\t validation accuracy: 0.8978\n",
      "iteration number: 556\t training loss: 0.8586\tvalidation loss: 0.7557\t validation accuracy: 0.8933\n",
      "iteration number: 557\t training loss: 0.8552\tvalidation loss: 0.7536\t validation accuracy: 0.8956\n",
      "iteration number: 558\t training loss: 0.8545\tvalidation loss: 0.7528\t validation accuracy: 0.9022\n",
      "iteration number: 559\t training loss: 0.8553\tvalidation loss: 0.7529\t validation accuracy: 0.9000\n",
      "iteration number: 560\t training loss: 0.8539\tvalidation loss: 0.7526\t validation accuracy: 0.8956\n",
      "iteration number: 561\t training loss: 0.8574\tvalidation loss: 0.7566\t validation accuracy: 0.8978\n",
      "iteration number: 562\t training loss: 0.8611\tvalidation loss: 0.7605\t validation accuracy: 0.8911\n",
      "iteration number: 563\t training loss: 0.8604\tvalidation loss: 0.7600\t validation accuracy: 0.8800\n",
      "iteration number: 564\t training loss: 0.8612\tvalidation loss: 0.7601\t validation accuracy: 0.8889\n",
      "iteration number: 565\t training loss: 0.8655\tvalidation loss: 0.7630\t validation accuracy: 0.9022\n",
      "iteration number: 566\t training loss: 0.8666\tvalidation loss: 0.7648\t validation accuracy: 0.8911\n",
      "iteration number: 567\t training loss: 0.8662\tvalidation loss: 0.7634\t validation accuracy: 0.9044\n",
      "iteration number: 568\t training loss: 0.8689\tvalidation loss: 0.7664\t validation accuracy: 0.9044\n",
      "iteration number: 569\t training loss: 0.8710\tvalidation loss: 0.7677\t validation accuracy: 0.9044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 570\t training loss: 0.8687\tvalidation loss: 0.7658\t validation accuracy: 0.9022\n",
      "iteration number: 571\t training loss: 0.8663\tvalidation loss: 0.7654\t validation accuracy: 0.8933\n",
      "iteration number: 572\t training loss: 0.8671\tvalidation loss: 0.7661\t validation accuracy: 0.8978\n",
      "iteration number: 573\t training loss: 0.8660\tvalidation loss: 0.7647\t validation accuracy: 0.8978\n",
      "iteration number: 574\t training loss: 0.8644\tvalidation loss: 0.7632\t validation accuracy: 0.8978\n",
      "iteration number: 575\t training loss: 0.8618\tvalidation loss: 0.7613\t validation accuracy: 0.8933\n",
      "iteration number: 576\t training loss: 0.8607\tvalidation loss: 0.7597\t validation accuracy: 0.9089\n",
      "iteration number: 577\t training loss: 0.8626\tvalidation loss: 0.7616\t validation accuracy: 0.9111\n",
      "iteration number: 578\t training loss: 0.8631\tvalidation loss: 0.7618\t validation accuracy: 0.9111\n",
      "iteration number: 579\t training loss: 0.8681\tvalidation loss: 0.7678\t validation accuracy: 0.9156\n",
      "iteration number: 580\t training loss: 0.8659\tvalidation loss: 0.7656\t validation accuracy: 0.9133\n",
      "iteration number: 581\t training loss: 0.8690\tvalidation loss: 0.7685\t validation accuracy: 0.9178\n",
      "iteration number: 582\t training loss: 0.8642\tvalidation loss: 0.7628\t validation accuracy: 0.9133\n",
      "iteration number: 583\t training loss: 0.8658\tvalidation loss: 0.7644\t validation accuracy: 0.9178\n",
      "iteration number: 584\t training loss: 0.8651\tvalidation loss: 0.7641\t validation accuracy: 0.9111\n",
      "iteration number: 585\t training loss: 0.8652\tvalidation loss: 0.7643\t validation accuracy: 0.9022\n",
      "iteration number: 586\t training loss: 0.8697\tvalidation loss: 0.7678\t validation accuracy: 0.9089\n",
      "iteration number: 587\t training loss: 0.8637\tvalidation loss: 0.7629\t validation accuracy: 0.9178\n",
      "iteration number: 588\t training loss: 0.8641\tvalidation loss: 0.7619\t validation accuracy: 0.9156\n",
      "iteration number: 589\t training loss: 0.8658\tvalidation loss: 0.7632\t validation accuracy: 0.9178\n",
      "iteration number: 590\t training loss: 0.8649\tvalidation loss: 0.7619\t validation accuracy: 0.9156\n",
      "iteration number: 591\t training loss: 0.8663\tvalidation loss: 0.7637\t validation accuracy: 0.9156\n",
      "iteration number: 592\t training loss: 0.8691\tvalidation loss: 0.7670\t validation accuracy: 0.9178\n",
      "iteration number: 593\t training loss: 0.8649\tvalidation loss: 0.7641\t validation accuracy: 0.9089\n",
      "iteration number: 594\t training loss: 0.8630\tvalidation loss: 0.7630\t validation accuracy: 0.9000\n",
      "iteration number: 595\t training loss: 0.8616\tvalidation loss: 0.7615\t validation accuracy: 0.9022\n",
      "iteration number: 596\t training loss: 0.8656\tvalidation loss: 0.7658\t validation accuracy: 0.9089\n",
      "iteration number: 597\t training loss: 0.8636\tvalidation loss: 0.7641\t validation accuracy: 0.9044\n",
      "iteration number: 598\t training loss: 0.8647\tvalidation loss: 0.7652\t validation accuracy: 0.9022\n",
      "iteration number: 599\t training loss: 0.8704\tvalidation loss: 0.7698\t validation accuracy: 0.9133\n",
      "iteration number: 600\t training loss: 0.8679\tvalidation loss: 0.7681\t validation accuracy: 0.9111\n",
      "iteration number: 601\t training loss: 0.8669\tvalidation loss: 0.7677\t validation accuracy: 0.9022\n",
      "iteration number: 602\t training loss: 0.8669\tvalidation loss: 0.7680\t validation accuracy: 0.8978\n",
      "iteration number: 603\t training loss: 0.8722\tvalidation loss: 0.7735\t validation accuracy: 0.9022\n",
      "iteration number: 604\t training loss: 0.8725\tvalidation loss: 0.7725\t validation accuracy: 0.9067\n",
      "iteration number: 605\t training loss: 0.8719\tvalidation loss: 0.7723\t validation accuracy: 0.9000\n",
      "iteration number: 606\t training loss: 0.8690\tvalidation loss: 0.7693\t validation accuracy: 0.9044\n",
      "iteration number: 607\t training loss: 0.8711\tvalidation loss: 0.7707\t validation accuracy: 0.9044\n",
      "iteration number: 608\t training loss: 0.8743\tvalidation loss: 0.7743\t validation accuracy: 0.9000\n",
      "iteration number: 609\t training loss: 0.8751\tvalidation loss: 0.7752\t validation accuracy: 0.9000\n",
      "iteration number: 610\t training loss: 0.8719\tvalidation loss: 0.7708\t validation accuracy: 0.9244\n",
      "iteration number: 611\t training loss: 0.8800\tvalidation loss: 0.7787\t validation accuracy: 0.9133\n"
     ]
    }
   ],
   "source": [
    "mlp = MultiLayerPerceptron(X, Y, hidden_size=50, activation='relu')\n",
    "mlp.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Questions:\n",
    "####  Did you succeed to train the MLP and get a high validation accuracy? <br> Display available metrics (training and validation accuracies, training and validation losses)\n",
    "- We managed to get a 93% validation accuracy, which is high.  \n",
    "- However, the training and validation losses are quite high and seems to increase during training.   \n",
    "Training loss = 2093.48  \n",
    "Validation loss = 636.43  \n",
    "Validation accuracy = 0.933  \n",
    "####  Plot the prediction for a given validation sample. Is it accurate?\n",
    "- The prediction is accurate as we managed to get a 93% accuracy\n",
    "#### Compare the full gradient descent with the SGD.\n",
    "- Using SGD as optimizer makes the training quite fast, while the batch gradient takes much longer. However, we do get similar results using the two techniques.\n",
    "#### Play with the hyper parameters you have: the hidden size, the activation function, the initial step and the batch size. <br> Comment. Don't hesitate to visualize results.  \n",
    "- Hidden size = 100 , activation function = relu, initial step = 0.01, batch size=64  \n",
    "Results : Accuracy = 0.34, Training loss = 1280, Validation loss= 380\n",
    "- Hidden size = 50, activation function = relu, initial step = 0.001, batch size =64  \n",
    "Results : Accuracy = 0.18, Traiing loss = 1280, Validation loss =380\n",
    "#### Once properly implemented, compare the training using early stopping, dropout, or both of them. <br> Why are these methods useful here?\n",
    "- Early stopping prevents unecessary iterations (the metrics aren't evolving after a number of iterations)\n",
    "- Dropout prevents overfitting by ensuring that all neurons are used during training\n",
    "#### Once properly implemented, compare the training using momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlp.plot_validation_prediction(sample_id=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlp.plot_loss_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III - Multiclass classification MLP with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Implement the same network architecture with Keras;\n",
    "    - First using the Sequential API\n",
    "    - Secondly using the functional API\n",
    "\n",
    "#### - Check that the Keras model can approximately reproduce the behavior of the Numpy model.\n",
    "\n",
    "#### - Compute the negative log likelihood of a sample 42 in the test set (can use `model.predict_proba`).\n",
    "\n",
    "#### - Compute the average negative log-likelihood on the full test set.\n",
    "\n",
    "#### - Compute the average negative log-likelihood  on the full training set and check that you can get the value of the loss reported by Keras.\n",
    "\n",
    "#### - Is the model overfitting or underfitting? (ensure that the model has fully converged by increasing the number of epochs to 500 or more if necessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_val, Y_tr, Y_val = train_test_split(X, Y)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1), copy=False)\n",
    "X_tr = scaler.fit_transform(X_tr)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X[0].shape[0]\n",
    "n_classes = len(np.unique(Y_tr))\n",
    "n_hidden = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with relu activation\n",
      "Train on 1347 samples, validate on 450 samples\n",
      "Epoch 1/50\n",
      "1347/1347 [==============================] - 1s 452us/sample - loss: 2.2877 - val_loss: 2.1990\n",
      "Epoch 2/50\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 2.1330 - val_loss: 2.0573\n",
      "Epoch 3/50\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 1.9921 - val_loss: 1.9100\n",
      "Epoch 4/50\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 1.8486 - val_loss: 1.7615\n",
      "Epoch 5/50\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 1.7000 - val_loss: 1.6061\n",
      "Epoch 6/50\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 1.5501 - val_loss: 1.4524\n",
      "Epoch 7/50\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 1.3984 - val_loss: 1.2967\n",
      "Epoch 8/50\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 1.2509 - val_loss: 1.1493\n",
      "Epoch 9/50\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 1.1127 - val_loss: 1.0209\n",
      "Epoch 10/50\n",
      "1347/1347 [==============================] - 0s 64us/sample - loss: 0.9938 - val_loss: 0.9116\n",
      "Epoch 11/50\n",
      "1347/1347 [==============================] - 0s 66us/sample - loss: 0.8891 - val_loss: 0.8117\n",
      "Epoch 12/50\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.8029 - val_loss: 0.7347\n",
      "Epoch 13/50\n",
      "1347/1347 [==============================] - 0s 65us/sample - loss: 0.7294 - val_loss: 0.6728\n",
      "Epoch 14/50\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.6679 - val_loss: 0.6156\n",
      "Epoch 15/50\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.6135 - val_loss: 0.5685\n",
      "Epoch 16/50\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.5680 - val_loss: 0.5295\n",
      "Epoch 17/50\n",
      "1347/1347 [==============================] - 0s 64us/sample - loss: 0.5287 - val_loss: 0.4945\n",
      "Epoch 18/50\n",
      "1347/1347 [==============================] - 0s 67us/sample - loss: 0.4933 - val_loss: 0.4656\n",
      "Epoch 19/50\n",
      "1347/1347 [==============================] - 0s 66us/sample - loss: 0.4635 - val_loss: 0.4371\n",
      "Epoch 20/50\n",
      "1347/1347 [==============================] - 0s 66us/sample - loss: 0.4372 - val_loss: 0.4158\n",
      "Epoch 21/50\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.4126 - val_loss: 0.3974\n",
      "Epoch 22/50\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.3905 - val_loss: 0.3790\n",
      "Epoch 23/50\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.3721 - val_loss: 0.3654\n",
      "Epoch 24/50\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.3560 - val_loss: 0.3498\n",
      "Epoch 25/50\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.3389 - val_loss: 0.3366\n",
      "Epoch 26/50\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.3254 - val_loss: 0.3244\n",
      "Epoch 27/50\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.3124 - val_loss: 0.3155\n",
      "Epoch 28/50\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.3009 - val_loss: 0.3060\n",
      "Epoch 29/50\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.2894 - val_loss: 0.2961\n",
      "Epoch 30/50\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.2804 - val_loss: 0.2904\n",
      "Epoch 31/50\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.2699 - val_loss: 0.2811\n",
      "Epoch 32/50\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.2614 - val_loss: 0.2763\n",
      "Epoch 33/50\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.2531 - val_loss: 0.2686\n",
      "Epoch 34/50\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 0.2459 - val_loss: 0.2613\n",
      "Epoch 35/50\n",
      "1347/1347 [==============================] - 0s 65us/sample - loss: 0.2400 - val_loss: 0.2552\n",
      "Epoch 36/50\n",
      "1347/1347 [==============================] - 0s 66us/sample - loss: 0.2321 - val_loss: 0.2521\n",
      "Epoch 37/50\n",
      "1347/1347 [==============================] - 0s 67us/sample - loss: 0.2249 - val_loss: 0.2446\n",
      "Epoch 38/50\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.2194 - val_loss: 0.2410\n",
      "Epoch 39/50\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.2133 - val_loss: 0.2402\n",
      "Epoch 40/50\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.2075 - val_loss: 0.2337\n",
      "Epoch 41/50\n",
      "1347/1347 [==============================] - 0s 66us/sample - loss: 0.2041 - val_loss: 0.2299\n",
      "Epoch 42/50\n",
      "1347/1347 [==============================] - 0s 65us/sample - loss: 0.1981 - val_loss: 0.2257\n",
      "Epoch 43/50\n",
      "1347/1347 [==============================] - 0s 68us/sample - loss: 0.1937 - val_loss: 0.2232\n",
      "Epoch 44/50\n",
      "1347/1347 [==============================] - 0s 69us/sample - loss: 0.1901 - val_loss: 0.2187\n",
      "Epoch 45/50\n",
      "1347/1347 [==============================] - 0s 64us/sample - loss: 0.1852 - val_loss: 0.2160\n",
      "Epoch 46/50\n",
      "1347/1347 [==============================] - 0s 65us/sample - loss: 0.1812 - val_loss: 0.2133\n",
      "Epoch 47/50\n",
      "1347/1347 [==============================] - 0s 64us/sample - loss: 0.1771 - val_loss: 0.2113\n",
      "Epoch 48/50\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 0.1754 - val_loss: 0.2078\n",
      "Epoch 49/50\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.1697 - val_loss: 0.2043\n",
      "Epoch 50/50\n",
      "1347/1347 [==============================] - 0s 66us/sample - loss: 0.1665 - val_loss: 0.2026\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb04c0140b8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "activation = \"relu\"\n",
    "# activation = \"sigmoid\"\n",
    "\n",
    "print('Model with {} activation'.format(activation))\n",
    "keras_model = Sequential()\n",
    "# TODO:\n",
    "keras_model.add(Dense(n_hidden,activation = activation))\n",
    "keras_model.add(Dense(n_classes,activation = \"softmax\"))\n",
    "keras_model.compile(optimizer= 'Adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# This builds the model for the first time:\n",
    "keras_model.fit(X_tr, Y_tr,validation_data=(X_val,Y_val), epochs=50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0000000e+00 1.1651497e-33 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 1.0000000e+00 0.0000000e+00 7.3190517e-33 0.0000000e+00]\n"
     ]
    }
   ],
   "source": [
    "predicts = keras_model.predict_proba(X)\n",
    "print(predicts[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with relu activation\n",
      "Train on 1347 samples, validate on 450 samples\n",
      "Epoch 1/500\n",
      "1347/1347 [==============================] - 0s 306us/sample - loss: 2.2270 - val_loss: 2.1434\n",
      "Epoch 2/500\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 2.0432 - val_loss: 1.9683\n",
      "Epoch 3/500\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 1.8764 - val_loss: 1.7904\n",
      "Epoch 4/500\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 1.7068 - val_loss: 1.6117\n",
      "Epoch 5/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 1.5350 - val_loss: 1.4354\n",
      "Epoch 6/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 1.3653 - val_loss: 1.2651\n",
      "Epoch 7/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 1.2066 - val_loss: 1.1119\n",
      "Epoch 8/500\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 1.0668 - val_loss: 0.9783\n",
      "Epoch 9/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.9481 - val_loss: 0.8674\n",
      "Epoch 10/500\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.8469 - val_loss: 0.7709\n",
      "Epoch 11/500\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.7643 - val_loss: 0.6993\n",
      "Epoch 12/500\n",
      "1347/1347 [==============================] - 0s 67us/sample - loss: 0.6958 - val_loss: 0.6362\n",
      "Epoch 13/500\n",
      "1347/1347 [==============================] - 0s 65us/sample - loss: 0.6377 - val_loss: 0.5879\n",
      "Epoch 14/500\n",
      "1347/1347 [==============================] - 0s 64us/sample - loss: 0.5886 - val_loss: 0.5390\n",
      "Epoch 15/500\n",
      "1347/1347 [==============================] - 0s 67us/sample - loss: 0.5461 - val_loss: 0.5054\n",
      "Epoch 16/500\n",
      "1347/1347 [==============================] - 0s 65us/sample - loss: 0.5108 - val_loss: 0.4663\n",
      "Epoch 17/500\n",
      "1347/1347 [==============================] - 0s 66us/sample - loss: 0.4781 - val_loss: 0.4419\n",
      "Epoch 18/500\n",
      "1347/1347 [==============================] - 0s 64us/sample - loss: 0.4512 - val_loss: 0.4178\n",
      "Epoch 19/500\n",
      "1347/1347 [==============================] - 0s 65us/sample - loss: 0.4246 - val_loss: 0.3936\n",
      "Epoch 20/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.4051 - val_loss: 0.3756\n",
      "Epoch 21/500\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.3824 - val_loss: 0.3584\n",
      "Epoch 22/500\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.3629 - val_loss: 0.3415\n",
      "Epoch 23/500\n",
      "1347/1347 [==============================] - 0s 65us/sample - loss: 0.3471 - val_loss: 0.3265\n",
      "Epoch 24/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.3331 - val_loss: 0.3135\n",
      "Epoch 25/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.3184 - val_loss: 0.3027\n",
      "Epoch 26/500\n",
      "1347/1347 [==============================] - 0s 67us/sample - loss: 0.3040 - val_loss: 0.2937\n",
      "Epoch 27/500\n",
      "1347/1347 [==============================] - 0s 64us/sample - loss: 0.2901 - val_loss: 0.2825\n",
      "Epoch 28/500\n",
      "1347/1347 [==============================] - 0s 66us/sample - loss: 0.2788 - val_loss: 0.2712\n",
      "Epoch 29/500\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 0.2691 - val_loss: 0.2656\n",
      "Epoch 30/500\n",
      "1347/1347 [==============================] - 0s 64us/sample - loss: 0.2573 - val_loss: 0.2563\n",
      "Epoch 31/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.2491 - val_loss: 0.2494\n",
      "Epoch 32/500\n",
      "1347/1347 [==============================] - 0s 65us/sample - loss: 0.2402 - val_loss: 0.2435\n",
      "Epoch 33/500\n",
      "1347/1347 [==============================] - 0s 67us/sample - loss: 0.2325 - val_loss: 0.2363\n",
      "Epoch 34/500\n",
      "1347/1347 [==============================] - 0s 65us/sample - loss: 0.2246 - val_loss: 0.2309\n",
      "Epoch 35/500\n",
      "1347/1347 [==============================] - 0s 72us/sample - loss: 0.2168 - val_loss: 0.2247\n",
      "Epoch 36/500\n",
      "1347/1347 [==============================] - 0s 70us/sample - loss: 0.2113 - val_loss: 0.2207\n",
      "Epoch 37/500\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 0.2045 - val_loss: 0.2171\n",
      "Epoch 38/500\n",
      "1347/1347 [==============================] - 0s 66us/sample - loss: 0.1984 - val_loss: 0.2111\n",
      "Epoch 39/500\n",
      "1347/1347 [==============================] - 0s 66us/sample - loss: 0.1938 - val_loss: 0.2072\n",
      "Epoch 40/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.1879 - val_loss: 0.2048\n",
      "Epoch 41/500\n",
      "1347/1347 [==============================] - 0s 64us/sample - loss: 0.1826 - val_loss: 0.1998\n",
      "Epoch 42/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.1777 - val_loss: 0.1967\n",
      "Epoch 43/500\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.1735 - val_loss: 0.1949\n",
      "Epoch 44/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.1689 - val_loss: 0.1896\n",
      "Epoch 45/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.1646 - val_loss: 0.1862\n",
      "Epoch 46/500\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.1618 - val_loss: 0.1839\n",
      "Epoch 47/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.1577 - val_loss: 0.1834\n",
      "Epoch 48/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.1545 - val_loss: 0.1792\n",
      "Epoch 49/500\n",
      "1347/1347 [==============================] - 0s 66us/sample - loss: 0.1513 - val_loss: 0.1771\n",
      "Epoch 50/500\n",
      "1347/1347 [==============================] - 0s 67us/sample - loss: 0.1482 - val_loss: 0.1745\n",
      "Epoch 51/500\n",
      "1347/1347 [==============================] - 0s 72us/sample - loss: 0.1451 - val_loss: 0.1729\n",
      "Epoch 52/500\n",
      "1347/1347 [==============================] - 0s 69us/sample - loss: 0.1421 - val_loss: 0.1687\n",
      "Epoch 53/500\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.1397 - val_loss: 0.1683\n",
      "Epoch 54/500\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.1370 - val_loss: 0.1666\n",
      "Epoch 55/500\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.1337 - val_loss: 0.1653\n",
      "Epoch 56/500\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.1318 - val_loss: 0.1623\n",
      "Epoch 57/500\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.1285 - val_loss: 0.1614\n",
      "Epoch 58/500\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.1262 - val_loss: 0.1595\n",
      "Epoch 59/500\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.1235 - val_loss: 0.1578\n",
      "Epoch 60/500\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.1209 - val_loss: 0.1560\n",
      "Epoch 61/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.1193 - val_loss: 0.1552\n",
      "Epoch 62/500\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.1175 - val_loss: 0.1542\n",
      "Epoch 63/500\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.1154 - val_loss: 0.1510\n",
      "Epoch 64/500\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.1131 - val_loss: 0.1505\n",
      "Epoch 65/500\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.1118 - val_loss: 0.1483\n",
      "Epoch 66/500\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.1102 - val_loss: 0.1471\n",
      "Epoch 67/500\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.1080 - val_loss: 0.1465\n",
      "Epoch 68/500\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 0.1062 - val_loss: 0.1455\n",
      "Epoch 69/500\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 0.1049 - val_loss: 0.1434\n",
      "Epoch 70/500\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 0.1048 - val_loss: 0.1426\n",
      "Epoch 71/500\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 0.1013 - val_loss: 0.1424\n",
      "Epoch 72/500\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 0.1001 - val_loss: 0.1407\n",
      "Epoch 73/500\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0989 - val_loss: 0.1407\n",
      "Epoch 74/500\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.0968 - val_loss: 0.1398\n",
      "Epoch 75/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0955 - val_loss: 0.1392\n",
      "Epoch 76/500\n",
      "1347/1347 [==============================] - 0s 67us/sample - loss: 0.0935 - val_loss: 0.1386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/500\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.0925 - val_loss: 0.1374\n",
      "Epoch 78/500\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.0918 - val_loss: 0.1366\n",
      "Epoch 79/500\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 0.0904 - val_loss: 0.1351\n",
      "Epoch 80/500\n",
      "1347/1347 [==============================] - 0s 65us/sample - loss: 0.0896 - val_loss: 0.1344\n",
      "Epoch 81/500\n",
      "1347/1347 [==============================] - 0s 66us/sample - loss: 0.0878 - val_loss: 0.1341\n",
      "Epoch 82/500\n",
      "1347/1347 [==============================] - 0s 65us/sample - loss: 0.0860 - val_loss: 0.1333\n",
      "Epoch 83/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0857 - val_loss: 0.1327\n",
      "Epoch 84/500\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.0867 - val_loss: 0.1308\n",
      "Epoch 85/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.0827 - val_loss: 0.1326\n",
      "Epoch 86/500\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 0.0819 - val_loss: 0.1321\n",
      "Epoch 87/500\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0810 - val_loss: 0.1318\n",
      "Epoch 88/500\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 0.0799 - val_loss: 0.1302\n",
      "Epoch 89/500\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0787 - val_loss: 0.1294\n",
      "Epoch 90/500\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 0.0774 - val_loss: 0.1298\n",
      "Epoch 91/500\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0760 - val_loss: 0.1282\n",
      "Epoch 92/500\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 0.0753 - val_loss: 0.1290\n",
      "Epoch 93/500\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.0745 - val_loss: 0.1273\n",
      "Epoch 94/500\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 0.0736 - val_loss: 0.1268\n",
      "Epoch 95/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.0732 - val_loss: 0.1265\n",
      "Epoch 96/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0715 - val_loss: 0.1272\n",
      "Epoch 97/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0706 - val_loss: 0.1270\n",
      "Epoch 98/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.0735 - val_loss: 0.1251\n",
      "Epoch 99/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.0689 - val_loss: 0.1250\n",
      "Epoch 100/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0683 - val_loss: 0.1228\n",
      "Epoch 101/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.0672 - val_loss: 0.1239\n",
      "Epoch 102/500\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 0.0665 - val_loss: 0.1228\n",
      "Epoch 103/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.0665 - val_loss: 0.1238\n",
      "Epoch 104/500\n",
      "1347/1347 [==============================] - 0s 67us/sample - loss: 0.0658 - val_loss: 0.1240\n",
      "Epoch 105/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0642 - val_loss: 0.1233\n",
      "Epoch 106/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.0637 - val_loss: 0.1227\n",
      "Epoch 107/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0625 - val_loss: 0.1226\n",
      "Epoch 108/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0614 - val_loss: 0.1210\n",
      "Epoch 109/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.0609 - val_loss: 0.1205\n",
      "Epoch 110/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0603 - val_loss: 0.1209\n",
      "Epoch 111/500\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.0595 - val_loss: 0.1211\n",
      "Epoch 112/500\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.0595 - val_loss: 0.1209\n",
      "Epoch 113/500\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0595 - val_loss: 0.1200\n",
      "Epoch 114/500\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.0574 - val_loss: 0.1201\n",
      "Epoch 115/500\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.0570 - val_loss: 0.1204\n",
      "Epoch 116/500\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 0.0565 - val_loss: 0.1203\n",
      "Epoch 117/500\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.0556 - val_loss: 0.1189\n",
      "Epoch 118/500\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 0.0549 - val_loss: 0.1191\n",
      "Epoch 119/500\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0549 - val_loss: 0.1189\n",
      "Epoch 120/500\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0533 - val_loss: 0.1197\n",
      "Epoch 121/500\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.0536 - val_loss: 0.1194\n",
      "Epoch 122/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.0524 - val_loss: 0.1187\n",
      "Epoch 123/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0520 - val_loss: 0.1183\n",
      "Epoch 124/500\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.0518 - val_loss: 0.1194\n",
      "Epoch 125/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0503 - val_loss: 0.1171\n",
      "Epoch 126/500\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.0501 - val_loss: 0.1182\n",
      "Epoch 127/500\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.0494 - val_loss: 0.1161\n",
      "Epoch 128/500\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 0.0491 - val_loss: 0.1162\n",
      "Epoch 129/500\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.0489 - val_loss: 0.1167\n",
      "Epoch 130/500\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.0481 - val_loss: 0.1181\n",
      "Epoch 131/500\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0475 - val_loss: 0.1186\n",
      "Epoch 132/500\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.0470 - val_loss: 0.1161\n",
      "Epoch 133/500\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.0463 - val_loss: 0.1176\n",
      "Epoch 134/500\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.0465 - val_loss: 0.1177\n",
      "Epoch 135/500\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.0453 - val_loss: 0.1158\n",
      "Epoch 136/500\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.0449 - val_loss: 0.1159\n",
      "Epoch 137/500\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.0440 - val_loss: 0.1162\n",
      "Epoch 138/500\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.0443 - val_loss: 0.1148\n",
      "Epoch 139/500\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.0442 - val_loss: 0.1154\n",
      "Epoch 140/500\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 0.0428 - val_loss: 0.1153\n",
      "Epoch 141/500\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.0424 - val_loss: 0.1147\n",
      "Epoch 142/500\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 0.0418 - val_loss: 0.1157\n",
      "Epoch 143/500\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.0432 - val_loss: 0.1166\n",
      "Epoch 144/500\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 0.0411 - val_loss: 0.1144\n",
      "Epoch 145/500\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 0.0410 - val_loss: 0.1156\n",
      "Epoch 146/500\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0400 - val_loss: 0.1145\n",
      "Epoch 147/500\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 0.0396 - val_loss: 0.1136\n",
      "Epoch 148/500\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 0.0391 - val_loss: 0.1140\n",
      "Epoch 149/500\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0391 - val_loss: 0.1141\n",
      "Epoch 150/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0398 - val_loss: 0.1153\n",
      "Epoch 151/500\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.0381 - val_loss: 0.1130\n",
      "Epoch 152/500\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0377 - val_loss: 0.1133\n",
      "Epoch 153/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1347/1347 [==============================] - 0s 58us/sample - loss: 0.0374 - val_loss: 0.1132\n",
      "Epoch 154/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.0369 - val_loss: 0.1122\n",
      "Epoch 155/500\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 0.0366 - val_loss: 0.1127\n",
      "Epoch 156/500\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.0367 - val_loss: 0.1132\n",
      "Epoch 157/500\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0357 - val_loss: 0.1117\n",
      "Epoch 158/500\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.0354 - val_loss: 0.1133\n",
      "Epoch 159/500\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.0348 - val_loss: 0.1130\n",
      "Epoch 160/500\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 0.0347 - val_loss: 0.1122\n",
      "Epoch 161/500\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0343 - val_loss: 0.1128\n",
      "Epoch 162/500\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.0340 - val_loss: 0.1127\n",
      "Epoch 163/500\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.0336 - val_loss: 0.1129\n",
      "Epoch 164/500\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.0334 - val_loss: 0.1116\n",
      "Epoch 165/500\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0330 - val_loss: 0.1116\n",
      "Epoch 166/500\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 0.0331 - val_loss: 0.1108\n",
      "Epoch 167/500\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 0.0326 - val_loss: 0.1103\n",
      "Epoch 168/500\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 0.0319 - val_loss: 0.1115\n",
      "Epoch 169/500\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 0.0367 - val_loss: 0.1129\n",
      "Epoch 170/500\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0314 - val_loss: 0.1099\n",
      "Epoch 171/500\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0319 - val_loss: 0.1117\n",
      "Epoch 172/500\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 0.0317 - val_loss: 0.1144\n",
      "Epoch 173/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0310 - val_loss: 0.1097\n",
      "Epoch 174/500\n",
      "1347/1347 [==============================] - 0s 65us/sample - loss: 0.0303 - val_loss: 0.1106\n",
      "Epoch 175/500\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.0295 - val_loss: 0.1109\n",
      "Epoch 176/500\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 0.0298 - val_loss: 0.1094\n",
      "Epoch 177/500\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 0.0298 - val_loss: 0.1096\n",
      "Epoch 178/500\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.0300 - val_loss: 0.1084\n",
      "Epoch 179/500\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.0287 - val_loss: 0.1094\n",
      "Epoch 180/500\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.0283 - val_loss: 0.1093\n",
      "Epoch 181/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0293 - val_loss: 0.1110\n",
      "Epoch 182/500\n",
      "1347/1347 [==============================] - 0s 65us/sample - loss: 0.0281 - val_loss: 0.1078\n",
      "Epoch 183/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0278 - val_loss: 0.1094\n",
      "Epoch 184/500\n",
      "1347/1347 [==============================] - 0s 64us/sample - loss: 0.0284 - val_loss: 0.1085\n",
      "Epoch 185/500\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.0268 - val_loss: 0.1089\n",
      "Epoch 186/500\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 0.0271 - val_loss: 0.1109\n",
      "Epoch 187/500\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.0270 - val_loss: 0.1083\n",
      "Epoch 188/500\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 0.0267 - val_loss: 0.1085\n",
      "Epoch 189/500\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0262 - val_loss: 0.1082\n",
      "Epoch 190/500\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.0259 - val_loss: 0.1086\n",
      "Epoch 191/500\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0259 - val_loss: 0.1091\n",
      "Epoch 192/500\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0257 - val_loss: 0.1092\n",
      "Epoch 193/500\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.0253 - val_loss: 0.1083\n",
      "Epoch 194/500\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 0.0250 - val_loss: 0.1080\n",
      "Epoch 195/500\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.0248 - val_loss: 0.1076\n",
      "Epoch 196/500\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 0.0246 - val_loss: 0.1082\n",
      "Epoch 197/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0247 - val_loss: 0.1082\n",
      "Epoch 198/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0240 - val_loss: 0.1074\n",
      "Epoch 199/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0238 - val_loss: 0.1102\n",
      "Epoch 200/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.0238 - val_loss: 0.1062\n",
      "Epoch 201/500\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 0.0232 - val_loss: 0.1079\n",
      "Epoch 202/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.0231 - val_loss: 0.1077\n",
      "Epoch 203/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0231 - val_loss: 0.1062\n",
      "Epoch 204/500\n",
      "1347/1347 [==============================] - 0s 66us/sample - loss: 0.0227 - val_loss: 0.1065\n",
      "Epoch 205/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.0225 - val_loss: 0.1056\n",
      "Epoch 206/500\n",
      "1347/1347 [==============================] - 0s 65us/sample - loss: 0.0224 - val_loss: 0.1059\n",
      "Epoch 207/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.0220 - val_loss: 0.1062\n",
      "Epoch 208/500\n",
      "1347/1347 [==============================] - 0s 64us/sample - loss: 0.0219 - val_loss: 0.1065\n",
      "Epoch 209/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0220 - val_loss: 0.1053\n",
      "Epoch 210/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.0214 - val_loss: 0.1077\n",
      "Epoch 211/500\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.0215 - val_loss: 0.1065\n",
      "Epoch 212/500\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.0212 - val_loss: 0.1073\n",
      "Epoch 213/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.0209 - val_loss: 0.1050\n",
      "Epoch 214/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0212 - val_loss: 0.1055\n",
      "Epoch 215/500\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.0218 - val_loss: 0.1061\n",
      "Epoch 216/500\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 0.0209 - val_loss: 0.1081\n",
      "Epoch 217/500\n",
      "1347/1347 [==============================] - 0s 64us/sample - loss: 0.0213 - val_loss: 0.1084\n",
      "Epoch 218/500\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 0.0200 - val_loss: 0.1074\n",
      "Epoch 219/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.0197 - val_loss: 0.1054\n",
      "Epoch 220/500\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.0199 - val_loss: 0.1080\n",
      "Epoch 221/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0198 - val_loss: 0.1064\n",
      "Epoch 222/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.0192 - val_loss: 0.1059\n",
      "Epoch 223/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0191 - val_loss: 0.1061\n",
      "Epoch 224/500\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.0189 - val_loss: 0.1077\n",
      "Epoch 225/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0186 - val_loss: 0.1062\n",
      "Epoch 226/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0185 - val_loss: 0.1064\n",
      "Epoch 227/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0195 - val_loss: 0.1068\n",
      "Epoch 228/500\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 0.0186 - val_loss: 0.1074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/500\n",
      "1347/1347 [==============================] - 0s 66us/sample - loss: 0.0179 - val_loss: 0.1063\n",
      "Epoch 230/500\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.0178 - val_loss: 0.1058\n",
      "Epoch 231/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.0177 - val_loss: 0.1064\n",
      "Epoch 232/500\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.0174 - val_loss: 0.1057\n",
      "Epoch 233/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0173 - val_loss: 0.1062\n",
      "Epoch 234/500\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.0169 - val_loss: 0.1064\n",
      "Epoch 235/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.0167 - val_loss: 0.1077\n",
      "Epoch 236/500\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.0167 - val_loss: 0.1061\n",
      "Epoch 237/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0167 - val_loss: 0.1073\n",
      "Epoch 238/500\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 0.0166 - val_loss: 0.1053\n",
      "Epoch 239/500\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.0165 - val_loss: 0.1048\n",
      "Epoch 240/500\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 0.0168 - val_loss: 0.1081\n",
      "Epoch 241/500\n",
      "1347/1347 [==============================] - 0s 64us/sample - loss: 0.0160 - val_loss: 0.1064\n",
      "Epoch 242/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0160 - val_loss: 0.1051\n",
      "Epoch 243/500\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 0.0158 - val_loss: 0.1072\n",
      "Epoch 244/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0207 - val_loss: 0.1081\n",
      "Epoch 245/500\n",
      "1347/1347 [==============================] - 0s 66us/sample - loss: 0.0153 - val_loss: 0.1069\n",
      "Epoch 246/500\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.0153 - val_loss: 0.1076\n",
      "Epoch 247/500\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0151 - val_loss: 0.1063\n",
      "Epoch 248/500\n",
      "1347/1347 [==============================] - 0s 65us/sample - loss: 0.0148 - val_loss: 0.1063\n",
      "Epoch 249/500\n",
      "1347/1347 [==============================] - 0s 64us/sample - loss: 0.0146 - val_loss: 0.1064\n",
      "Epoch 250/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0147 - val_loss: 0.1060\n",
      "Epoch 251/500\n",
      "1347/1347 [==============================] - 0s 64us/sample - loss: 0.0140 - val_loss: 0.1094\n",
      "Epoch 252/500\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0146 - val_loss: 0.1071\n",
      "Epoch 253/500\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 0.0144 - val_loss: 0.1048\n",
      "Epoch 254/500\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0144 - val_loss: 0.1071\n",
      "Epoch 255/500\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.0139 - val_loss: 0.1064\n",
      "Epoch 256/500\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.0138 - val_loss: 0.1075\n",
      "Epoch 257/500\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0137 - val_loss: 0.1074\n",
      "Epoch 258/500\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0135 - val_loss: 0.1063\n",
      "Epoch 259/500\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.0134 - val_loss: 0.1060\n",
      "Epoch 260/500\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.0133 - val_loss: 0.1062\n",
      "Epoch 261/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0130 - val_loss: 0.1071\n",
      "Epoch 262/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0130 - val_loss: 0.1053\n",
      "Epoch 263/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0130 - val_loss: 0.1060\n",
      "Epoch 264/500\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.0128 - val_loss: 0.1052\n",
      "Epoch 265/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.0129 - val_loss: 0.1072\n",
      "Epoch 266/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0125 - val_loss: 0.1064\n",
      "Epoch 267/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.0124 - val_loss: 0.1087\n",
      "Epoch 268/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0130 - val_loss: 0.1073\n",
      "Epoch 269/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.0123 - val_loss: 0.1058\n",
      "Epoch 270/500\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 0.0120 - val_loss: 0.1065\n",
      "Epoch 271/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0119 - val_loss: 0.1083\n",
      "Epoch 272/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0122 - val_loss: 0.1053\n",
      "Epoch 273/500\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 0.0117 - val_loss: 0.1070\n",
      "Epoch 274/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0115 - val_loss: 0.1064\n",
      "Epoch 275/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0114 - val_loss: 0.1065\n",
      "Epoch 276/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0114 - val_loss: 0.1061\n",
      "Epoch 277/500\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.0112 - val_loss: 0.1067\n",
      "Epoch 278/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.0114 - val_loss: 0.1099\n",
      "Epoch 279/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0110 - val_loss: 0.1083\n",
      "Epoch 280/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0117 - val_loss: 0.1091\n",
      "Epoch 281/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.0114 - val_loss: 0.1074\n",
      "Epoch 282/500\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 0.0108 - val_loss: 0.1098\n",
      "Epoch 283/500\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 0.0106 - val_loss: 0.1099\n",
      "Epoch 284/500\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 0.0105 - val_loss: 0.1093\n",
      "Epoch 285/500\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 0.0105 - val_loss: 0.1077\n",
      "Epoch 286/500\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 0.0104 - val_loss: 0.1086\n",
      "Epoch 287/500\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.0103 - val_loss: 0.1096\n",
      "Epoch 288/500\n",
      "1347/1347 [==============================] - 0s 64us/sample - loss: 0.0101 - val_loss: 0.1085\n",
      "Epoch 289/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.0100 - val_loss: 0.1095\n",
      "Epoch 290/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.0099 - val_loss: 0.1090\n",
      "Epoch 291/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.0098 - val_loss: 0.1091\n",
      "Epoch 292/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0099 - val_loss: 0.1110\n",
      "Epoch 293/500\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.0104 - val_loss: 0.1087\n",
      "Epoch 294/500\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.0095 - val_loss: 0.1104\n",
      "Epoch 295/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0093 - val_loss: 0.1095\n",
      "Epoch 296/500\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.0096 - val_loss: 0.1075\n",
      "Epoch 297/500\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.0097 - val_loss: 0.1080\n",
      "Epoch 298/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0093 - val_loss: 0.1096\n",
      "Epoch 299/500\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 0.0091 - val_loss: 0.1110\n",
      "Epoch 300/500\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 0.0090 - val_loss: 0.1094\n",
      "Epoch 301/500\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.0091 - val_loss: 0.1097\n",
      "Epoch 302/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0088 - val_loss: 0.1096\n",
      "Epoch 303/500\n",
      "1347/1347 [==============================] - 0s 66us/sample - loss: 0.0086 - val_loss: 0.1122\n",
      "Epoch 304/500\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 0.0087 - val_loss: 0.1091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 305/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.0086 - val_loss: 0.1092\n",
      "Epoch 306/500\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.0086 - val_loss: 0.1099\n",
      "Epoch 307/500\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 0.0084 - val_loss: 0.1096\n",
      "Epoch 308/500\n",
      "1347/1347 [==============================] - 0s 64us/sample - loss: 0.0082 - val_loss: 0.1113\n",
      "Epoch 309/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0083 - val_loss: 0.1109\n",
      "Epoch 310/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0083 - val_loss: 0.1106\n",
      "Epoch 311/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.0080 - val_loss: 0.1114\n",
      "Epoch 312/500\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.0082 - val_loss: 0.1157\n",
      "Epoch 313/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0081 - val_loss: 0.1112\n",
      "Epoch 314/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0077 - val_loss: 0.1121\n",
      "Epoch 315/500\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.0078 - val_loss: 0.1107\n",
      "Epoch 316/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.0076 - val_loss: 0.1126\n",
      "Epoch 317/500\n",
      "1347/1347 [==============================] - 0s 64us/sample - loss: 0.0081 - val_loss: 0.1105\n",
      "Epoch 318/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0075 - val_loss: 0.1103\n",
      "Epoch 319/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0074 - val_loss: 0.1133\n",
      "Epoch 320/500\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.0073 - val_loss: 0.1107\n",
      "Epoch 321/500\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.0074 - val_loss: 0.1115\n",
      "Epoch 322/500\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.0076 - val_loss: 0.1137\n",
      "Epoch 323/500\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.0072 - val_loss: 0.1109\n",
      "Epoch 324/500\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 0.0071 - val_loss: 0.1118\n",
      "Epoch 325/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0069 - val_loss: 0.1130\n",
      "Epoch 326/500\n",
      "1347/1347 [==============================] - 0s 67us/sample - loss: 0.0069 - val_loss: 0.1137\n",
      "Epoch 327/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0069 - val_loss: 0.1131\n",
      "Epoch 328/500\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 0.0068 - val_loss: 0.1133\n",
      "Epoch 329/500\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.0067 - val_loss: 0.1120\n",
      "Epoch 330/500\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 0.0066 - val_loss: 0.1127\n",
      "Epoch 331/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0065 - val_loss: 0.1130\n",
      "Epoch 332/500\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 0.0065 - val_loss: 0.1140\n",
      "Epoch 333/500\n",
      "1347/1347 [==============================] - 0s 66us/sample - loss: 0.0063 - val_loss: 0.1128\n",
      "Epoch 334/500\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.0063 - val_loss: 0.1128\n",
      "Epoch 335/500\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.0072 - val_loss: 0.1169\n",
      "Epoch 336/500\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 0.0066 - val_loss: 0.1145\n",
      "Epoch 337/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0062 - val_loss: 0.1129\n",
      "Epoch 338/500\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.0062 - val_loss: 0.1165\n",
      "Epoch 339/500\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 0.0061 - val_loss: 0.1143\n",
      "Epoch 340/500\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 0.0059 - val_loss: 0.1153\n",
      "Epoch 341/500\n",
      "1347/1347 [==============================] - 0s 66us/sample - loss: 0.0059 - val_loss: 0.1155\n",
      "Epoch 342/500\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 0.0058 - val_loss: 0.1148\n",
      "Epoch 343/500\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.0057 - val_loss: 0.1171\n",
      "Epoch 344/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0057 - val_loss: 0.1131\n",
      "Epoch 345/500\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.0058 - val_loss: 0.1163\n",
      "Epoch 346/500\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.0056 - val_loss: 0.1161\n",
      "Epoch 347/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0056 - val_loss: 0.1140\n",
      "Epoch 348/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0054 - val_loss: 0.1155\n",
      "Epoch 349/500\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 0.0054 - val_loss: 0.1176\n",
      "Epoch 350/500\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.0053 - val_loss: 0.1152\n",
      "Epoch 351/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0054 - val_loss: 0.1168\n",
      "Epoch 352/500\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 0.0053 - val_loss: 0.1157\n",
      "Epoch 353/500\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.0052 - val_loss: 0.1199\n",
      "Epoch 354/500\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.0052 - val_loss: 0.1168\n",
      "Epoch 355/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0052 - val_loss: 0.1192\n",
      "Epoch 356/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0051 - val_loss: 0.1193\n",
      "Epoch 357/500\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 0.0051 - val_loss: 0.1172\n",
      "Epoch 358/500\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.0049 - val_loss: 0.1211\n",
      "Epoch 359/500\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 0.0050 - val_loss: 0.1168\n",
      "Epoch 360/500\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 0.0049 - val_loss: 0.1205\n",
      "Epoch 361/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0047 - val_loss: 0.1180\n",
      "Epoch 362/500\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.0047 - val_loss: 0.1191\n",
      "Epoch 363/500\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.0046 - val_loss: 0.1180\n",
      "Epoch 364/500\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 0.0046 - val_loss: 0.1196\n",
      "Epoch 365/500\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 0.0046 - val_loss: 0.1181\n",
      "Epoch 366/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0045 - val_loss: 0.1206\n",
      "Epoch 367/500\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.0046 - val_loss: 0.1222\n",
      "Epoch 368/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0045 - val_loss: 0.1179\n",
      "Epoch 369/500\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 0.0043 - val_loss: 0.1205\n",
      "Epoch 370/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0044 - val_loss: 0.1229\n",
      "Epoch 371/500\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.0043 - val_loss: 0.1197\n",
      "Epoch 372/500\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 0.0043 - val_loss: 0.1213\n",
      "Epoch 373/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.0043 - val_loss: 0.1214\n",
      "Epoch 374/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0042 - val_loss: 0.1204\n",
      "Epoch 375/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.0041 - val_loss: 0.1214\n",
      "Epoch 376/500\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.0040 - val_loss: 0.1223\n",
      "Epoch 377/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0041 - val_loss: 0.1207\n",
      "Epoch 378/500\n",
      "1347/1347 [==============================] - 0s 67us/sample - loss: 0.0040 - val_loss: 0.1192\n",
      "Epoch 379/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0038 - val_loss: 0.1240\n",
      "Epoch 380/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0040 - val_loss: 0.1217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 381/500\n",
      "1347/1347 [==============================] - 0s 66us/sample - loss: 0.0038 - val_loss: 0.1208\n",
      "Epoch 382/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.0038 - val_loss: 0.1216\n",
      "Epoch 383/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0038 - val_loss: 0.1201\n",
      "Epoch 384/500\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 0.0038 - val_loss: 0.1231\n",
      "Epoch 385/500\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 0.0036 - val_loss: 0.1205\n",
      "Epoch 386/500\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 0.0036 - val_loss: 0.1225\n",
      "Epoch 387/500\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.0036 - val_loss: 0.1225\n",
      "Epoch 388/500\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.0035 - val_loss: 0.1215\n",
      "Epoch 389/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0035 - val_loss: 0.1231\n",
      "Epoch 390/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0035 - val_loss: 0.1237\n",
      "Epoch 391/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0035 - val_loss: 0.1228\n",
      "Epoch 392/500\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.0034 - val_loss: 0.1240\n",
      "Epoch 393/500\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.0033 - val_loss: 0.1239\n",
      "Epoch 394/500\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 0.0033 - val_loss: 0.1282\n",
      "Epoch 395/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0034 - val_loss: 0.1253\n",
      "Epoch 396/500\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.0033 - val_loss: 0.1238\n",
      "Epoch 397/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0032 - val_loss: 0.1238\n",
      "Epoch 398/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.0032 - val_loss: 0.1219\n",
      "Epoch 399/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0032 - val_loss: 0.1265\n",
      "Epoch 400/500\n",
      "1347/1347 [==============================] - 0s 64us/sample - loss: 0.0031 - val_loss: 0.1243\n",
      "Epoch 401/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0031 - val_loss: 0.1249\n",
      "Epoch 402/500\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 0.0030 - val_loss: 0.1270\n",
      "Epoch 403/500\n",
      "1347/1347 [==============================] - 0s 64us/sample - loss: 0.0030 - val_loss: 0.1265\n",
      "Epoch 404/500\n",
      "1347/1347 [==============================] - 0s 65us/sample - loss: 0.0030 - val_loss: 0.1273\n",
      "Epoch 405/500\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.0030 - val_loss: 0.1233\n",
      "Epoch 406/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0029 - val_loss: 0.1273\n",
      "Epoch 407/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0031 - val_loss: 0.1274\n",
      "Epoch 408/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0029 - val_loss: 0.1249\n",
      "Epoch 409/500\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.0028 - val_loss: 0.1285\n",
      "Epoch 410/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.0028 - val_loss: 0.1278\n",
      "Epoch 411/500\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.0027 - val_loss: 0.1260\n",
      "Epoch 412/500\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 0.0027 - val_loss: 0.1286\n",
      "Epoch 413/500\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.0027 - val_loss: 0.1265\n",
      "Epoch 414/500\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.0027 - val_loss: 0.1281\n",
      "Epoch 415/500\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.0026 - val_loss: 0.1266\n",
      "Epoch 416/500\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 0.0026 - val_loss: 0.1292\n",
      "Epoch 417/500\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0025 - val_loss: 0.1264\n",
      "Epoch 418/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0026 - val_loss: 0.1273\n",
      "Epoch 419/500\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.0025 - val_loss: 0.1275\n",
      "Epoch 420/500\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.0025 - val_loss: 0.1281\n",
      "Epoch 421/500\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.0024 - val_loss: 0.1261\n",
      "Epoch 422/500\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 0.0024 - val_loss: 0.1286\n",
      "Epoch 423/500\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0024 - val_loss: 0.1313\n",
      "Epoch 424/500\n",
      "1347/1347 [==============================] - 0s 66us/sample - loss: 0.0024 - val_loss: 0.1260\n",
      "Epoch 425/500\n",
      "1347/1347 [==============================] - 0s 64us/sample - loss: 0.0027 - val_loss: 0.1272\n",
      "Epoch 426/500\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 0.0023 - val_loss: 0.1316\n",
      "Epoch 427/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0022 - val_loss: 0.1282\n",
      "Epoch 428/500\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 0.0023 - val_loss: 0.1299\n",
      "Epoch 429/500\n",
      "1347/1347 [==============================] - 0s 65us/sample - loss: 0.0023 - val_loss: 0.1299\n",
      "Epoch 430/500\n",
      "1347/1347 [==============================] - 0s 64us/sample - loss: 0.0022 - val_loss: 0.1292\n",
      "Epoch 431/500\n",
      "1347/1347 [==============================] - 0s 64us/sample - loss: 0.0024 - val_loss: 0.1306\n",
      "Epoch 432/500\n",
      "1347/1347 [==============================] - 0s 64us/sample - loss: 0.0022 - val_loss: 0.1320\n",
      "Epoch 433/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0021 - val_loss: 0.1315\n",
      "Epoch 434/500\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.0022 - val_loss: 0.1337\n",
      "Epoch 435/500\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0022 - val_loss: 0.1343\n",
      "Epoch 436/500\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 0.0021 - val_loss: 0.1333\n",
      "Epoch 437/500\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0020 - val_loss: 0.1305\n",
      "Epoch 438/500\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0020 - val_loss: 0.1325\n",
      "Epoch 439/500\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.0020 - val_loss: 0.1324\n",
      "Epoch 440/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0020 - val_loss: 0.1306\n",
      "Epoch 441/500\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0020 - val_loss: 0.1316\n",
      "Epoch 442/500\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0019 - val_loss: 0.1340\n",
      "Epoch 443/500\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 0.0020 - val_loss: 0.1356\n",
      "Epoch 444/500\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.0019 - val_loss: 0.1329\n",
      "Epoch 445/500\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 0.0019 - val_loss: 0.1339\n",
      "Epoch 446/500\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 0.0019 - val_loss: 0.1375\n",
      "Epoch 447/500\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.0018 - val_loss: 0.1348\n",
      "Epoch 448/500\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.0018 - val_loss: 0.1349\n",
      "Epoch 449/500\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.0018 - val_loss: 0.1331\n",
      "Epoch 450/500\n",
      "1347/1347 [==============================] - 0s 67us/sample - loss: 0.0018 - val_loss: 0.1354\n",
      "Epoch 451/500\n",
      "1347/1347 [==============================] - 0s 65us/sample - loss: 0.0017 - val_loss: 0.1342\n",
      "Epoch 452/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0018 - val_loss: 0.1348\n",
      "Epoch 453/500\n",
      "1347/1347 [==============================] - 0s 64us/sample - loss: 0.0017 - val_loss: 0.1348\n",
      "Epoch 454/500\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 0.0017 - val_loss: 0.1361\n",
      "Epoch 455/500\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.0017 - val_loss: 0.1348\n",
      "Epoch 456/500\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.0017 - val_loss: 0.1359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 457/500\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0016 - val_loss: 0.1375\n",
      "Epoch 458/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0016 - val_loss: 0.1377\n",
      "Epoch 459/500\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0016 - val_loss: 0.1389\n",
      "Epoch 460/500\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 0.0016 - val_loss: 0.1354\n",
      "Epoch 461/500\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 0.0026 - val_loss: 0.1424\n",
      "Epoch 462/500\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.0016 - val_loss: 0.1399\n",
      "Epoch 463/500\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0015 - val_loss: 0.1371\n",
      "Epoch 464/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0015 - val_loss: 0.1384\n",
      "Epoch 465/500\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.0014 - val_loss: 0.1388\n",
      "Epoch 466/500\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 0.0015 - val_loss: 0.1382\n",
      "Epoch 467/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0014 - val_loss: 0.1396\n",
      "Epoch 468/500\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.0014 - val_loss: 0.1398\n",
      "Epoch 469/500\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.0015 - val_loss: 0.1405\n",
      "Epoch 470/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0015 - val_loss: 0.1390\n",
      "Epoch 471/500\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.0014 - val_loss: 0.1389\n",
      "Epoch 472/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0014 - val_loss: 0.1409\n",
      "Epoch 473/500\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.0014 - val_loss: 0.1381\n",
      "Epoch 474/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0014 - val_loss: 0.1397\n",
      "Epoch 475/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0013 - val_loss: 0.1393\n",
      "Epoch 476/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0013 - val_loss: 0.1400\n",
      "Epoch 477/500\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 0.0013 - val_loss: 0.1422\n",
      "Epoch 478/500\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.0013 - val_loss: 0.1407\n",
      "Epoch 479/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0013 - val_loss: 0.1395\n",
      "Epoch 480/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.0012 - val_loss: 0.1416\n",
      "Epoch 481/500\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.0012 - val_loss: 0.1410\n",
      "Epoch 482/500\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.0012 - val_loss: 0.1415\n",
      "Epoch 483/500\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 0.0012 - val_loss: 0.1444\n",
      "Epoch 484/500\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 0.0012 - val_loss: 0.1416\n",
      "Epoch 485/500\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0012 - val_loss: 0.1428\n",
      "Epoch 486/500\n",
      "1347/1347 [==============================] - 0s 64us/sample - loss: 0.0012 - val_loss: 0.1428\n",
      "Epoch 487/500\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.0012 - val_loss: 0.1460\n",
      "Epoch 488/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.0011 - val_loss: 0.1443\n",
      "Epoch 489/500\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.0011 - val_loss: 0.1426\n",
      "Epoch 490/500\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.0012 - val_loss: 0.1447\n",
      "Epoch 491/500\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.0011 - val_loss: 0.1439\n",
      "Epoch 492/500\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 0.0011 - val_loss: 0.1467\n",
      "Epoch 493/500\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 0.0011 - val_loss: 0.1444\n",
      "Epoch 494/500\n",
      "1347/1347 [==============================] - 0s 64us/sample - loss: 0.0011 - val_loss: 0.1433\n",
      "Epoch 495/500\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.0011 - val_loss: 0.1485\n",
      "Epoch 496/500\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.0011 - val_loss: 0.1465\n",
      "Epoch 497/500\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 0.0010 - val_loss: 0.1439\n",
      "Epoch 498/500\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 0.0011 - val_loss: 0.1452\n",
      "Epoch 499/500\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 0.0010 - val_loss: 0.1461\n",
      "Epoch 500/500\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 0.0010 - val_loss: 0.1449\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb04403d390>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "activation = \"relu\"\n",
    "# activation = \"sigmoid\"\n",
    "\n",
    "print('Model with {} activation'.format(activation))\n",
    "\n",
    "inputs = Input(shape=(n_features,))\n",
    "# TODO:\n",
    "x = layers.Dense(n_hidden,activation = activation)(inputs)\n",
    "outputs = layers.Dense(n_classes,activation = \"softmax\")(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs, name='keras_model')\n",
    "\n",
    "model.compile(optimizer= 'Adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# This builds the model for the first time:\n",
    "model.fit(X_tr, Y_tr,validation_data=(X_val,Y_val), epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that you know if the model is underfitting or overfitting:\n",
    "#### - In case of underfitting, can you explain why ? Also change the structure of the 2 previous networks to cancell underfitting\n",
    "#### - In case of overfitting, explain why and change the structure of the 2 previous networks to cancell the overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With 50 epochs, our model does not overfit or underfit\n",
    "- With 50 epochs, we get a very low training loss at 0.00005 and a validation loss at 0.1682, which means or "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
