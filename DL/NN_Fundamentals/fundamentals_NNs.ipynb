{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "EPSILON = 1e-8 # small constant to avoid underflow or divide per 0\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I - Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This time, the data will correspond to greyscale images. <br> Two different datasets can be used here:\n",
    "- The MNIST dataset, small 8*8 images, corresponding to handwritten digits &rightarrow; 10 classes\n",
    "- The Fashion MNIST dataset, medium 28*28 images, corresponding to clothes pictures &rightarrow; 10 classes\n",
    "\n",
    "#### Starting with the simple MNIST is recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"MNIST\"\n",
    "# dataset = \"FASHION_MNIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset='MNIST'):\n",
    "    if dataset == 'MNIST':\n",
    "        digits = load_digits()\n",
    "        X, Y = np.asarray(digits['data'], dtype='float32'), np.asarray(digits['target'], dtype='int32')\n",
    "        return X, Y\n",
    "    elif dataset == 'FASHION_MNIST':\n",
    "        import tensorflow as tf\n",
    "        fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "        (X, Y), (_, _) = fashion_mnist.load_data()\n",
    "        X = X.reshape((X.shape[0], X.shape[1] * X.shape[2]))\n",
    "        X, Y = np.asarray(X, dtype='float32'), np.asarray(Y, dtype='int32')\n",
    "        return X, Y\n",
    "X, Y = load_data(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(np.unique(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 1797\n",
      "Input dimension: 64\n",
      "Number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples: {:d}'.format(X.shape[0]))\n",
    "print('Input dimension: {:d}'.format(X.shape[1]))  # images 8x8 or 28*28 actually\n",
    "print('Number of classes: {:d}'.format(n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range max-min of greyscale pixel values: (16.0, 0.0)\n",
      "First image sample:\n",
      "[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.\n",
      " 15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.\n",
      "  0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.\n",
      "  0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]\n",
      "First image label: 0\n",
      "Input design matrix shape: (1797, 64)\n"
     ]
    }
   ],
   "source": [
    "print(\"Range max-min of greyscale pixel values: ({0:.1f}, {1:.1f})\".format(np.max(X), np.min(X)))\n",
    "print(\"First image sample:\\n{0}\".format(X[0]))\n",
    "print(\"First image label: {0}\".format(Y[0]))\n",
    "print(\"Input design matrix shape: {0}\".format(X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does the data look like?\n",
    "Each image in the dataset consists of a 8 x 8 (or 28 x 28) matrix, of greyscale pixels. For the MNIST dataset, the values are between 0 and 16 where 0 represents white, 16 represents black and there are many shades of grey in-between. For the Fashion MNIST dataset, the values are between 0 and 255.<br>Each image is assigned a corresponding numerical label, so the image in ```X[i]``` has its corresponding label stored in ```Y[i]```.\n",
    "\n",
    "The next cells below demonstrate how to visualise the input data. Make sure you understand what's happening, particularly how the indices correspond to individual items in the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data_sample(X, Y, nrows=2, ncols=2):\n",
    "    fig, ax = plt.subplots(nrows, ncols)\n",
    "    for row in ax:\n",
    "        for col in row:\n",
    "            index = random.randint(0, X.shape[0])\n",
    "            dim = np.sqrt(X.shape[1]).astype(int)\n",
    "            col.imshow(X[index].reshape((dim, dim)), cmap=plt.cm.gray_r)\n",
    "            col.set_title(\"image label: %d\" % Y[index])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_data_sample(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II - Multiclass classification MLP with Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II a) - Problem definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/mlp_mnist.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task here will be to implement \"from scratch\" a Multilayer Perceptron for classification.\n",
    "\n",
    "We will define the formal categorical cross entropy loss as follows:\n",
    "$$\n",
    "l(\\mathbf{\\Theta}, \\mathbf{X}, \\mathbf{Y}) = - \\frac{1}{n} \\sum_{i=1}^n \\log \\mathbf{f}(\\mathbf{x}_i ; \\mathbf{\\Theta})^\\top y_i\n",
    "$$\n",
    "<center>with $y_i$ being the one-hot encoded true label for the sample $i$, and $\\Theta = (\\mathbf{W}^h; \\mathbf{b}^h; \\mathbf{W}^o; \\mathbf{b}^o)$</center>\n",
    "<center>In addition, $\\mathbf{f}(\\mathbf{x}) = softmax(\\mathbf{z^o}(\\mathbf{x})) = softmax(\\mathbf{W}^o\\mathbf{h}(\\mathbf{x}) + \\mathbf{b}^o)$</center>\n",
    "<center>and $\\mathbf{h}(\\mathbf{x}) = g(\\mathbf{z^h}(\\mathbf{x})) = g(\\mathbf{W}^h\\mathbf{x} + \\mathbf{b}^h)$, $g$ being the activation function and could be implemented with $sigmoid$ or $relu$</center>\n",
    "\n",
    "## Objectives:\n",
    "- Write the categorical cross entropy loss function\n",
    "- Write the activation functions with their associated gradient\n",
    "- Write the softmax function that is going to be used to output the predicted probabilities\n",
    "- Implement the forward pass through the neural network\n",
    "- Implement the backpropagation according to the used loss: progagate the gradients using the chain rule and return $(\\mathbf{\\nabla_{W^h}}l ; \\mathbf{\\nabla_{b^h}}l ; \\mathbf{\\nabla_{W^o}}l ; \\mathbf{\\nabla_{b^o}}l)$\n",
    "- Implement dropout regularization in the forward pass: be careful to consider both training and prediction cases\n",
    "- Implement the SGD optimization algorithm, and improve it with simple momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple graph function to let you have a global overview:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/function_graph.png\" style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) You may find numpy outer products useful: <br>\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.outer.html <br>\n",
    "We have: $outer(u, v) = u \\cdot v^T$, with $u, v$ two vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, v = np.random.normal(size=(5,)), np.random.normal(size=(10,))\n",
    "assert np.array_equal(\n",
    "    np.outer(u, v),\n",
    "    np.dot(np.reshape(u, (u.size, 1)), np.reshape(v, (1, v.size)))\n",
    ")\n",
    "assert np.outer(u, v).shape == (5, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) You also may find numpy matmul function useful: <br>\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.matmul.html <br>\n",
    "It can be used to perform matrix products along one fixed dimension (i.e. the batch size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, B = np.random.randint(0, 100, size=(64, 5, 10)), np.random.randint(0, 100, size=(64, 10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.array_equal(\n",
    "    np.stack([np.dot(A_i, B_i) for A_i, B_i in zip(A, B)]),\n",
    "    np.matmul(A, B)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-28719bea9ef9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mvelocities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'W_h'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'b_h'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'W_o'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'b_o'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mvelocities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.9\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mvelocities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvelocities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W_h'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    }
   ],
   "source": [
    "Grads = namedtuple('Grads', ['W_h', 'b_h', 'W_o', 'b_o'])\n",
    "test = Grads(1,2,3,4)\n",
    "print(test.W_h)\n",
    "\n",
    "velocities = {'W_h': 1., 'b_h': 1., 'W_o': 0., 'b_o': 0.}\n",
    "velocities[:,] = 0.9*velocities[:,]\n",
    "print(velocities['W_h'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II b) - Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron():\n",
    "    \"\"\"MLP with one hidden layer having a hidden activation,\n",
    "    and one output layer having a softmax activation\"\"\"\n",
    "    def __init__(self, X, Y, hidden_size, activation='relu',\n",
    "                 initialization='uniform', dropout=False, dropout_rate=1):\n",
    "        # input, hidden, and output dimensions on the MLP based on X, Y\n",
    "        self.input_size, self.output_size = X.shape[1], len(np.unique(Y))\n",
    "        self.hidden_size = hidden_size\n",
    "        # initialization strategies: avoid a full-0 initialization of the weight matricest\n",
    "        if initialization == 'uniform':\n",
    "            self.W_h = np.random.uniform(size=(self.hidden_size, self.input_size), high=0.01, low=-0.01)\n",
    "            self.W_o = np.random.uniform(size=(self.output_size, self.hidden_size), high=0.01, low=-0.01)\n",
    "        elif initialization == 'normal':\n",
    "            self.W_h = np.random.normal(size=(self.hidden_size, self.input_size), loc=0, scale=0.01)\n",
    "            self.W_o = np.random.normal(size=(self.output_size, self.hidden_size), loc=0, scale=0.01)\n",
    "        # the bias could be initializated to 0 or a random low constant\n",
    "        self.b_h = np.zeros(self.hidden_size)\n",
    "        self.b_o = np.zeros(self.output_size)\n",
    "        # our namedtuple structure of gradients\n",
    "        self.Grads = namedtuple('Grads', ['W_h', 'b_h', 'W_o', 'b_o'])\n",
    "        # and the velocities associated which are going to be useful for the momentum\n",
    "        self.velocities = {'W_h': 0., 'b_h': 0., 'W_o': 0., 'b_o': 0.}\n",
    "        # the hidden activation function used\n",
    "        self.activation = activation\n",
    "        # arrays to track back the losses and accuracies evolution\n",
    "        self.training_losses_history = []\n",
    "        self.validation_losses_history = []\n",
    "        self.training_acc_history = []\n",
    "        self.validation_acc_history = []\n",
    "        # train val split and normalization of the features\n",
    "        self.X_tr, self.X_val, self.Y_tr, self.Y_val = self.split_train_validation(X, Y)\n",
    "        self.scaler = MinMaxScaler(feature_range=(0, 1), copy=False)\n",
    "        self.X_tr = self.scaler.fit_transform(self.X_tr)\n",
    "        self.X_val = self.scaler.transform(self.X_val)\n",
    "        # dropout parameters\n",
    "        self.dropout = dropout\n",
    "        self.dropout_rate = dropout_rate\n",
    "        # step used for the optimization algorithm and setted later (learning rate)\n",
    "        self.step = 0\n",
    "    \n",
    "    # One-hot encoding of the target\n",
    "    # Transform the integer represensation to a sparse one\n",
    "    @staticmethod\n",
    "    def one_hot(n_classes, Y):\n",
    "        return np.eye(n_classes)[Y]\n",
    "    \n",
    "    # Reverse one-hot encoding of the target\n",
    "    # Recover the former integer representation\n",
    "    # ex: from (0,0,1,0) to 2\n",
    "    @staticmethod\n",
    "    def reverse_one_hot(Y_one_hot):\n",
    "        return np.asarray(np.where(Y_one_hot==1)[1], dtype='int32')\n",
    "    \n",
    "    \"\"\"\n",
    "    Activation functions and their gradient\n",
    "    \"\"\"\n",
    "    # In implementations below X is a matrix of shape (n_samples, p)\n",
    "    \n",
    "    # A max_value value is indicated for the relu and grad_relu functions\n",
    "    # Make sure to clip the output to it to prevent numerical overflow (exploding gradient)\n",
    "    # Make it so the max value reachable is max_value\n",
    "    @staticmethod\n",
    "    def relu(X, max_value=20):\n",
    "        assert max_value > 0\n",
    "        # TODO:\n",
    "        return np.clip(X,0,max_value)\n",
    "    \n",
    "    # Make it so the gradient becomes 0 when X becomes greater than max_value\n",
    "    @staticmethod\n",
    "    def grad_relu(X, max_value=20):\n",
    "        assert max_value > 0\n",
    "        # TODO:\n",
    "        grad_logic = (X >= 0)*(X < max_value)\n",
    "        return grad_logic.astype(int)\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(X):\n",
    "        # TODO:\n",
    "        return 1/(1+np.exp(-X))\n",
    "    \n",
    "    @staticmethod\n",
    "    def grad_sigmoid(X):\n",
    "        # TODO:\n",
    "        return self.sigmoid(X)*(1-self.sigmoid(X))\n",
    "    \n",
    "    # Softmax function to output probabilities\n",
    "    @staticmethod\n",
    "    def softmax(X):\n",
    "        # TODO:\n",
    "        return np.exp(X)/np.sum(np.exp(X))\n",
    "    \n",
    "    # Loss function\n",
    "    # Consider using EPSILON to prevent numerical issues (log(0) is undefined)\n",
    "    # Y_true and Y_pred are of shape (n_samples,n_classes)\n",
    "    @staticmethod\n",
    "    def categorical_cross_entropy(Y_true, Y_pred):\n",
    "        # TODO:\n",
    "        #n = Y_true.shape\n",
    "        return -np.mean(np.dot(np.log(Y_pred).T,Y_true))\n",
    "    \n",
    "    @staticmethod\n",
    "    def split_train_validation(X, Y, test_size=0.25, seed=False):\n",
    "        random_state = 42 if seed else np.random.randint(1e3)\n",
    "        X_tr, X_val, Y_tr, Y_val = train_test_split(X, Y, test_size=test_size, random_state=random_state)\n",
    "        return X_tr, X_val, Y_tr, Y_val\n",
    "    \n",
    "    # Sample random batch in (X, Y) with a given batch_size for SGD\n",
    "    @staticmethod\n",
    "    def get_random_batch(X, Y, batch_size):\n",
    "        indexes = np.random.choice(X.shape[0], size=batch_size, replace=False)\n",
    "        return X[indexes], Y[indexes]\n",
    "        \n",
    "    # Forward pass: compute f(x) as y, and return optionally the hidden states h(x) and z_h(x) for compute_grads\n",
    "    def forward(self, X, return_activation=False, training=False):\n",
    "        if self.activation == 'relu':\n",
    "            g_activation = self.relu\n",
    "        elif self.activation == 'sigmoid':\n",
    "            g_activation = self.sigmoid\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "            \n",
    "        #z_h = np.zeros((X.shape[0], self.hidden_size)) if len(X.shape) > 1 else np.zeros(self.hidden_size)\n",
    "        #h = np.zeros((X.shape[0], self.hidden_size)) if len(X.shape) > 1 else np.zeros(self.hidden_size)\n",
    "        # TODO:\n",
    "        #print(\"Parameters shape : \", self.W_h.shape, X.shape, self.b_h.shape)\n",
    "        \n",
    "        \n",
    "        z_h = np.dot(self.W_h,X.T).T + self.b_h\n",
    "        #print(\"z_h forward shape : \",z_h.shape)\n",
    "        h = g_activation(z_h)\n",
    "        #print(\"H forward shape : \", h.shape)\n",
    "        if self.dropout:\n",
    "            if training:\n",
    "                # TODO:\n",
    "                pass\n",
    "                #h_size = h.shape[0]\n",
    "                #dropout_vec = np.binomial(1,self.dropout_rate,h_size)\n",
    "                #h = h*dropout_vec\n",
    "            else:\n",
    "                # TODO:\n",
    "                pass\n",
    "                #h = h*self.dropout_rate\n",
    "        # TODO:\n",
    "        #y = np.zeros((X.shape[0], self.output_size)) if len(X.shape) > 1 else np.zeros(self.output_size)        \n",
    "        z_o = np.dot(self.W_o,h.T).T + self.b_o\n",
    "        #print(\"Z_o forward shape\",z_o.shape)\n",
    "        y = self.softmax(z_o)\n",
    "        #print(\"f(x) forward shape : \",y.shape)\n",
    "        if return_activation:\n",
    "            return y, h, z_h\n",
    "        else:\n",
    "            return y\n",
    "    \n",
    "    # Backpropagation: return an instantiation of self.Grads that contains the average gradients for the given batch\n",
    "    def compute_grads(self, X, Y_true, vectorized=False):\n",
    "        if self.activation == 'relu':\n",
    "            g_grad = self.grad_relu\n",
    "        elif self.activation == 'sigmoid':\n",
    "            g_grad = self.grad_sigmoid\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        if len(X.shape) == 1:\n",
    "            X = X.reshape((1,) + X.shape)\n",
    "        \n",
    "        if not vectorized:\n",
    "            n = X.shape[0]\n",
    "            grad_W_h = np.zeros((self.hidden_size, self.input_size))\n",
    "            grad_b_h = np.zeros((self.hidden_size, )) \n",
    "            grad_W_o = np.zeros((self.output_size, self.hidden_size))\n",
    "            grad_b_o = np.zeros((self.output_size, ))\n",
    "           \n",
    "            for x, y_true in zip(X, Y_true):\n",
    "                y_pred, h, z_h = self.forward(x, return_activation=True, training=True)\n",
    "                #compute activation gradients\n",
    "                grad_z_o = y_pred - self.one_hot(self.output_size, y_true)\n",
    "                #print(\"Grad z_o shape : \", grad_z_o.shape)\n",
    "                #computer last layer parameters gradients \n",
    "                grad_W_o += np.outer(grad_z_o, h.T)\n",
    "              \n",
    "                #print(\"Grad W_o shape : \",grad_W_o.shape)\n",
    "                grad_b_o += grad_z_o\n",
    "                #print(\"Grad b_o shape : \",grad_b_o.shape)\n",
    "                #compute first layer activation gradients\n",
    "                grad_h = np.dot(self.W_o.T,grad_z_o)\n",
    "                #print(\"Grad h shape : \",grad_h.shape)\n",
    "\n",
    "                grad_z_h = np.multiply(grad_h,g_grad(z_h))\n",
    "                #print(\"Grad z_h shape : \",grad_z_h.shape)\n",
    "\n",
    "                #compute first layer parameters gradients\n",
    "                grad_W_h += np.outer(grad_z_h,x.T)\n",
    "                #print(\"Grad W_h shape : \",grad_W_h.shape)\n",
    "                grad_b_h += grad_z_h\n",
    "                #print(\"Grad b_h shape : \",grad_b_h.shape)\n",
    "                # TODO:\n",
    "                \n",
    "            grads = self.Grads(grad_W_h/n, grad_b_h/n, grad_W_o/n, grad_b_o/n)\n",
    "            \n",
    "        else: \n",
    "            Y_pred, h, z_h = self.forward(X, return_activation=True, training=True)\n",
    "\n",
    "            # TODO (optional), try to do the backprop without Python loops in a vectorized way:\n",
    "            \n",
    "            grad_W_h = np.zeros((X.shape[0], self.hidden_size, self.input_size))\n",
    "            grad_b_h = np.zeros((X.shape[0], self.hidden_size, )) \n",
    "            grad_W_o = np.zeros((X.shape[0], self.output_size, self.hidden_size))\n",
    "            grad_b_o = np.zeros((X.shape[0], self.output_size, ))\n",
    "            \n",
    "            grads = self.Grads(\n",
    "                np.mean(grad_W_h, axis=0), \n",
    "                np.mean(grad_b_h, axis=0), \n",
    "                np.mean(grad_W_o, axis=0), \n",
    "                np.mean(grad_b_o, axis=0)\n",
    "            )\n",
    "            \n",
    "        return grads\n",
    "    \n",
    "    # Perform the update of the parameters (W_h, b_h, W_o, b_o) based of their gradient\n",
    "    def optimizer_step(self, optimizer='gd', momentum=False, momentum_alpha=0.9, \n",
    "                       batch_size=None, vectorized=True):\n",
    "        if optimizer == 'gd':\n",
    "            grads = self.compute_grads(self.X_tr, self.Y_tr, vectorized=vectorized)\n",
    "        elif optimizer == 'sgd':\n",
    "            batch_X_tr, batch_Y_tr = self.get_random_batch(self.X_tr, self.Y_tr, batch_size)\n",
    "            grads = self.compute_grads(batch_X_tr, batch_Y_tr, vectorized=vectorized)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        if not momentum:\n",
    "            # TODO:\n",
    "            #print(\"Shape step and shape grad W_h\",self.step,grads.W_h.shape)\n",
    "            self.W_h = self.W_h - self.step * grads.W_h\n",
    "            self.b_h = self.b_h - self.step * grads.b_h\n",
    "            self.W_o = self.W_o - self.step * grads.W_o\n",
    "            self.b_o = self.b_o - self.step * grads.b_o\n",
    "        else:\n",
    "            # remember: use the stored velocities\n",
    "            # TODO:\n",
    "            # compute velocities update for momentum\n",
    "            self.velocities['W_h'] = momentum_alpha * self.velocities['W_h'] - self.step * grads.W_h\n",
    "            self.velocities['W_o'] = momentum_alpha * self.velocities['W_o'] - self.step * grads.W_o\n",
    "            self.velocities['b_h'] = momentum_alpha * self.velocities['b_h'] - self.step * grads.b_h\n",
    "            self.velocities['b_o'] = momentum_alpha * self.velocities['b_o'] - self.step * grads.b_o\n",
    "            # update parameters\n",
    "            self.W_h += self.velocities['W_h']\n",
    "            self.b_h += self.velocities['b_h']\n",
    "            self.W_o += self.velocities['W_o']\n",
    "            self.b_o += self.velocities['b_o']\n",
    "            \n",
    "            \n",
    "            pass\n",
    "    \n",
    "    # Loss wrapper\n",
    "    def loss(self, Y_true, Y_pred):\n",
    "        return self.categorical_cross_entropy(self.one_hot(self.output_size, Y_true), Y_pred)\n",
    "    \n",
    "    def loss_history_flush(self):\n",
    "        self.training_losses_history = []\n",
    "        self.validation_losses_history = []\n",
    "        \n",
    "    # Main function that trains the MLP with a design matrix X and a target vector Y\n",
    "    def train(self, optimizer='sgd', momentum=False, min_iterations=500, max_iterations=5000, initial_step=1e-1,\n",
    "              batch_size=64, early_stopping=True, early_stopping_lookbehind=100, early_stopping_delta=1e-4, \n",
    "              vectorized=False, flush_history=True, verbose=True):\n",
    "        if flush_history:\n",
    "            self.loss_history_flush()\n",
    "        cpt_patience, best_validation_loss = 0, np.inf\n",
    "        iteration_number = 0\n",
    "        self.step = initial_step\n",
    "        while len(self.training_losses_history) < max_iterations:\n",
    "            iteration_number += 1\n",
    "            self.optimizer_step(\n",
    "                optimizer=optimizer, momentum=momentum, batch_size=batch_size, vectorized=vectorized\n",
    "            )\n",
    "            \n",
    "            training_loss = self.loss(self.Y_tr, self.forward(self.X_tr))\n",
    "            self.training_losses_history.append(training_loss)\n",
    "            training_accuracy = self.accuracy_on_train()\n",
    "            self.training_acc_history.append(training_accuracy)\n",
    "            validation_loss = self.loss(self.Y_val, self.forward(self.X_val))\n",
    "            self.validation_losses_history.append(validation_loss)\n",
    "            validation_accuracy = self.accuracy_on_validation()\n",
    "            self.validation_acc_history.append(validation_accuracy)\n",
    "            if iteration_number > min_iterations and early_stopping:\n",
    "                if validation_loss + early_stopping_delta < best_validation_loss:\n",
    "                    best_validation_loss = validation_loss\n",
    "                    cpt_patience = 0\n",
    "                else:\n",
    "                    cpt_patience += 1\n",
    "            if verbose:\n",
    "                msg = \"iteration number: {0}\\t training loss: {1:.4f}\\t\" + \\\n",
    "                \"validation loss: {2:.4f}\\t validation accuracy: {3:.4f}\"\n",
    "                print(msg.format(iteration_number, \n",
    "                                 training_loss, \n",
    "                                 validation_loss,\n",
    "                                 validation_accuracy))\n",
    "            if cpt_patience >= early_stopping_lookbehind:\n",
    "                break\n",
    "    \n",
    "    # Return the predicted class once the MLP has been trained\n",
    "    def predict(self, X, normalize=True):\n",
    "        if normalize:\n",
    "            X = self.scaler.transform(X)\n",
    "        if len(X.shape) == 1:\n",
    "            return np.argmax(self.forward(X))\n",
    "        else:\n",
    "            return np.argmax(self.forward(X), axis=1)\n",
    "        \n",
    "    \"\"\"\n",
    "    Metrics and plots\n",
    "    \"\"\"\n",
    "    def accuracy_on_train(self):\n",
    "        return (self.predict(self.X_tr, normalize=False) == self.Y_tr).mean()\n",
    "\n",
    "    def accuracy_on_validation(self):\n",
    "        return (self.predict(self.X_val, normalize=False) == self.Y_val).mean()\n",
    "\n",
    "    def plot_loss_history(self, add_to_title=None):\n",
    "        import warnings\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.plot(range(len(self.training_losses_history)), \n",
    "                 self.training_losses_history, label='Training loss evolution')\n",
    "        plt.plot(range(len(self.validation_losses_history)), \n",
    "                 self.validation_losses_history, label='Validation loss evolution')\n",
    "        plt.legend(fontsize=15)\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel(\"iteration number\", fontsize=15)\n",
    "        plt.ylabel(\"Cross entropy loss\", fontsize=15)\n",
    "        base_title = \"Cross entropy loss evolution during training\"\n",
    "        if not self.dropout:\n",
    "            base_title += \", no dropout penalization\"\n",
    "        else:\n",
    "            base_title += \", {:.1f} dropout penalization\"\n",
    "            base_title = base_title.format(self.dropout_rate)\n",
    "        title = base_title + \", \" + add_to_title if add_to_title else base_title\n",
    "        plt.title(title, fontsize=20)\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_validation_prediction(self, sample_id):\n",
    "        fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "        classes = np.unique(self.Y_tr)\n",
    "        dim = np.sqrt(self.X_val.shape[1]).astype(int)\n",
    "        ax0.imshow(self.scaler.inverse_transform([self.X_val[sample_id]]).reshape(dim, dim), cmap=plt.cm.gray_r,\n",
    "                   interpolation='nearest')\n",
    "        ax0.set_title(\"True image label: %d\" % self.Y_val[sample_id]);\n",
    "\n",
    "        ax1.bar(classes, self.one_hot(len(classes), self.Y_val[sample_id]), label='true')\n",
    "        ax1.bar(classes, self.forward(self.X_val[sample_id]), label='prediction', color=\"red\")\n",
    "        ax1.set_xticks(classes)\n",
    "        prediction = self.predict(self.X_val[sample_id], normalize=False)\n",
    "        ax1.set_title('Output probabilities (prediction: %d)' % prediction)\n",
    "        ax1.set_xlabel('Digit class')\n",
    "        ax1.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MultiLayerPerceptron(X, Y, hidden_size=50, activation='relu')\n",
    "mlp.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Questions:\n",
    "####  Did you succeed to train the MLP and get a high validation accuracy? <br> Display available metrics (training and validation accuracies, training and validation losses)\n",
    "- We managed to get a 93% validation accuracy, which is high.  \n",
    "- However, the training and validation losses are quite high and seems to increase during training. \n",
    "Training loss = 2093.48  \n",
    "Validation loss = 636.43  \n",
    "Validation accuracy = 0.933  \n",
    "####  Plot the prediction for a given validation sample. Is it accurate?\n",
    "- The prediction is accuracate as we managed to get a 93% accuracy\n",
    "#### Compare the full gradient descent with the SGD.\n",
    "- Using SGD as optimizer makes the training quite fast, while the batch gradient takes much longer. However, we do get similar results using the two techniques.\n",
    "#### Play with the hyper parameters you have: the hidden size, the activation function, the initial step and the batch size. <br> Comment. Don't hesitate to visualize results.\n",
    "#### Once properly implemented, compare the training using early stopping, dropout, or both of them. <br> Why are these methods useful here?\n",
    "#### Once properly implemented, compare the training using momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlp.plot_validation_prediction(sample_id=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlp.plot_loss_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III - Multiclass classification MLP with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Implement the same network architecture with Keras;\n",
    "    - First using the Sequential API\n",
    "    - Secondly using the functional API\n",
    "\n",
    "#### - Check that the Keras model can approximately reproduce the behavior of the Numpy model.\n",
    "\n",
    "#### - Compute the negative log likelihood of a sample 42 in the test set (can use `model.predict_proba`).\n",
    "\n",
    "#### - Compute the average negative log-likelihood on the full test set.\n",
    "\n",
    "#### - Compute the average negative log-likelihood  on the full training set and check that you can get the value of the loss reported by Keras.\n",
    "\n",
    "#### - Is the model overfitting or underfitting? (ensure that the model has fully converged by increasing the number of epochs to 500 or more if necessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_val, Y_tr, Y_val = train_test_split(X, Y)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1), copy=False)\n",
    "X_tr = scaler.fit_transform(X_tr)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X[0].shape[0]\n",
    "n_classes = len(np.unique(Y_tr))\n",
    "n_hidden = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with relu activation\n",
      "Train on 1347 samples, validate on 450 samples\n",
      "Epoch 1/50\n",
      "1347/1347 [==============================] - 0s 267us/sample - loss: 2.2744 - val_loss: 2.1933\n",
      "Epoch 2/50\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 2.1076 - val_loss: 2.0394\n",
      "Epoch 3/50\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 1.9441 - val_loss: 1.8777\n",
      "Epoch 4/50\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 1.7693 - val_loss: 1.7025\n",
      "Epoch 5/50\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 1.5952 - val_loss: 1.5325\n",
      "Epoch 6/50\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 1.4270 - val_loss: 1.3661\n",
      "Epoch 7/50\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 1.2649 - val_loss: 1.2091\n",
      "Epoch 8/50\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 1.1200 - val_loss: 1.0775\n",
      "Epoch 9/50\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.9914 - val_loss: 0.9598\n",
      "Epoch 10/50\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.8829 - val_loss: 0.8590\n",
      "Epoch 11/50\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 0.7929 - val_loss: 0.7860\n",
      "Epoch 12/50\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.7161 - val_loss: 0.7183\n",
      "Epoch 13/50\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.6535 - val_loss: 0.6610\n",
      "Epoch 14/50\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.5993 - val_loss: 0.6140\n",
      "Epoch 15/50\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.5541 - val_loss: 0.5728\n",
      "Epoch 16/50\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.5153 - val_loss: 0.5386\n",
      "Epoch 17/50\n",
      "1347/1347 [==============================] - 0s 41us/sample - loss: 0.4804 - val_loss: 0.5067\n",
      "Epoch 18/50\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.4496 - val_loss: 0.4784\n",
      "Epoch 19/50\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.4231 - val_loss: 0.4566\n",
      "Epoch 20/50\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 0.4003 - val_loss: 0.4345\n",
      "Epoch 21/50\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 0.3788 - val_loss: 0.4137\n",
      "Epoch 22/50\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.3598 - val_loss: 0.3958\n",
      "Epoch 23/50\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.3415 - val_loss: 0.3806\n",
      "Epoch 24/50\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.3255 - val_loss: 0.3658\n",
      "Epoch 25/50\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.3109 - val_loss: 0.3519\n",
      "Epoch 26/50\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.2978 - val_loss: 0.3404\n",
      "Epoch 27/50\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 0.2869 - val_loss: 0.3300\n",
      "Epoch 28/50\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.2776 - val_loss: 0.3203\n",
      "Epoch 29/50\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 0.2649 - val_loss: 0.3105\n",
      "Epoch 30/50\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 0.2574 - val_loss: 0.3000\n",
      "Epoch 31/50\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.2472 - val_loss: 0.2919\n",
      "Epoch 32/50\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.2395 - val_loss: 0.2851\n",
      "Epoch 33/50\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.2321 - val_loss: 0.2775\n",
      "Epoch 34/50\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 0.2252 - val_loss: 0.2726\n",
      "Epoch 35/50\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.2179 - val_loss: 0.2661\n",
      "Epoch 36/50\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.2116 - val_loss: 0.2597\n",
      "Epoch 37/50\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.2057 - val_loss: 0.2530\n",
      "Epoch 38/50\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.2011 - val_loss: 0.2480\n",
      "Epoch 39/50\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.1954 - val_loss: 0.2422\n",
      "Epoch 40/50\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.1895 - val_loss: 0.2380\n",
      "Epoch 41/50\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.1848 - val_loss: 0.2337\n",
      "Epoch 42/50\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.1812 - val_loss: 0.2291\n",
      "Epoch 43/50\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.1769 - val_loss: 0.2255\n",
      "Epoch 44/50\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.1725 - val_loss: 0.2219\n",
      "Epoch 45/50\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.1696 - val_loss: 0.2195\n",
      "Epoch 46/50\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 0.1653 - val_loss: 0.2144\n",
      "Epoch 47/50\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.1613 - val_loss: 0.2123\n",
      "Epoch 48/50\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.1592 - val_loss: 0.2092\n",
      "Epoch 49/50\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.1558 - val_loss: 0.2054\n",
      "Epoch 50/50\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 0.070 - 0s 49us/sample - loss: 0.1555 - val_loss: 0.2034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2610d3d5a20>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "activation = \"relu\"\n",
    "# activation = \"sigmoid\"\n",
    "\n",
    "print('Model with {} activation'.format(activation))\n",
    "keras_model = Sequential()\n",
    "# TODO:\n",
    "keras_model.add(Dense(n_hidden,activation = activation))\n",
    "keras_model.add(Dense(n_classes,activation = \"softmax\"))\n",
    "keras_model.compile(optimizer= 'Adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# This builds the model for the first time:\n",
    "keras_model.fit(X_tr, Y_tr,validation_data=(X_val,Y_val), epochs=50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.7477688e-35 1.2456182e-36 0.0000000e+00 0.0000000e+00 5.6247795e-35\n",
      " 0.0000000e+00 1.0000000e+00 0.0000000e+00 4.2871497e-18 0.0000000e+00]\n"
     ]
    }
   ],
   "source": [
    "predicts = keras_model.predict_proba(X)\n",
    "print(predicts[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with relu activation\n",
      "Train on 1347 samples, validate on 450 samples\n",
      "Epoch 1/5000\n",
      "1347/1347 [==============================] - 0s 193us/sample - loss: 2.3046 - val_loss: 2.2369\n",
      "Epoch 2/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 2.1714 - val_loss: 2.1147\n",
      "Epoch 3/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 2.0409 - val_loss: 1.9779\n",
      "Epoch 4/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.8909 - val_loss: 1.8247\n",
      "Epoch 5/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 1.7287 - val_loss: 1.6592\n",
      "Epoch 6/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.5620 - val_loss: 1.4959\n",
      "Epoch 7/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.3895 - val_loss: 1.3135\n",
      "Epoch 8/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.2133 - val_loss: 1.1600\n",
      "Epoch 9/5000\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 1.0502 - val_loss: 1.0062\n",
      "Epoch 10/5000\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.9112 - val_loss: 0.8784\n",
      "Epoch 11/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.7969 - val_loss: 0.7796\n",
      "Epoch 12/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.7068 - val_loss: 0.6999\n",
      "Epoch 13/5000\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 0.6338 - val_loss: 0.6330\n",
      "Epoch 14/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.5758 - val_loss: 0.5815\n",
      "Epoch 15/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.5267 - val_loss: 0.5375\n",
      "Epoch 16/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.4863 - val_loss: 0.5038\n",
      "Epoch 17/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.4519 - val_loss: 0.4705\n",
      "Epoch 18/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.4222 - val_loss: 0.4412\n",
      "Epoch 19/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.3949 - val_loss: 0.4151\n",
      "Epoch 20/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 0.3734 - val_loss: 0.3944\n",
      "Epoch 21/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.3519 - val_loss: 0.3756\n",
      "Epoch 22/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.3344 - val_loss: 0.3574\n",
      "Epoch 23/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.3172 - val_loss: 0.3406\n",
      "Epoch 24/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.3013 - val_loss: 0.3259\n",
      "Epoch 25/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.2882 - val_loss: 0.3164\n",
      "Epoch 26/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.2752 - val_loss: 0.3036\n",
      "Epoch 27/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.2656 - val_loss: 0.2930\n",
      "Epoch 28/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2542 - val_loss: 0.2834\n",
      "Epoch 29/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 0.226 - 0s 47us/sample - loss: 0.2448 - val_loss: 0.2761\n",
      "Epoch 30/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.2379 - val_loss: 0.2678\n",
      "Epoch 31/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.2288 - val_loss: 0.2616\n",
      "Epoch 32/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.2211 - val_loss: 0.2554\n",
      "Epoch 33/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.2136 - val_loss: 0.2479\n",
      "Epoch 34/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2077 - val_loss: 0.2446\n",
      "Epoch 35/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2024 - val_loss: 0.2379\n",
      "Epoch 36/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1963 - val_loss: 0.2348\n",
      "Epoch 37/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1913 - val_loss: 0.2306\n",
      "Epoch 38/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.1863 - val_loss: 0.2273\n",
      "Epoch 39/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.1824 - val_loss: 0.2264\n",
      "Epoch 40/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1778 - val_loss: 0.2194\n",
      "Epoch 41/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.1736 - val_loss: 0.2165\n",
      "Epoch 42/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1696 - val_loss: 0.2122\n",
      "Epoch 43/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1659 - val_loss: 0.2122\n",
      "Epoch 44/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 0.117 - 0s 46us/sample - loss: 0.1638 - val_loss: 0.2084\n",
      "Epoch 45/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1595 - val_loss: 0.2049\n",
      "Epoch 46/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.1570 - val_loss: 0.2042\n",
      "Epoch 47/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.1528 - val_loss: 0.2007\n",
      "Epoch 48/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 0.1507 - val_loss: 0.1961\n",
      "Epoch 49/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.1465 - val_loss: 0.1952\n",
      "Epoch 50/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.1444 - val_loss: 0.1937\n",
      "Epoch 51/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.1421 - val_loss: 0.1930\n",
      "Epoch 52/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.1414 - val_loss: 0.1893\n",
      "Epoch 53/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.1369 - val_loss: 0.1883\n",
      "Epoch 54/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1345 - val_loss: 0.1866\n",
      "Epoch 55/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1316 - val_loss: 0.1853\n",
      "Epoch 56/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1297 - val_loss: 0.1825\n",
      "Epoch 57/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1271 - val_loss: 0.1812\n",
      "Epoch 58/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1261 - val_loss: 0.1812\n",
      "Epoch 59/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1237 - val_loss: 0.1809\n",
      "Epoch 60/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 0.194 - 0s 44us/sample - loss: 0.1236 - val_loss: 0.1792\n",
      "Epoch 61/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1197 - val_loss: 0.1778\n",
      "Epoch 62/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1184 - val_loss: 0.1754\n",
      "Epoch 63/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1167 - val_loss: 0.1727\n",
      "Epoch 64/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1142 - val_loss: 0.1733\n",
      "Epoch 65/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 0.104 - 0s 45us/sample - loss: 0.1129 - val_loss: 0.1730\n",
      "Epoch 66/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1117 - val_loss: 0.1724\n",
      "Epoch 67/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1091 - val_loss: 0.1684\n",
      "Epoch 68/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.1077 - val_loss: 0.1680\n",
      "Epoch 69/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1056 - val_loss: 0.1679\n",
      "Epoch 70/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.1041 - val_loss: 0.1674\n",
      "Epoch 71/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1047 - val_loss: 0.1658\n",
      "Epoch 72/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.1020 - val_loss: 0.1658\n",
      "Epoch 73/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.1002 - val_loss: 0.1642\n",
      "Epoch 74/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0989 - val_loss: 0.1631\n",
      "Epoch 75/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0974 - val_loss: 0.1640\n",
      "Epoch 76/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0958 - val_loss: 0.1642\n",
      "Epoch 77/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.0948 - val_loss: 0.1624\n",
      "Epoch 78/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0935 - val_loss: 0.1607\n",
      "Epoch 79/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0922 - val_loss: 0.1623\n",
      "Epoch 80/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0917 - val_loss: 0.1630\n",
      "Epoch 81/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0901 - val_loss: 0.1607\n",
      "Epoch 82/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0889 - val_loss: 0.1615\n",
      "Epoch 83/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0869 - val_loss: 0.1581\n",
      "Epoch 84/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0862 - val_loss: 0.1598\n",
      "Epoch 85/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0847 - val_loss: 0.1568\n",
      "Epoch 86/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 0.107 - 0s 45us/sample - loss: 0.0833 - val_loss: 0.1576\n",
      "Epoch 87/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.0823 - val_loss: 0.1571\n",
      "Epoch 88/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0809 - val_loss: 0.1542\n",
      "Epoch 89/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0798 - val_loss: 0.1548\n",
      "Epoch 90/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 0.055 - 0s 43us/sample - loss: 0.0789 - val_loss: 0.1552\n",
      "Epoch 91/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0783 - val_loss: 0.1546\n",
      "Epoch 92/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0770 - val_loss: 0.1547\n",
      "Epoch 93/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0764 - val_loss: 0.1527\n",
      "Epoch 94/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0750 - val_loss: 0.1531\n",
      "Epoch 95/5000\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.0737 - val_loss: 0.1529\n",
      "Epoch 96/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0734 - val_loss: 0.1533\n",
      "Epoch 97/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 0.0720 - val_loss: 0.1533\n",
      "Epoch 98/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 0.0731 - val_loss: 0.1529\n",
      "Epoch 99/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 0.0702 - val_loss: 0.1531\n",
      "Epoch 100/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.0699 - val_loss: 0.1494\n",
      "Epoch 101/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0686 - val_loss: 0.1523\n",
      "Epoch 102/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0677 - val_loss: 0.1530\n",
      "Epoch 103/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 0.083 - 0s 44us/sample - loss: 0.0665 - val_loss: 0.1490\n",
      "Epoch 104/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0661 - val_loss: 0.1512\n",
      "Epoch 105/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0648 - val_loss: 0.1498\n",
      "Epoch 106/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0639 - val_loss: 0.1486\n",
      "Epoch 107/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0629 - val_loss: 0.1485\n",
      "Epoch 108/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0639 - val_loss: 0.1538\n",
      "Epoch 109/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0620 - val_loss: 0.1503\n",
      "Epoch 110/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0604 - val_loss: 0.1499\n",
      "Epoch 111/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0604 - val_loss: 0.1491\n",
      "Epoch 112/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0591 - val_loss: 0.1481\n",
      "Epoch 113/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0587 - val_loss: 0.1487\n",
      "Epoch 114/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0576 - val_loss: 0.1480\n",
      "Epoch 115/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0566 - val_loss: 0.1492\n",
      "Epoch 116/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0567 - val_loss: 0.1494\n",
      "Epoch 117/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0556 - val_loss: 0.1498\n",
      "Epoch 118/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0548 - val_loss: 0.1466\n",
      "Epoch 119/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0543 - val_loss: 0.1486\n",
      "Epoch 120/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0544 - val_loss: 0.1524\n",
      "Epoch 121/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0531 - val_loss: 0.1480\n",
      "Epoch 122/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0520 - val_loss: 0.1493\n",
      "Epoch 123/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0518 - val_loss: 0.1524\n",
      "Epoch 124/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0508 - val_loss: 0.1483\n",
      "Epoch 125/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0515 - val_loss: 0.1500\n",
      "Epoch 126/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0493 - val_loss: 0.1482\n",
      "Epoch 127/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0487 - val_loss: 0.1491\n",
      "Epoch 128/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0486 - val_loss: 0.1490\n",
      "Epoch 129/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0478 - val_loss: 0.1468\n",
      "Epoch 130/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0468 - val_loss: 0.1503\n",
      "Epoch 131/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0462 - val_loss: 0.1470\n",
      "Epoch 132/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0458 - val_loss: 0.1493\n",
      "Epoch 133/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0456 - val_loss: 0.1469\n",
      "Epoch 134/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 0.048 - 0s 45us/sample - loss: 0.0447 - val_loss: 0.1496\n",
      "Epoch 135/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0443 - val_loss: 0.1465\n",
      "Epoch 136/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0439 - val_loss: 0.1473\n",
      "Epoch 137/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0432 - val_loss: 0.1499\n",
      "Epoch 138/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0426 - val_loss: 0.1485\n",
      "Epoch 139/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0421 - val_loss: 0.1489\n",
      "Epoch 140/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0433 - val_loss: 0.1488\n",
      "Epoch 141/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0418 - val_loss: 0.1485\n",
      "Epoch 142/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0407 - val_loss: 0.1475\n",
      "Epoch 143/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0412 - val_loss: 0.1479\n",
      "Epoch 144/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0394 - val_loss: 0.1475\n",
      "Epoch 145/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0391 - val_loss: 0.1496\n",
      "Epoch 146/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 0.039 - 0s 43us/sample - loss: 0.0390 - val_loss: 0.1493\n",
      "Epoch 147/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0381 - val_loss: 0.1489\n",
      "Epoch 148/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0386 - val_loss: 0.1484\n",
      "Epoch 149/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0380 - val_loss: 0.1481\n",
      "Epoch 150/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0366 - val_loss: 0.1487\n",
      "Epoch 151/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0365 - val_loss: 0.1485\n",
      "Epoch 152/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0362 - val_loss: 0.1471\n",
      "Epoch 153/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0358 - val_loss: 0.1498\n",
      "Epoch 154/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0351 - val_loss: 0.1478\n",
      "Epoch 155/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0349 - val_loss: 0.1486\n",
      "Epoch 156/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0345 - val_loss: 0.1489\n",
      "Epoch 157/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0345 - val_loss: 0.1474\n",
      "Epoch 158/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0333 - val_loss: 0.1485\n",
      "Epoch 159/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0329 - val_loss: 0.1476\n",
      "Epoch 160/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0328 - val_loss: 0.1492\n",
      "Epoch 161/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0323 - val_loss: 0.1478\n",
      "Epoch 162/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0318 - val_loss: 0.1484\n",
      "Epoch 163/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0318 - val_loss: 0.1467\n",
      "Epoch 164/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0320 - val_loss: 0.1481\n",
      "Epoch 165/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0305 - val_loss: 0.1472\n",
      "Epoch 166/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0305 - val_loss: 0.1490\n",
      "Epoch 167/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0304 - val_loss: 0.1495\n",
      "Epoch 168/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0297 - val_loss: 0.1496\n",
      "Epoch 169/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0292 - val_loss: 0.1489\n",
      "Epoch 170/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0295 - val_loss: 0.1507\n",
      "Epoch 171/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0293 - val_loss: 0.1495\n",
      "Epoch 172/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0288 - val_loss: 0.1494\n",
      "Epoch 173/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0291 - val_loss: 0.1519\n",
      "Epoch 174/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0278 - val_loss: 0.1491\n",
      "Epoch 175/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0276 - val_loss: 0.1509\n",
      "Epoch 176/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0271 - val_loss: 0.1532\n",
      "Epoch 177/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.0266 - val_loss: 0.1496\n",
      "Epoch 178/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 0.0266 - val_loss: 0.1509\n",
      "Epoch 179/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0259 - val_loss: 0.1522\n",
      "Epoch 180/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0260 - val_loss: 0.1519\n",
      "Epoch 181/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0255 - val_loss: 0.1509\n",
      "Epoch 182/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0254 - val_loss: 0.1503\n",
      "Epoch 183/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0249 - val_loss: 0.1515\n",
      "Epoch 184/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0247 - val_loss: 0.1498\n",
      "Epoch 185/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0246 - val_loss: 0.1533\n",
      "Epoch 186/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0245 - val_loss: 0.1537\n",
      "Epoch 187/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0238 - val_loss: 0.1540\n",
      "Epoch 188/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0234 - val_loss: 0.1505\n",
      "Epoch 189/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0230 - val_loss: 0.1560\n",
      "Epoch 190/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.0236 - val_loss: 0.1556\n",
      "Epoch 191/5000\n",
      "1347/1347 [==============================] - 0s 68us/sample - loss: 0.0228 - val_loss: 0.1523\n",
      "Epoch 192/5000\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.0236 - val_loss: 0.1519\n",
      "Epoch 193/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 0.0226 - val_loss: 0.1538\n",
      "Epoch 194/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0219 - val_loss: 0.1549\n",
      "Epoch 195/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.0218 - val_loss: 0.1544\n",
      "Epoch 196/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0213 - val_loss: 0.1538\n",
      "Epoch 197/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 0.025 - 0s 49us/sample - loss: 0.0211 - val_loss: 0.1571\n",
      "Epoch 198/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.0209 - val_loss: 0.1530\n",
      "Epoch 199/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.0210 - val_loss: 0.1565\n",
      "Epoch 200/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0205 - val_loss: 0.1552\n",
      "Epoch 201/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0204 - val_loss: 0.1521\n",
      "Epoch 202/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0201 - val_loss: 0.1540\n",
      "Epoch 203/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0202 - val_loss: 0.1558\n",
      "Epoch 204/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0197 - val_loss: 0.1575\n",
      "Epoch 205/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0200 - val_loss: 0.1555\n",
      "Epoch 206/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0193 - val_loss: 0.1541\n",
      "Epoch 207/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0190 - val_loss: 0.1542\n",
      "Epoch 208/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0189 - val_loss: 0.1580\n",
      "Epoch 209/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0184 - val_loss: 0.1554\n",
      "Epoch 210/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.0195 - val_loss: 0.1495\n",
      "Epoch 211/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0183 - val_loss: 0.1527\n",
      "Epoch 212/5000\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0180 - val_loss: 0.1549\n",
      "Epoch 213/5000\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0177 - val_loss: 0.1581\n",
      "Epoch 214/5000\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 0.0177 - val_loss: 0.1571\n",
      "Epoch 215/5000\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 0.0172 - val_loss: 0.1554\n",
      "Epoch 216/5000\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.0171 - val_loss: 0.1551\n",
      "Epoch 217/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 0.0171 - val_loss: 0.1559\n",
      "Epoch 218/5000\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.0181 - val_loss: 0.1566\n",
      "Epoch 219/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0166 - val_loss: 0.1597\n",
      "Epoch 220/5000\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 0.0163 - val_loss: 0.1582\n",
      "Epoch 221/5000\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 0.0163 - val_loss: 0.1592\n",
      "Epoch 222/5000\n",
      "1347/1347 [==============================] - 0s 64us/sample - loss: 0.0160 - val_loss: 0.1587\n",
      "Epoch 223/5000\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 0.0156 - val_loss: 0.1591\n",
      "Epoch 224/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.0155 - val_loss: 0.1591\n",
      "Epoch 225/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.0152 - val_loss: 0.1590\n",
      "Epoch 226/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.0151 - val_loss: 0.1609\n",
      "Epoch 227/5000\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 0.0152 - val_loss: 0.1597\n",
      "Epoch 228/5000\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0146 - val_loss: 0.1595\n",
      "Epoch 229/5000\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.0151 - val_loss: 0.1596\n",
      "Epoch 230/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 0.0145 - val_loss: 0.1583\n",
      "Epoch 231/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 0.0143 - val_loss: 0.1603\n",
      "Epoch 232/5000\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 0.0145 - val_loss: 0.1626\n",
      "Epoch 233/5000\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.0141 - val_loss: 0.1612\n",
      "Epoch 234/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 0.0142 - val_loss: 0.1611\n",
      "Epoch 235/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 0.0136 - val_loss: 0.1625\n",
      "Epoch 236/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 0.0135 - val_loss: 0.1618\n",
      "Epoch 237/5000\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0137 - val_loss: 0.1603\n",
      "Epoch 238/5000\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0132 - val_loss: 0.1637\n",
      "Epoch 239/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 0.0134 - val_loss: 0.1641\n",
      "Epoch 240/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 0.0130 - val_loss: 0.1646\n",
      "Epoch 241/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 0.0127 - val_loss: 0.1652\n",
      "Epoch 242/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0125 - val_loss: 0.1652\n",
      "Epoch 243/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0129 - val_loss: 0.1657\n",
      "Epoch 244/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0126 - val_loss: 0.1639\n",
      "Epoch 245/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0122 - val_loss: 0.1643\n",
      "Epoch 246/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0120 - val_loss: 0.1661\n",
      "Epoch 247/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0118 - val_loss: 0.1656\n",
      "Epoch 248/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0121 - val_loss: 0.1674\n",
      "Epoch 249/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0117 - val_loss: 0.1651\n",
      "Epoch 250/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0115 - val_loss: 0.1654\n",
      "Epoch 251/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 0.007 - 0s 46us/sample - loss: 0.0115 - val_loss: 0.1642\n",
      "Epoch 252/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 0.003 - 0s 45us/sample - loss: 0.0115 - val_loss: 0.1654\n",
      "Epoch 253/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0111 - val_loss: 0.1694\n",
      "Epoch 254/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0111 - val_loss: 0.1671\n",
      "Epoch 255/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0110 - val_loss: 0.1680\n",
      "Epoch 256/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0106 - val_loss: 0.1677\n",
      "Epoch 257/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0107 - val_loss: 0.1694\n",
      "Epoch 258/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0105 - val_loss: 0.1660\n",
      "Epoch 259/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0103 - val_loss: 0.1691\n",
      "Epoch 260/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 0.0105 - val_loss: 0.1669\n",
      "Epoch 261/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0101 - val_loss: 0.1700\n",
      "Epoch 262/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 0.014 - 0s 47us/sample - loss: 0.0101 - val_loss: 0.1697\n",
      "Epoch 263/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0101 - val_loss: 0.1705\n",
      "Epoch 264/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 0.005 - 0s 46us/sample - loss: 0.0098 - val_loss: 0.1714\n",
      "Epoch 265/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0097 - val_loss: 0.1699\n",
      "Epoch 266/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0095 - val_loss: 0.1721\n",
      "Epoch 267/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0094 - val_loss: 0.1709\n",
      "Epoch 268/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0092 - val_loss: 0.1716\n",
      "Epoch 269/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0091 - val_loss: 0.1718\n",
      "Epoch 270/5000\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 0.0090 - val_loss: 0.1709\n",
      "Epoch 271/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0089 - val_loss: 0.1725\n",
      "Epoch 272/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 0.0089 - val_loss: 0.1712\n",
      "Epoch 273/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0087 - val_loss: 0.1719\n",
      "Epoch 274/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0086 - val_loss: 0.1719\n",
      "Epoch 275/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0085 - val_loss: 0.1745\n",
      "Epoch 276/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0084 - val_loss: 0.1732\n",
      "Epoch 277/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0083 - val_loss: 0.1716\n",
      "Epoch 278/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0082 - val_loss: 0.1744\n",
      "Epoch 279/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.0081 - val_loss: 0.1745\n",
      "Epoch 280/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0081 - val_loss: 0.1751\n",
      "Epoch 281/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0079 - val_loss: 0.1734\n",
      "Epoch 282/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0079 - val_loss: 0.1751\n",
      "Epoch 283/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0077 - val_loss: 0.1717\n",
      "Epoch 284/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0076 - val_loss: 0.1739\n",
      "Epoch 285/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0074 - val_loss: 0.1754\n",
      "Epoch 286/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0074 - val_loss: 0.1765\n",
      "Epoch 287/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0074 - val_loss: 0.1741\n",
      "Epoch 288/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0072 - val_loss: 0.1764\n",
      "Epoch 289/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 0.0073 - val_loss: 0.1726\n",
      "Epoch 290/5000\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.0071 - val_loss: 0.1760\n",
      "Epoch 291/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 0.0070 - val_loss: 0.1777\n",
      "Epoch 292/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 0.0070 - val_loss: 0.1789\n",
      "Epoch 293/5000\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 0.0069 - val_loss: 0.1779\n",
      "Epoch 294/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 0.0067 - val_loss: 0.1777\n",
      "Epoch 295/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 0.0068 - val_loss: 0.1791\n",
      "Epoch 296/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 0.0066 - val_loss: 0.1773\n",
      "Epoch 297/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 0.0067 - val_loss: 0.1813\n",
      "Epoch 298/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0065 - val_loss: 0.1820\n",
      "Epoch 299/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0064 - val_loss: 0.1770\n",
      "Epoch 300/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0062 - val_loss: 0.1803\n",
      "Epoch 301/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0063 - val_loss: 0.1767\n",
      "Epoch 302/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0063 - val_loss: 0.1792\n",
      "Epoch 303/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0060 - val_loss: 0.1808\n",
      "Epoch 304/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0060 - val_loss: 0.1820\n",
      "Epoch 305/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0058 - val_loss: 0.1800\n",
      "Epoch 306/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0059 - val_loss: 0.1810\n",
      "Epoch 307/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0058 - val_loss: 0.1832\n",
      "Epoch 308/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0057 - val_loss: 0.1800\n",
      "Epoch 309/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0056 - val_loss: 0.1838\n",
      "Epoch 310/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0057 - val_loss: 0.1810\n",
      "Epoch 311/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0055 - val_loss: 0.1805\n",
      "Epoch 312/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.0058 - val_loss: 0.1826\n",
      "Epoch 313/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.0053 - val_loss: 0.1830\n",
      "Epoch 314/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.0053 - val_loss: 0.1830\n",
      "Epoch 315/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0052 - val_loss: 0.1834\n",
      "Epoch 316/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0056 - val_loss: 0.1861\n",
      "Epoch 317/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0052 - val_loss: 0.1849\n",
      "Epoch 318/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.0051 - val_loss: 0.1880\n",
      "Epoch 319/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0050 - val_loss: 0.1849\n",
      "Epoch 320/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.0049 - val_loss: 0.1838\n",
      "Epoch 321/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0060 - val_loss: 0.1893\n",
      "Epoch 322/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0048 - val_loss: 0.1885\n",
      "Epoch 323/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0050 - val_loss: 0.1878\n",
      "Epoch 324/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0048 - val_loss: 0.1900\n",
      "Epoch 325/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0046 - val_loss: 0.1895\n",
      "Epoch 326/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 0.0045 - val_loss: 0.1881\n",
      "Epoch 327/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.0044 - val_loss: 0.1858\n",
      "Epoch 328/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0048 - val_loss: 0.1886\n",
      "Epoch 329/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0043 - val_loss: 0.1887\n",
      "Epoch 330/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0043 - val_loss: 0.1850\n",
      "Epoch 331/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0042 - val_loss: 0.1866\n",
      "Epoch 332/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0043 - val_loss: 0.1872\n",
      "Epoch 333/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0041 - val_loss: 0.1872\n",
      "Epoch 334/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.0041 - val_loss: 0.1903\n",
      "Epoch 335/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0041 - val_loss: 0.1889\n",
      "Epoch 336/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0041 - val_loss: 0.1914\n",
      "Epoch 337/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0040 - val_loss: 0.1888\n",
      "Epoch 338/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0039 - val_loss: 0.1892\n",
      "Epoch 339/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0038 - val_loss: 0.1897\n",
      "Epoch 340/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0039 - val_loss: 0.1885\n",
      "Epoch 341/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0038 - val_loss: 0.1899\n",
      "Epoch 342/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0038 - val_loss: 0.1880\n",
      "Epoch 343/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0037 - val_loss: 0.1919\n",
      "Epoch 344/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0036 - val_loss: 0.1899\n",
      "Epoch 345/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0036 - val_loss: 0.1932\n",
      "Epoch 346/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0036 - val_loss: 0.1931\n",
      "Epoch 347/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0035 - val_loss: 0.1917\n",
      "Epoch 348/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0035 - val_loss: 0.1910\n",
      "Epoch 349/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0034 - val_loss: 0.1899\n",
      "Epoch 350/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0034 - val_loss: 0.1912\n",
      "Epoch 351/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0033 - val_loss: 0.1916\n",
      "Epoch 352/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0033 - val_loss: 0.1923\n",
      "Epoch 353/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0032 - val_loss: 0.1924\n",
      "Epoch 354/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0033 - val_loss: 0.1916\n",
      "Epoch 355/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0031 - val_loss: 0.1933\n",
      "Epoch 356/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0032 - val_loss: 0.1935\n",
      "Epoch 357/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0032 - val_loss: 0.1928\n",
      "Epoch 358/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 0.003 - 0s 45us/sample - loss: 0.0031 - val_loss: 0.1926\n",
      "Epoch 359/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0031 - val_loss: 0.1927\n",
      "Epoch 360/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0031 - val_loss: 0.1935\n",
      "Epoch 361/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0031 - val_loss: 0.1949\n",
      "Epoch 362/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0032 - val_loss: 0.1977\n",
      "Epoch 363/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0029 - val_loss: 0.1925\n",
      "Epoch 364/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 0.002 - 0s 45us/sample - loss: 0.0029 - val_loss: 0.1925\n",
      "Epoch 365/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0029 - val_loss: 0.1954\n",
      "Epoch 366/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0028 - val_loss: 0.1957\n",
      "Epoch 367/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0028 - val_loss: 0.1956\n",
      "Epoch 368/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0027 - val_loss: 0.1966\n",
      "Epoch 369/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0027 - val_loss: 0.1955\n",
      "Epoch 370/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0027 - val_loss: 0.1937\n",
      "Epoch 371/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0026 - val_loss: 0.1966\n",
      "Epoch 372/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0026 - val_loss: 0.1976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 373/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0025 - val_loss: 0.1944\n",
      "Epoch 374/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0026 - val_loss: 0.1955\n",
      "Epoch 375/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 3.1512e-0 - 0s 44us/sample - loss: 0.0025 - val_loss: 0.1996\n",
      "Epoch 376/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0025 - val_loss: 0.1965\n",
      "Epoch 377/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0025 - val_loss: 0.1986\n",
      "Epoch 378/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0024 - val_loss: 0.1982\n",
      "Epoch 379/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.0024 - val_loss: 0.1963\n",
      "Epoch 380/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0025 - val_loss: 0.1986\n",
      "Epoch 381/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 0.001 - 0s 45us/sample - loss: 0.0023 - val_loss: 0.1999\n",
      "Epoch 382/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0023 - val_loss: 0.1991\n",
      "Epoch 383/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0023 - val_loss: 0.1993\n",
      "Epoch 384/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0022 - val_loss: 0.2012\n",
      "Epoch 385/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0023 - val_loss: 0.2038\n",
      "Epoch 386/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0022 - val_loss: 0.2012\n",
      "Epoch 387/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0021 - val_loss: 0.2028\n",
      "Epoch 388/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0021 - val_loss: 0.2026\n",
      "Epoch 389/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0021 - val_loss: 0.2035\n",
      "Epoch 390/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0021 - val_loss: 0.2034\n",
      "Epoch 391/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0021 - val_loss: 0.2040\n",
      "Epoch 392/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0020 - val_loss: 0.2040\n",
      "Epoch 393/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0021 - val_loss: 0.2034\n",
      "Epoch 394/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0020 - val_loss: 0.2033\n",
      "Epoch 395/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0019 - val_loss: 0.2030\n",
      "Epoch 396/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0020 - val_loss: 0.2041\n",
      "Epoch 397/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0020 - val_loss: 0.2063\n",
      "Epoch 398/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0019 - val_loss: 0.2051\n",
      "Epoch 399/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0019 - val_loss: 0.2050\n",
      "Epoch 400/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0018 - val_loss: 0.2061\n",
      "Epoch 401/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0018 - val_loss: 0.2052\n",
      "Epoch 402/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0018 - val_loss: 0.2073\n",
      "Epoch 403/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0018 - val_loss: 0.2060\n",
      "Epoch 404/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0017 - val_loss: 0.2069\n",
      "Epoch 405/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0017 - val_loss: 0.2085\n",
      "Epoch 406/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0018 - val_loss: 0.2080\n",
      "Epoch 407/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0017 - val_loss: 0.2071\n",
      "Epoch 408/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0016 - val_loss: 0.2101\n",
      "Epoch 409/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0016 - val_loss: 0.2072\n",
      "Epoch 410/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0016 - val_loss: 0.2081\n",
      "Epoch 411/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0016 - val_loss: 0.2078\n",
      "Epoch 412/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0016 - val_loss: 0.2114\n",
      "Epoch 413/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0015 - val_loss: 0.2103\n",
      "Epoch 414/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0015 - val_loss: 0.2116\n",
      "Epoch 415/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0015 - val_loss: 0.2087\n",
      "Epoch 416/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0015 - val_loss: 0.2120\n",
      "Epoch 417/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0015 - val_loss: 0.2095\n",
      "Epoch 418/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0015 - val_loss: 0.2118\n",
      "Epoch 419/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0014 - val_loss: 0.2118\n",
      "Epoch 420/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0017 - val_loss: 0.2142\n",
      "Epoch 421/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0015 - val_loss: 0.2135\n",
      "Epoch 422/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0014 - val_loss: 0.2122\n",
      "Epoch 423/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0014 - val_loss: 0.2117\n",
      "Epoch 424/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0014 - val_loss: 0.2121\n",
      "Epoch 425/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0014 - val_loss: 0.2145\n",
      "Epoch 426/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0013 - val_loss: 0.2136\n",
      "Epoch 427/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0013 - val_loss: 0.2131\n",
      "Epoch 428/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 5.8464e-0 - 0s 47us/sample - loss: 0.0013 - val_loss: 0.2135\n",
      "Epoch 429/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0013 - val_loss: 0.2151\n",
      "Epoch 430/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0012 - val_loss: 0.2137\n",
      "Epoch 431/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0013 - val_loss: 0.2127\n",
      "Epoch 432/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0012 - val_loss: 0.2142\n",
      "Epoch 433/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0012 - val_loss: 0.2136\n",
      "Epoch 434/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0012 - val_loss: 0.2156\n",
      "Epoch 435/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0012 - val_loss: 0.2135\n",
      "Epoch 436/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0011 - val_loss: 0.2146\n",
      "Epoch 437/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0011 - val_loss: 0.2167\n",
      "Epoch 438/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0011 - val_loss: 0.2166\n",
      "Epoch 439/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0011 - val_loss: 0.2175\n",
      "Epoch 440/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0011 - val_loss: 0.2197\n",
      "Epoch 441/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0011 - val_loss: 0.2180\n",
      "Epoch 442/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 0.001 - 0s 47us/sample - loss: 0.0010 - val_loss: 0.2163\n",
      "Epoch 443/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 0.001 - 0s 45us/sample - loss: 0.0010 - val_loss: 0.2178\n",
      "Epoch 444/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0010 - val_loss: 0.2185\n",
      "Epoch 445/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0010 - val_loss: 0.2212\n",
      "Epoch 446/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0010 - val_loss: 0.2213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 447/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 9.8669e-04 - val_loss: 0.2190\n",
      "Epoch 448/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 9.8265e-04 - val_loss: 0.2189\n",
      "Epoch 449/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 9.8756e-04 - val_loss: 0.2202\n",
      "Epoch 450/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0010 - val_loss: 0.2190\n",
      "Epoch 451/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 9.6282e-04 - val_loss: 0.2200\n",
      "Epoch 452/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 9.1753e-04 - val_loss: 0.2201\n",
      "Epoch 453/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 9.1045e-04 - val_loss: 0.2191\n",
      "Epoch 454/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 8.9951e-04 - val_loss: 0.2205\n",
      "Epoch 455/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 9.1627e-04 - val_loss: 0.2212\n",
      "Epoch 456/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 8.8265e-04 - val_loss: 0.2216\n",
      "Epoch 457/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 8.6263e-04 - val_loss: 0.2253\n",
      "Epoch 458/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 8.6461e-04 - val_loss: 0.2234\n",
      "Epoch 459/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 8.4534e-04 - val_loss: 0.2213\n",
      "Epoch 460/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 8.3920e-04 - val_loss: 0.2242\n",
      "Epoch 461/5000\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 8.2810e-04 - val_loss: 0.2233\n",
      "Epoch 462/5000\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 8.3122e-04 - val_loss: 0.2245\n",
      "Epoch 463/5000\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 7.9406e-04 - val_loss: 0.2238\n",
      "Epoch 464/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 7.8856e-04 - val_loss: 0.2250\n",
      "Epoch 465/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 7.9250e-04 - val_loss: 0.2294\n",
      "Epoch 466/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 9.0821e-04 - val_loss: 0.2245\n",
      "Epoch 467/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 7.7126e-04 - val_loss: 0.2261\n",
      "Epoch 468/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 7.8927e-04 - val_loss: 0.2248\n",
      "Epoch 469/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 7.4121e-04 - val_loss: 0.2259\n",
      "Epoch 470/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 7.2813e-04 - val_loss: 0.2265\n",
      "Epoch 471/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 7.1397e-04 - val_loss: 0.2280\n",
      "Epoch 472/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 7.3214e-04 - val_loss: 0.2286\n",
      "Epoch 473/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 6.9272e-04 - val_loss: 0.2278\n",
      "Epoch 474/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.8205e-04 - val_loss: 0.2276\n",
      "Epoch 475/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.8043e-04 - val_loss: 0.2280\n",
      "Epoch 476/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.7490e-04 - val_loss: 0.2289\n",
      "Epoch 477/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 6.5733e-04 - val_loss: 0.2296\n",
      "Epoch 478/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 6.7753e-04 - val_loss: 0.2296\n",
      "Epoch 479/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.7435e-04 - val_loss: 0.2306\n",
      "Epoch 480/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.4043e-04 - val_loss: 0.2291\n",
      "Epoch 481/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 6.4133e-04 - val_loss: 0.2289\n",
      "Epoch 482/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 6.1158e-04 - val_loss: 0.2335\n",
      "Epoch 483/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.1319e-04 - val_loss: 0.2329\n",
      "Epoch 484/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.1767e-04 - val_loss: 0.2317\n",
      "Epoch 485/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.1938e-04 - val_loss: 0.2308\n",
      "Epoch 486/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.9290e-04 - val_loss: 0.2324\n",
      "Epoch 487/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.9278e-04 - val_loss: 0.2338\n",
      "Epoch 488/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 5.7796e-04 - val_loss: 0.2341\n",
      "Epoch 489/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 5.6429e-04 - val_loss: 0.2327\n",
      "Epoch 490/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.5269e-04 - val_loss: 0.2329\n",
      "Epoch 491/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.4464e-04 - val_loss: 0.2323\n",
      "Epoch 492/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.2974e-04 - val_loss: 0.2354\n",
      "Epoch 493/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 5.4203e-04 - val_loss: 0.2400\n",
      "Epoch 494/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.4038e-04 - val_loss: 0.2389\n",
      "Epoch 495/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 5.3481e-04 - val_loss: 0.2381\n",
      "Epoch 496/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.2601e-04 - val_loss: 0.2381\n",
      "Epoch 497/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 5.0140e-04 - val_loss: 0.2377\n",
      "Epoch 498/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 4.9598e-04 - val_loss: 0.2371\n",
      "Epoch 499/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.0461e-04 - val_loss: 0.2393\n",
      "Epoch 500/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 4.9402e-04 - val_loss: 0.2378\n",
      "Epoch 501/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.7513e-04 - val_loss: 0.2377\n",
      "Epoch 502/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 7.5874e-04 - val_loss: 0.2355\n",
      "Epoch 503/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.0170e-04 - val_loss: 0.2351\n",
      "Epoch 504/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.7266e-04 - val_loss: 0.2347\n",
      "Epoch 505/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.4752e-04 - val_loss: 0.2376\n",
      "Epoch 506/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.4309e-04 - val_loss: 0.2381\n",
      "Epoch 507/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.4615e-04 - val_loss: 0.2386\n",
      "Epoch 508/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.4552e-04 - val_loss: 0.2366\n",
      "Epoch 509/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 4.2596e-04 - val_loss: 0.2368\n",
      "Epoch 510/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.1790e-04 - val_loss: 0.2381\n",
      "Epoch 511/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 4.1595e-04 - val_loss: 0.2390\n",
      "Epoch 512/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 4.1443e-04 - val_loss: 0.2396\n",
      "Epoch 513/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 4.0026e-04 - val_loss: 0.2405\n",
      "Epoch 514/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 3.9331e-04 - val_loss: 0.2398\n",
      "Epoch 515/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 3.8852e-04 - val_loss: 0.2400\n",
      "Epoch 516/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 3.9016e-04 - val_loss: 0.2411\n",
      "Epoch 517/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 3.8474e-04 - val_loss: 0.2407\n",
      "Epoch 518/5000\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 3.8671e-04 - val_loss: 0.2395\n",
      "Epoch 519/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1347/1347 [==============================] - 0s 51us/sample - loss: 4.5695e-04 - val_loss: 0.2480\n",
      "Epoch 520/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 3.7752e-04 - val_loss: 0.2480\n",
      "Epoch 521/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 3.6252e-04 - val_loss: 0.2471\n",
      "Epoch 522/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 3.6492e-04 - val_loss: 0.2429\n",
      "Epoch 523/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 3.5802e-04 - val_loss: 0.2434\n",
      "Epoch 524/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 3.5038e-04 - val_loss: 0.2444\n",
      "Epoch 525/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 3.4565e-04 - val_loss: 0.2440\n",
      "Epoch 526/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 3.4279e-04 - val_loss: 0.2452\n",
      "Epoch 527/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 3.4260e-04 - val_loss: 0.2460\n",
      "Epoch 528/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 3.3964e-04 - val_loss: 0.2433\n",
      "Epoch 529/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 3.3584e-04 - val_loss: 0.2465\n",
      "Epoch 530/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 3.1038e-0 - 0s 47us/sample - loss: 3.3118e-04 - val_loss: 0.2449\n",
      "Epoch 531/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 3.2346e-04 - val_loss: 0.2458\n",
      "Epoch 532/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 3.2713e-04 - val_loss: 0.2462\n",
      "Epoch 533/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 3.1670e-04 - val_loss: 0.2467\n",
      "Epoch 534/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 3.1986e-04 - val_loss: 0.2464\n",
      "Epoch 535/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 3.0643e-04 - val_loss: 0.2458\n",
      "Epoch 536/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 3.0643e-04 - val_loss: 0.2476\n",
      "Epoch 537/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 2.8979e-04 - val_loss: 0.2480\n",
      "Epoch 538/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 3.1765e-04 - val_loss: 0.2479\n",
      "Epoch 539/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 2.9216e-04 - val_loss: 0.2484\n",
      "Epoch 540/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 2.8231e-04 - val_loss: 0.2483\n",
      "Epoch 541/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 2.7752e-04 - val_loss: 0.2496\n",
      "Epoch 542/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 2.9327e-04 - val_loss: 0.2514\n",
      "Epoch 543/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 2.7728e-04 - val_loss: 0.2496\n",
      "Epoch 544/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 2.6983e-04 - val_loss: 0.2511\n",
      "Epoch 545/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 2.6863e-04 - val_loss: 0.2495\n",
      "Epoch 546/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 2.6653e-04 - val_loss: 0.2504\n",
      "Epoch 547/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 2.5865e-04 - val_loss: 0.2503\n",
      "Epoch 548/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 2.5864e-04 - val_loss: 0.2487\n",
      "Epoch 549/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 2.5164e-04 - val_loss: 0.2500\n",
      "Epoch 550/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 2.5239e-04 - val_loss: 0.2512\n",
      "Epoch 551/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 2.4652e-04 - val_loss: 0.2529\n",
      "Epoch 552/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 2.4659e-04 - val_loss: 0.2509\n",
      "Epoch 553/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 2.5360e-04 - val_loss: 0.2531\n",
      "Epoch 554/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 2.4135e-04 - val_loss: 0.2507\n",
      "Epoch 555/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 2.3533e-04 - val_loss: 0.2508\n",
      "Epoch 556/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 2.2771e-04 - val_loss: 0.2527\n",
      "Epoch 557/5000\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 2.2635e-04 - val_loss: 0.2506\n",
      "Epoch 558/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 2.3256e-04 - val_loss: 0.2535\n",
      "Epoch 559/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 2.1921e-04 - val_loss: 0.2533\n",
      "Epoch 560/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 2.2157e-04 - val_loss: 0.2510\n",
      "Epoch 561/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 2.1822e-04 - val_loss: 0.2536\n",
      "Epoch 562/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 2.1143e-04 - val_loss: 0.2549\n",
      "Epoch 563/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 2.0870e-04 - val_loss: 0.2547\n",
      "Epoch 564/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 2.0813e-04 - val_loss: 0.2529\n",
      "Epoch 565/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 2.0484e-04 - val_loss: 0.2531\n",
      "Epoch 566/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 2.0181e-04 - val_loss: 0.2528\n",
      "Epoch 567/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 1.9894e-04 - val_loss: 0.2534\n",
      "Epoch 568/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 1.9960e-04 - val_loss: 0.2543\n",
      "Epoch 569/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 1.9910e-04 - val_loss: 0.2546\n",
      "Epoch 570/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.9066e-04 - val_loss: 0.2573\n",
      "Epoch 571/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.9409e-04 - val_loss: 0.2566\n",
      "Epoch 572/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.8580e-04 - val_loss: 0.2565\n",
      "Epoch 573/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.8099e-04 - val_loss: 0.2574\n",
      "Epoch 574/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.8256e-04 - val_loss: 0.2562\n",
      "Epoch 575/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 1.7715e-04 - val_loss: 0.2570\n",
      "Epoch 576/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.7312e-04 - val_loss: 0.2577\n",
      "Epoch 577/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.7329e-04 - val_loss: 0.2592\n",
      "Epoch 578/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.7379e-04 - val_loss: 0.2572\n",
      "Epoch 579/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.6819e-04 - val_loss: 0.2578\n",
      "Epoch 580/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.6644e-04 - val_loss: 0.2589\n",
      "Epoch 581/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.5974e-04 - val_loss: 0.2596\n",
      "Epoch 582/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.6428e-04 - val_loss: 0.2593\n",
      "Epoch 583/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 1.7020e-04 - val_loss: 0.2579\n",
      "Epoch 584/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.6046e-04 - val_loss: 0.2601\n",
      "Epoch 585/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.6455e-04 - val_loss: 0.2597\n",
      "Epoch 586/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.4993e-04 - val_loss: 0.2605\n",
      "Epoch 587/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.5145e-04 - val_loss: 0.2599\n",
      "Epoch 588/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 1.5643e-04 - val_loss: 0.2601\n",
      "Epoch 589/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.4801e-04 - val_loss: 0.2604\n",
      "Epoch 590/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.4650e-04 - val_loss: 0.2612\n",
      "Epoch 591/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.4647e-04 - val_loss: 0.2623\n",
      "Epoch 592/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.4163e-04 - val_loss: 0.2581\n",
      "Epoch 593/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.4139e-04 - val_loss: 0.2610\n",
      "Epoch 594/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 1.3904e-04 - val_loss: 0.2604\n",
      "Epoch 595/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.3744e-04 - val_loss: 0.2648\n",
      "Epoch 596/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.3123e-04 - val_loss: 0.2642\n",
      "Epoch 597/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.2911e-04 - val_loss: 0.2619\n",
      "Epoch 598/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 1.0980e-0 - 0s 48us/sample - loss: 1.2720e-04 - val_loss: 0.2642\n",
      "Epoch 599/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.2528e-04 - val_loss: 0.2666\n",
      "Epoch 600/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 1.2596e-04 - val_loss: 0.2652\n",
      "Epoch 601/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.2973e-04 - val_loss: 0.2644\n",
      "Epoch 602/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 1.1932e-04 - val_loss: 0.2659\n",
      "Epoch 603/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 1.1957e-04 - val_loss: 0.2655\n",
      "Epoch 604/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.1560e-04 - val_loss: 0.2642\n",
      "Epoch 605/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.1338e-04 - val_loss: 0.2672\n",
      "Epoch 606/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.1396e-04 - val_loss: 0.2650\n",
      "Epoch 607/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.1235e-04 - val_loss: 0.2678\n",
      "Epoch 608/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 1.1096e-04 - val_loss: 0.2667\n",
      "Epoch 609/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.0856e-04 - val_loss: 0.2687\n",
      "Epoch 610/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 1.1028e-04 - val_loss: 0.2701\n",
      "Epoch 611/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 1.2080e-04 - val_loss: 0.2651\n",
      "Epoch 612/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.0671e-04 - val_loss: 0.2717\n",
      "Epoch 613/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.0250e-04 - val_loss: 0.2682\n",
      "Epoch 614/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.0213e-04 - val_loss: 0.2697\n",
      "Epoch 615/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 9.9877e-05 - val_loss: 0.2694\n",
      "Epoch 616/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 9.9499e-05 - val_loss: 0.2698\n",
      "Epoch 617/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.0091e-04 - val_loss: 0.2708\n",
      "Epoch 618/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 9.7332e-05 - val_loss: 0.2709\n",
      "Epoch 619/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 9.6465e-05 - val_loss: 0.2696\n",
      "Epoch 620/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 1.2268e-0 - 0s 46us/sample - loss: 9.1286e-05 - val_loss: 0.2723\n",
      "Epoch 621/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 9.2398e-05 - val_loss: 0.2710\n",
      "Epoch 622/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 8.9027e-05 - val_loss: 0.2726\n",
      "Epoch 623/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 8.9353e-05 - val_loss: 0.2716\n",
      "Epoch 624/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 8.7637e-05 - val_loss: 0.2710\n",
      "Epoch 625/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 8.6916e-05 - val_loss: 0.2730\n",
      "Epoch 626/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 8.4736e-05 - val_loss: 0.2723\n",
      "Epoch 627/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 8.5020e-05 - val_loss: 0.2717\n",
      "Epoch 628/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 8.6300e-05 - val_loss: 0.2734\n",
      "Epoch 629/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 8.2969e-05 - val_loss: 0.2730\n",
      "Epoch 630/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 8.3471e-05 - val_loss: 0.2733\n",
      "Epoch 631/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 8.3191e-05 - val_loss: 0.2732\n",
      "Epoch 632/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 7.8020e-05 - val_loss: 0.2749\n",
      "Epoch 633/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 7.5848e-05 - val_loss: 0.2739\n",
      "Epoch 634/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 8.0302e-05 - val_loss: 0.2752\n",
      "Epoch 635/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 8.3060e-05 - val_loss: 0.2755\n",
      "Epoch 636/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 7.3444e-05 - val_loss: 0.2749\n",
      "Epoch 637/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 7.1580e-05 - val_loss: 0.2769\n",
      "Epoch 638/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 7.5100e-05 - val_loss: 0.2748\n",
      "Epoch 639/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 7.0561e-05 - val_loss: 0.2771\n",
      "Epoch 640/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.8650e-05 - val_loss: 0.2759\n",
      "Epoch 641/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 2.2522e-0 - 0s 44us/sample - loss: 6.9323e-05 - val_loss: 0.2775\n",
      "Epoch 642/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 6.6315e-05 - val_loss: 0.2771\n",
      "Epoch 643/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 6.6002e-05 - val_loss: 0.2789\n",
      "Epoch 644/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 7.1122e-05 - val_loss: 0.2783\n",
      "Epoch 645/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.4733e-05 - val_loss: 0.2779\n",
      "Epoch 646/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.3412e-05 - val_loss: 0.2799\n",
      "Epoch 647/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.4885e-05 - val_loss: 0.2791\n",
      "Epoch 648/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.1505e-05 - val_loss: 0.2802\n",
      "Epoch 649/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.0401e-05 - val_loss: 0.2788\n",
      "Epoch 650/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.0943e-05 - val_loss: 0.2813\n",
      "Epoch 651/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 5.8659e-05 - val_loss: 0.2810\n",
      "Epoch 652/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 6.0260e-05 - val_loss: 0.2840\n",
      "Epoch 653/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.7507e-05 - val_loss: 0.2818\n",
      "Epoch 654/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.8195e-05 - val_loss: 0.2850\n",
      "Epoch 655/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.6275e-05 - val_loss: 0.2821\n",
      "Epoch 656/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.4928e-05 - val_loss: 0.2820\n",
      "Epoch 657/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.5622e-05 - val_loss: 0.2811\n",
      "Epoch 658/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.2710e-05 - val_loss: 0.2839\n",
      "Epoch 659/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.2656e-05 - val_loss: 0.2869\n",
      "Epoch 660/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 5.2393e-05 - val_loss: 0.2839\n",
      "Epoch 661/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.1021e-05 - val_loss: 0.2829\n",
      "Epoch 662/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.0355e-05 - val_loss: 0.2861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 663/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 4.9458e-05 - val_loss: 0.2859\n",
      "Epoch 664/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.8902e-05 - val_loss: 0.2854\n",
      "Epoch 665/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 4.7823e-05 - val_loss: 0.2852\n",
      "Epoch 666/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 4.7423e-05 - val_loss: 0.2882\n",
      "Epoch 667/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 2.2717e-0 - 0s 47us/sample - loss: 4.6843e-05 - val_loss: 0.2882\n",
      "Epoch 668/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.6720e-05 - val_loss: 0.2879\n",
      "Epoch 669/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 4.7461e-05 - val_loss: 0.2846\n",
      "Epoch 670/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.5809e-05 - val_loss: 0.2847\n",
      "Epoch 671/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 8.0540e-0 - 0s 45us/sample - loss: 4.4704e-05 - val_loss: 0.2866\n",
      "Epoch 672/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.4269e-05 - val_loss: 0.2875\n",
      "Epoch 673/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 4.5598e-05 - val_loss: 0.2836\n",
      "Epoch 674/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.4571e-05 - val_loss: 0.2899\n",
      "Epoch 675/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.1160e-05 - val_loss: 0.2880\n",
      "Epoch 676/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.2006e-05 - val_loss: 0.2914\n",
      "Epoch 677/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.1777e-05 - val_loss: 0.2896\n",
      "Epoch 678/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 4.0210e-05 - val_loss: 0.2898\n",
      "Epoch 679/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 3.8927e-05 - val_loss: 0.2909\n",
      "Epoch 680/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 3.8317e-05 - val_loss: 0.2912\n",
      "Epoch 681/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 3.8383e-05 - val_loss: 0.2910\n",
      "Epoch 682/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 3.7916e-05 - val_loss: 0.2926\n",
      "Epoch 683/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 3.6715e-05 - val_loss: 0.2915\n",
      "Epoch 684/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 3.8264e-05 - val_loss: 0.2943\n",
      "Epoch 685/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 3.5342e-05 - val_loss: 0.2935\n",
      "Epoch 686/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 3.7290e-05 - val_loss: 0.2921\n",
      "Epoch 687/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 3.4495e-05 - val_loss: 0.2937\n",
      "Epoch 688/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 3.3884e-05 - val_loss: 0.2902\n",
      "Epoch 689/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 3.3456e-05 - val_loss: 0.2942\n",
      "Epoch 690/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 3.2677e-05 - val_loss: 0.2952\n",
      "Epoch 691/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 3.2797e-05 - val_loss: 0.2929\n",
      "Epoch 692/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 3.2113e-05 - val_loss: 0.2945\n",
      "Epoch 693/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 3.1907e-05 - val_loss: 0.2942\n",
      "Epoch 694/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 3.1723e-05 - val_loss: 0.2967\n",
      "Epoch 695/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 3.0680e-05 - val_loss: 0.2964\n",
      "Epoch 696/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 2.9840e-05 - val_loss: 0.2956\n",
      "Epoch 697/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 2.9451e-05 - val_loss: 0.2959\n",
      "Epoch 698/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 2.9053e-05 - val_loss: 0.2957\n",
      "Epoch 699/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 2.9954e-05 - val_loss: 0.2975\n",
      "Epoch 700/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 2.8385e-05 - val_loss: 0.2964\n",
      "Epoch 701/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 2.8621e-05 - val_loss: 0.2969\n",
      "Epoch 702/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 2.8840e-05 - val_loss: 0.2969\n",
      "Epoch 703/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 2.7977e-05 - val_loss: 0.2969\n",
      "Epoch 704/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 2.6784e-05 - val_loss: 0.2970\n",
      "Epoch 705/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 2.5948e-05 - val_loss: 0.2976\n",
      "Epoch 706/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 1.1555e-0 - 0s 45us/sample - loss: 2.5446e-05 - val_loss: 0.3008\n",
      "Epoch 707/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 2.6786e-05 - val_loss: 0.2995\n",
      "Epoch 708/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 2.5761e-05 - val_loss: 0.3017\n",
      "Epoch 709/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 2.5680e-05 - val_loss: 0.2980\n",
      "Epoch 710/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 2.4151e-05 - val_loss: 0.2991\n",
      "Epoch 711/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 2.3858e-05 - val_loss: 0.3037\n",
      "Epoch 712/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 2.4308e-05 - val_loss: 0.3026\n",
      "Epoch 713/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 2.4996e-05 - val_loss: 0.3006\n",
      "Epoch 714/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 2.3400e-05 - val_loss: 0.3027\n",
      "Epoch 715/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 2.3508e-05 - val_loss: 0.3036\n",
      "Epoch 716/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 2.2834e-05 - val_loss: 0.3030\n",
      "Epoch 717/5000\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 2.2466e-05 - val_loss: 0.3056\n",
      "Epoch 718/5000\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 2.1860e-05 - val_loss: 0.3036\n",
      "Epoch 719/5000\n",
      "1347/1347 [==============================] - 0s 63us/sample - loss: 2.1228e-05 - val_loss: 0.3053\n",
      "Epoch 720/5000\n",
      "1347/1347 [==============================] - 0s 65us/sample - loss: 2.1530e-05 - val_loss: 0.3026\n",
      "Epoch 721/5000\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 2.0947e-05 - val_loss: 0.3054\n",
      "Epoch 722/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 2.0108e-05 - val_loss: 0.3051\n",
      "Epoch 723/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 2.1049e-05 - val_loss: 0.3083\n",
      "Epoch 724/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 1.9977e-05 - val_loss: 0.3055\n",
      "Epoch 725/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 1.9162e-05 - val_loss: 0.3071\n",
      "Epoch 726/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 1.9529e-05 - val_loss: 0.3065\n",
      "Epoch 727/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.9108e-05 - val_loss: 0.3073\n",
      "Epoch 728/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.8844e-05 - val_loss: 0.3085\n",
      "Epoch 729/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 1.8206e-05 - val_loss: 0.3067\n",
      "Epoch 730/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.9355e-05 - val_loss: 0.3063\n",
      "Epoch 731/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 7.7149e-0 - 0s 45us/sample - loss: 2.1731e-05 - val_loss: 0.3091\n",
      "Epoch 732/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 1.7664e-05 - val_loss: 0.3102\n",
      "Epoch 733/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 1.7409e-05 - val_loss: 0.3068\n",
      "Epoch 734/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1347/1347 [==============================] - 0s 46us/sample - loss: 2.3213e-05 - val_loss: 0.3227\n",
      "Epoch 735/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 2.4749e-0 - 0s 45us/sample - loss: 1.7472e-05 - val_loss: 0.3147\n",
      "Epoch 736/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.6944e-05 - val_loss: 0.3127\n",
      "Epoch 737/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 1.6602e-05 - val_loss: 0.3114\n",
      "Epoch 738/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.5562e-05 - val_loss: 0.3121\n",
      "Epoch 739/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.5745e-05 - val_loss: 0.3141\n",
      "Epoch 740/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 1.5628e-05 - val_loss: 0.3141\n",
      "Epoch 741/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 1.5349e-05 - val_loss: 0.3113\n",
      "Epoch 742/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 1.4671e-05 - val_loss: 0.3121\n",
      "Epoch 743/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.6250e-05 - val_loss: 0.3154\n",
      "Epoch 744/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.4618e-05 - val_loss: 0.3124\n",
      "Epoch 745/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.4278e-05 - val_loss: 0.3130\n",
      "Epoch 746/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 1.3949e-05 - val_loss: 0.3148\n",
      "Epoch 747/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.4436e-05 - val_loss: 0.3136\n",
      "Epoch 748/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 1.7776e-0 - 0s 46us/sample - loss: 1.4553e-05 - val_loss: 0.3099\n",
      "Epoch 749/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.3784e-05 - val_loss: 0.3147\n",
      "Epoch 750/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 1.3218e-05 - val_loss: 0.3143\n",
      "Epoch 751/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.3254e-05 - val_loss: 0.3167\n",
      "Epoch 752/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 1.3136e-05 - val_loss: 0.3181\n",
      "Epoch 753/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.2760e-05 - val_loss: 0.3161\n",
      "Epoch 754/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 1.2821e-05 - val_loss: 0.3165\n",
      "Epoch 755/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 1.3360e-05 - val_loss: 0.3156\n",
      "Epoch 756/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.2468e-05 - val_loss: 0.3172\n",
      "Epoch 757/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 1.2024e-05 - val_loss: 0.3173\n",
      "Epoch 758/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.1822e-05 - val_loss: 0.3170\n",
      "Epoch 759/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.1400e-05 - val_loss: 0.3198\n",
      "Epoch 760/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.1686e-05 - val_loss: 0.3186\n",
      "Epoch 761/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 1.1185e-05 - val_loss: 0.3196\n",
      "Epoch 762/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.2054e-05 - val_loss: 0.3156\n",
      "Epoch 763/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.1120e-05 - val_loss: 0.3157\n",
      "Epoch 764/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.1084e-05 - val_loss: 0.3199\n",
      "Epoch 765/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 1.1092e-05 - val_loss: 0.3202\n",
      "Epoch 766/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.0652e-05 - val_loss: 0.3210\n",
      "Epoch 767/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.0513e-05 - val_loss: 0.3203\n",
      "Epoch 768/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.0719e-05 - val_loss: 0.3229\n",
      "Epoch 769/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.0355e-05 - val_loss: 0.3236\n",
      "Epoch 770/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 1.0144e-05 - val_loss: 0.3195\n",
      "Epoch 771/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.0245e-05 - val_loss: 0.3242\n",
      "Epoch 772/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 9.6252e-06 - val_loss: 0.3227\n",
      "Epoch 773/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 9.4795e-06 - val_loss: 0.3250\n",
      "Epoch 774/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 9.4641e-06 - val_loss: 0.3241\n",
      "Epoch 775/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 9.4540e-06 - val_loss: 0.3244\n",
      "Epoch 776/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 9.4031e-06 - val_loss: 0.3259\n",
      "Epoch 777/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 1.0591e-05 - val_loss: 0.3276\n",
      "Epoch 778/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 8.8944e-06 - val_loss: 0.3260\n",
      "Epoch 779/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 8.7151e-06 - val_loss: 0.3257\n",
      "Epoch 780/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 8.7855e-06 - val_loss: 0.3258\n",
      "Epoch 781/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 8.4371e-06 - val_loss: 0.3272\n",
      "Epoch 782/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 8.3171e-06 - val_loss: 0.3248\n",
      "Epoch 783/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 8.8093e-06 - val_loss: 0.3277\n",
      "Epoch 784/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 8.2015e-06 - val_loss: 0.3274\n",
      "Epoch 785/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 8.0088e-06 - val_loss: 0.3276\n",
      "Epoch 786/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 7.8927e-06 - val_loss: 0.3288\n",
      "Epoch 787/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 7.8215e-06 - val_loss: 0.3270\n",
      "Epoch 788/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 7.7760e-06 - val_loss: 0.3293\n",
      "Epoch 789/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 7.5156e-06 - val_loss: 0.3290\n",
      "Epoch 790/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 7.3483e-06 - val_loss: 0.3289\n",
      "Epoch 791/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 7.1618e-06 - val_loss: 0.3297\n",
      "Epoch 792/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 7.2199e-06 - val_loss: 0.3315\n",
      "Epoch 793/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 7.0148e-06 - val_loss: 0.3304\n",
      "Epoch 794/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 7.0050e-06 - val_loss: 0.3304\n",
      "Epoch 795/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 7.1664e-06 - val_loss: 0.3284\n",
      "Epoch 796/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.7358e-06 - val_loss: 0.3312\n",
      "Epoch 797/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 6.6356e-06 - val_loss: 0.3295\n",
      "Epoch 798/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.6202e-06 - val_loss: 0.3311\n",
      "Epoch 799/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.4968e-06 - val_loss: 0.3302\n",
      "Epoch 800/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.4170e-06 - val_loss: 0.3316\n",
      "Epoch 801/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 1.3709e-0 - 0s 46us/sample - loss: 6.2771e-06 - val_loss: 0.3333\n",
      "Epoch 802/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.2941e-06 - val_loss: 0.3352\n",
      "Epoch 803/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.0383e-06 - val_loss: 0.3338\n",
      "Epoch 804/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.0252e-06 - val_loss: 0.3334\n",
      "Epoch 805/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 5.9949e-06 - val_loss: 0.3355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 806/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.7683e-06 - val_loss: 0.3352\n",
      "Epoch 807/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 5.7097e-06 - val_loss: 0.3355\n",
      "Epoch 808/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.7149e-06 - val_loss: 0.3375\n",
      "Epoch 809/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.7295e-06 - val_loss: 0.3376\n",
      "Epoch 810/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.7346e-06 - val_loss: 0.3375\n",
      "Epoch 811/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 2.3469e-0 - 0s 45us/sample - loss: 5.6503e-06 - val_loss: 0.3364\n",
      "Epoch 812/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 5.2840e-06 - val_loss: 0.3384\n",
      "Epoch 813/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.3525e-06 - val_loss: 0.3379\n",
      "Epoch 814/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 5.2398e-06 - val_loss: 0.3389\n",
      "Epoch 815/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.0236e-06 - val_loss: 0.3384\n",
      "Epoch 816/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 4.9193e-06 - val_loss: 0.3398\n",
      "Epoch 817/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 4.9454e-06 - val_loss: 0.3396\n",
      "Epoch 818/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.8084e-06 - val_loss: 0.3378\n",
      "Epoch 819/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 4.8445e-06 - val_loss: 0.3395\n",
      "Epoch 820/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.7025e-06 - val_loss: 0.3395\n",
      "Epoch 821/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 4.9682e-06 - val_loss: 0.3404\n",
      "Epoch 822/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 2.9317e-0 - 0s 46us/sample - loss: 4.6908e-06 - val_loss: 0.3390\n",
      "Epoch 823/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.4789e-06 - val_loss: 0.3412\n",
      "Epoch 824/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 4.3620e-06 - val_loss: 0.3395\n",
      "Epoch 825/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.4990e-06 - val_loss: 0.3468\n",
      "Epoch 826/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.3645e-06 - val_loss: 0.3438\n",
      "Epoch 827/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.2225e-06 - val_loss: 0.3430\n",
      "Epoch 828/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.1497e-06 - val_loss: 0.3438\n",
      "Epoch 829/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.0943e-06 - val_loss: 0.3438\n",
      "Epoch 830/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.0498e-06 - val_loss: 0.3420\n",
      "Epoch 831/5000\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 3.9584e-06 - val_loss: 0.3456\n",
      "Epoch 832/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 3.9461e-06 - val_loss: 0.3456\n",
      "Epoch 833/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 3.8678e-06 - val_loss: 0.3453\n",
      "Epoch 834/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 9.9607e-0 - 0s 45us/sample - loss: 3.9194e-06 - val_loss: 0.3455\n",
      "Epoch 835/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 3.9744e-06 - val_loss: 0.3451\n",
      "Epoch 836/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 3.7002e-06 - val_loss: 0.3448\n",
      "Epoch 837/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 3.6851e-06 - val_loss: 0.3459\n",
      "Epoch 838/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 3.5514e-06 - val_loss: 0.3456\n",
      "Epoch 839/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 3.4640e-06 - val_loss: 0.3449\n",
      "Epoch 840/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.2970e-06 - val_loss: 0.3517\n",
      "Epoch 841/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 3.3997e-06 - val_loss: 0.3510\n",
      "Epoch 842/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 3.3403e-06 - val_loss: 0.3484\n",
      "Epoch 843/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 3.4273e-06 - val_loss: 0.3495\n",
      "Epoch 844/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 3.2286e-06 - val_loss: 0.3495\n",
      "Epoch 845/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 3.3046e-06 - val_loss: 0.3477\n",
      "Epoch 846/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 3.1786e-06 - val_loss: 0.3478\n",
      "Epoch 847/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 3.0841e-06 - val_loss: 0.3501\n",
      "Epoch 848/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 3.0524e-06 - val_loss: 0.3467\n",
      "Epoch 849/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 3.0114e-06 - val_loss: 0.3514\n",
      "Epoch 850/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 2.8967e-06 - val_loss: 0.3479\n",
      "Epoch 851/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 2.8497e-06 - val_loss: 0.3490\n",
      "Epoch 852/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 2.8594e-06 - val_loss: 0.3505\n",
      "Epoch 853/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 2.8672e-06 - val_loss: 0.3522\n",
      "Epoch 854/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 2.7898e-06 - val_loss: 0.3556\n",
      "Epoch 855/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 4.2840e-0 - 0s 45us/sample - loss: 2.9322e-06 - val_loss: 0.3549\n",
      "Epoch 856/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 2.7172e-06 - val_loss: 0.3540\n",
      "Epoch 857/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 3.0939e-06 - val_loss: 0.3535\n",
      "Epoch 858/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 2.9619e-06 - val_loss: 0.3570\n",
      "Epoch 859/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 2.5887e-06 - val_loss: 0.3549\n",
      "Epoch 860/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 2.5026e-06 - val_loss: 0.3535\n",
      "Epoch 861/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 2.5200e-06 - val_loss: 0.3562\n",
      "Epoch 862/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 3.9071e-06 - val_loss: 0.3510\n",
      "Epoch 863/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 3.8593e-0 - 0s 47us/sample - loss: 2.6310e-06 - val_loss: 0.3529\n",
      "Epoch 864/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 2.4614e-06 - val_loss: 0.3530\n",
      "Epoch 865/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 2.3650e-06 - val_loss: 0.3527\n",
      "Epoch 866/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 2.2436e-06 - val_loss: 0.3520\n",
      "Epoch 867/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 2.2469e-06 - val_loss: 0.3530\n",
      "Epoch 868/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 2.2413e-06 - val_loss: 0.3545\n",
      "Epoch 869/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 2.1805e-06 - val_loss: 0.3546\n",
      "Epoch 870/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 2.1118e-06 - val_loss: 0.3547\n",
      "Epoch 871/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 2.0891e-06 - val_loss: 0.3569\n",
      "Epoch 872/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 2.1100e-06 - val_loss: 0.3584\n",
      "Epoch 873/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 2.0587e-06 - val_loss: 0.3569\n",
      "Epoch 874/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 2.0981e-06 - val_loss: 0.3575\n",
      "Epoch 875/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 2.0191e-06 - val_loss: 0.3592\n",
      "Epoch 876/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 1.9674e-06 - val_loss: 0.3576\n",
      "Epoch 877/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.9566e-06 - val_loss: 0.3578\n",
      "Epoch 878/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 2.0725e-06 - val_loss: 0.3589\n",
      "Epoch 879/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 1.8759e-06 - val_loss: 0.3583\n",
      "Epoch 880/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 1.8608e-06 - val_loss: 0.3576\n",
      "Epoch 881/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.8158e-06 - val_loss: 0.3604\n",
      "Epoch 882/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.7999e-06 - val_loss: 0.3610\n",
      "Epoch 883/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.7609e-06 - val_loss: 0.3606\n",
      "Epoch 884/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 1.7611e-06 - val_loss: 0.3604\n",
      "Epoch 885/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.7521e-06 - val_loss: 0.3602\n",
      "Epoch 886/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 1.7064e-06 - val_loss: 0.3619\n",
      "Epoch 887/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.7425e-06 - val_loss: 0.3631\n",
      "Epoch 888/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.9468e-06 - val_loss: 0.3638\n",
      "Epoch 889/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 1.6469e-06 - val_loss: 0.3581\n",
      "Epoch 890/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.6099e-06 - val_loss: 0.3635\n",
      "Epoch 891/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.5917e-06 - val_loss: 0.3628\n",
      "Epoch 892/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 1.2442e-0 - 0s 48us/sample - loss: 1.5727e-06 - val_loss: 0.3631\n",
      "Epoch 893/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 1.5626e-06 - val_loss: 0.3641\n",
      "Epoch 894/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.5074e-06 - val_loss: 0.3641\n",
      "Epoch 895/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.4616e-06 - val_loss: 0.3635\n",
      "Epoch 896/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 1.4762e-06 - val_loss: 0.3657\n",
      "Epoch 897/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.4311e-06 - val_loss: 0.3652\n",
      "Epoch 898/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 1.4621e-06 - val_loss: 0.3662\n",
      "Epoch 899/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 1.4012e-06 - val_loss: 0.3651\n",
      "Epoch 900/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 1.3747e-06 - val_loss: 0.3677\n",
      "Epoch 901/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.3457e-06 - val_loss: 0.3653\n",
      "Epoch 902/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.6340e-06 - val_loss: 0.3741\n",
      "Epoch 903/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.3483e-06 - val_loss: 0.3712\n",
      "Epoch 904/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.3276e-06 - val_loss: 0.3695\n",
      "Epoch 905/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 1.2906e-06 - val_loss: 0.3708\n",
      "Epoch 906/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.2939e-06 - val_loss: 0.3710\n",
      "Epoch 907/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 1.2591e-06 - val_loss: 0.3717\n",
      "Epoch 908/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 1.3665e-06 - val_loss: 0.3735\n",
      "Epoch 909/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 1.2073e-06 - val_loss: 0.3704\n",
      "Epoch 910/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.1670e-06 - val_loss: 0.3704\n",
      "Epoch 911/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 1.2425e-06 - val_loss: 0.3722\n",
      "Epoch 912/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 1.1421e-06 - val_loss: 0.3718\n",
      "Epoch 913/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.1370e-06 - val_loss: 0.3691\n",
      "Epoch 914/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.1501e-06 - val_loss: 0.3718\n",
      "Epoch 915/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.1113e-06 - val_loss: 0.3713\n",
      "Epoch 916/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.1042e-06 - val_loss: 0.3714\n",
      "Epoch 917/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.0555e-06 - val_loss: 0.3722\n",
      "Epoch 918/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.0745e-06 - val_loss: 0.3723\n",
      "Epoch 919/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 1.0643e-06 - val_loss: 0.3735\n",
      "Epoch 920/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.0794e-06 - val_loss: 0.3760\n",
      "Epoch 921/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 1.0192e-06 - val_loss: 0.3735\n",
      "Epoch 922/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 9.9703e-07 - val_loss: 0.3752\n",
      "Epoch 923/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 9.7411e-07 - val_loss: 0.3748\n",
      "Epoch 924/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 9.6845e-07 - val_loss: 0.3763\n",
      "Epoch 925/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 9.6084e-07 - val_loss: 0.3774\n",
      "Epoch 926/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 9.3411e-07 - val_loss: 0.3759\n",
      "Epoch 927/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 9.2535e-07 - val_loss: 0.3781\n",
      "Epoch 928/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 9.3039e-07 - val_loss: 0.3785\n",
      "Epoch 929/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 9.1579e-07 - val_loss: 0.3785\n",
      "Epoch 930/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 8.8039e-07 - val_loss: 0.3770\n",
      "Epoch 931/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 8.7897e-07 - val_loss: 0.3783\n",
      "Epoch 932/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 8.8128e-07 - val_loss: 0.3803\n",
      "Epoch 933/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 1.7769e-0 - 0s 47us/sample - loss: 8.5543e-07 - val_loss: 0.3794\n",
      "Epoch 934/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 8.5057e-07 - val_loss: 0.3789\n",
      "Epoch 935/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 8.3136e-07 - val_loss: 0.3786\n",
      "Epoch 936/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 8.1092e-07 - val_loss: 0.3799\n",
      "Epoch 937/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 7.9579e-07 - val_loss: 0.3790\n",
      "Epoch 938/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 8.0561e-07 - val_loss: 0.3798\n",
      "Epoch 939/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 7.7508e-07 - val_loss: 0.3827\n",
      "Epoch 940/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 7.7322e-07 - val_loss: 0.3803\n",
      "Epoch 941/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 7.7145e-07 - val_loss: 0.3819\n",
      "Epoch 942/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 7.3437e-07 - val_loss: 0.3811\n",
      "Epoch 943/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 7.3030e-07 - val_loss: 0.3833\n",
      "Epoch 944/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 7.3941e-07 - val_loss: 0.3819\n",
      "Epoch 945/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 5.7742e-0 - 0s 46us/sample - loss: 7.2286e-07 - val_loss: 0.3819\n",
      "Epoch 946/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 7.1516e-07 - val_loss: 0.3839\n",
      "Epoch 947/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.9684e-07 - val_loss: 0.3842\n",
      "Epoch 948/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 6.7189e-07 - val_loss: 0.3846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 949/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 6.6100e-07 - val_loss: 0.3846\n",
      "Epoch 950/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 7.1180e-07 - val_loss: 0.3827\n",
      "Epoch 951/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.4764e-07 - val_loss: 0.3854\n",
      "Epoch 952/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.3852e-07 - val_loss: 0.3844\n",
      "Epoch 953/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 6.3985e-07 - val_loss: 0.3868\n",
      "Epoch 954/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.0684e-07 - val_loss: 0.3850\n",
      "Epoch 955/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 6.2295e-07 - val_loss: 0.3853\n",
      "Epoch 956/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.1596e-07 - val_loss: 0.3857\n",
      "Epoch 957/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.0312e-07 - val_loss: 0.3876\n",
      "Epoch 958/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 5.8135e-07 - val_loss: 0.3873\n",
      "Epoch 959/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.7613e-07 - val_loss: 0.3856\n",
      "Epoch 960/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.3551e-07 - val_loss: 0.3833\n",
      "Epoch 961/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.7153e-07 - val_loss: 0.3882\n",
      "Epoch 962/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.5489e-07 - val_loss: 0.3858\n",
      "Epoch 963/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.4383e-07 - val_loss: 0.3876\n",
      "Epoch 964/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 5.3817e-07 - val_loss: 0.3893\n",
      "Epoch 965/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 5.3578e-07 - val_loss: 0.3898\n",
      "Epoch 966/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 5.1224e-07 - val_loss: 0.3881\n",
      "Epoch 967/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.0896e-07 - val_loss: 0.3895\n",
      "Epoch 968/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 5.0064e-07 - val_loss: 0.3892\n",
      "Epoch 969/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.0657e-07 - val_loss: 0.3918\n",
      "Epoch 970/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.7905e-07 - val_loss: 0.3905\n",
      "Epoch 971/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 7.1153e-0 - 0s 50us/sample - loss: 4.8179e-07 - val_loss: 0.3913\n",
      "Epoch 972/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 4.6728e-07 - val_loss: 0.3914\n",
      "Epoch 973/5000\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 4.6542e-07 - val_loss: 0.3921\n",
      "Epoch 974/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 4.5489e-07 - val_loss: 0.3938\n",
      "Epoch 975/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 4.4763e-07 - val_loss: 0.3925\n",
      "Epoch 976/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 4.5648e-07 - val_loss: 0.3921\n",
      "Epoch 977/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 4.7214e-07 - val_loss: 0.3960\n",
      "Epoch 978/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 4.7533e-07 - val_loss: 0.3927\n",
      "Epoch 979/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 4.3975e-07 - val_loss: 0.3933\n",
      "Epoch 980/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.2338e-07 - val_loss: 0.3970\n",
      "Epoch 981/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 4.2037e-07 - val_loss: 0.3948\n",
      "Epoch 982/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.1170e-07 - val_loss: 0.3941\n",
      "Epoch 983/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 4.0807e-07 - val_loss: 0.3966\n",
      "Epoch 984/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 3.0920e-0 - 0s 45us/sample - loss: 4.1798e-07 - val_loss: 0.3966\n",
      "Epoch 985/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 3.9329e-07 - val_loss: 0.3976\n",
      "Epoch 986/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 3.8798e-07 - val_loss: 0.3984\n",
      "Epoch 987/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 3.8356e-07 - val_loss: 0.3982\n",
      "Epoch 988/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 3.9887e-07 - val_loss: 0.3987\n",
      "Epoch 989/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 3.7453e-07 - val_loss: 0.4018\n",
      "Epoch 990/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 3.7426e-07 - val_loss: 0.4017\n",
      "Epoch 991/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 3.7533e-07 - val_loss: 0.3978\n",
      "Epoch 992/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 3.6347e-07 - val_loss: 0.3996\n",
      "Epoch 993/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 3.4807e-07 - val_loss: 0.3985\n",
      "Epoch 994/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 3.9055e-07 - val_loss: 0.4019\n",
      "Epoch 995/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 3.4656e-07 - val_loss: 0.4023\n",
      "Epoch 996/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 3.3993e-07 - val_loss: 0.3999\n",
      "Epoch 997/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 3.5152e-07 - val_loss: 0.3996\n",
      "Epoch 998/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 3.3099e-07 - val_loss: 0.4007\n",
      "Epoch 999/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 3.1586e-07 - val_loss: 0.4028\n",
      "Epoch 1000/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 3.1417e-07 - val_loss: 0.4021\n",
      "Epoch 1001/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 3.1072e-07 - val_loss: 0.4037\n",
      "Epoch 1002/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 3.1214e-07 - val_loss: 0.4018\n",
      "Epoch 1003/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 3.0090e-07 - val_loss: 0.4020\n",
      "Epoch 1004/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 2.9568e-07 - val_loss: 0.4023\n",
      "Epoch 1005/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 2.9435e-07 - val_loss: 0.4031\n",
      "Epoch 1006/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 2.9249e-07 - val_loss: 0.4023\n",
      "Epoch 1007/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 2.8957e-07 - val_loss: 0.4046\n",
      "Epoch 1008/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 2.8010e-07 - val_loss: 0.4033\n",
      "Epoch 1009/5000\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 2.7904e-07 - val_loss: 0.4044\n",
      "Epoch 1010/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 2.7869e-07 - val_loss: 0.4024\n",
      "Epoch 1011/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 2.7541e-07 - val_loss: 0.4061\n",
      "Epoch 1012/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 2.6798e-07 - val_loss: 0.4057\n",
      "Epoch 1013/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 2.6638e-07 - val_loss: 0.4064\n",
      "Epoch 1014/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 2.6939e-07 - val_loss: 0.4072\n",
      "Epoch 1015/5000\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 2.5638e-07 - val_loss: 0.4070\n",
      "Epoch 1016/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 2.4824e-07 - val_loss: 0.4066\n",
      "Epoch 1017/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 2.5798e-07 - val_loss: 0.4085\n",
      "Epoch 1018/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 2.4656e-07 - val_loss: 0.4063\n",
      "Epoch 1019/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 2.4364e-07 - val_loss: 0.4066\n",
      "Epoch 1020/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 2.4612e-07 - val_loss: 0.4058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1021/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 2.3992e-07 - val_loss: 0.4081\n",
      "Epoch 1022/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 2.3028e-07 - val_loss: 0.4089\n",
      "Epoch 1023/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 2.3585e-07 - val_loss: 0.4081\n",
      "Epoch 1024/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 2.2904e-07 - val_loss: 0.4081\n",
      "Epoch 1025/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 2.2267e-07 - val_loss: 0.4077\n",
      "Epoch 1026/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 2.3736e-07 - val_loss: 0.4103\n",
      "Epoch 1027/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 2.7107e-07 - val_loss: 0.4120\n",
      "Epoch 1028/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 2.2028e-07 - val_loss: 0.4108\n",
      "Epoch 1029/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 2.1629e-07 - val_loss: 0.4108\n",
      "Epoch 1030/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 3.4645e-0 - 0s 45us/sample - loss: 2.0789e-07 - val_loss: 0.4094\n",
      "Epoch 1031/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 2.1125e-07 - val_loss: 0.4130\n",
      "Epoch 1032/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.9930e-07 - val_loss: 0.4099\n",
      "Epoch 1033/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 1.1176e-0 - 0s 44us/sample - loss: 2.0815e-07 - val_loss: 0.4129\n",
      "Epoch 1034/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.9780e-07 - val_loss: 0.4146\n",
      "Epoch 1035/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.9620e-07 - val_loss: 0.4120\n",
      "Epoch 1036/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 1.9488e-07 - val_loss: 0.4152\n",
      "Epoch 1037/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.8983e-07 - val_loss: 0.4123\n",
      "Epoch 1038/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.9045e-07 - val_loss: 0.4146\n",
      "Epoch 1039/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.8257e-07 - val_loss: 0.4150\n",
      "Epoch 1040/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.8965e-07 - val_loss: 0.4143\n",
      "Epoch 1041/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.8355e-07 - val_loss: 0.4145\n",
      "Epoch 1042/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.8116e-07 - val_loss: 0.4147\n",
      "Epoch 1043/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 1.7638e-07 - val_loss: 0.4147\n",
      "Epoch 1044/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.7408e-07 - val_loss: 0.4141\n",
      "Epoch 1045/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 1.7125e-07 - val_loss: 0.4171\n",
      "Epoch 1046/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 1.6992e-07 - val_loss: 0.4163\n",
      "Epoch 1047/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 1.6833e-07 - val_loss: 0.4172\n",
      "Epoch 1048/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 1.6240e-07 - val_loss: 0.4152\n",
      "Epoch 1049/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.6169e-07 - val_loss: 0.4184\n",
      "Epoch 1050/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.6195e-07 - val_loss: 0.4163\n",
      "Epoch 1051/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.6594e-07 - val_loss: 0.4148\n",
      "Epoch 1052/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 1.6868e-07 - val_loss: 0.4188\n",
      "Epoch 1053/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.5718e-07 - val_loss: 0.4191\n",
      "Epoch 1054/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 1.5372e-07 - val_loss: 0.4190\n",
      "Epoch 1055/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.4921e-07 - val_loss: 0.4197\n",
      "Epoch 1056/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.4709e-07 - val_loss: 0.4187\n",
      "Epoch 1057/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 1.3784e-0 - 0s 46us/sample - loss: 1.4797e-07 - val_loss: 0.4192\n",
      "Epoch 1058/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 1.4266e-07 - val_loss: 0.4197\n",
      "Epoch 1059/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.4116e-07 - val_loss: 0.4190\n",
      "Epoch 1060/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 1.4063e-07 - val_loss: 0.4206\n",
      "Epoch 1061/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.3824e-07 - val_loss: 0.4237\n",
      "Epoch 1062/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.4213e-07 - val_loss: 0.4231\n",
      "Epoch 1063/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.3815e-07 - val_loss: 0.4233\n",
      "Epoch 1064/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.3611e-07 - val_loss: 0.4227\n",
      "Epoch 1065/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.3425e-07 - val_loss: 0.4245\n",
      "Epoch 1066/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.3558e-07 - val_loss: 0.4232\n",
      "Epoch 1067/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.3027e-07 - val_loss: 0.4250\n",
      "Epoch 1068/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 1.5337e-07 - val_loss: 0.4241\n",
      "Epoch 1069/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 1.2655e-07 - val_loss: 0.4240\n",
      "Epoch 1070/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 1.2417e-07 - val_loss: 0.4234\n",
      "Epoch 1071/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 1.2301e-07 - val_loss: 0.4241\n",
      "Epoch 1072/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 1.2266e-07 - val_loss: 0.4253\n",
      "Epoch 1073/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 1.1956e-07 - val_loss: 0.4252\n",
      "Epoch 1074/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.1850e-07 - val_loss: 0.4247\n",
      "Epoch 1075/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 1.1673e-07 - val_loss: 0.4260\n",
      "Epoch 1076/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.1983e-07 - val_loss: 0.4256\n",
      "Epoch 1077/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.1416e-07 - val_loss: 0.4244\n",
      "Epoch 1078/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.1540e-07 - val_loss: 0.4253\n",
      "Epoch 1079/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.1160e-07 - val_loss: 0.4269\n",
      "Epoch 1080/5000\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 1.1124e-07 - val_loss: 0.4259\n",
      "Epoch 1081/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 1.0735e-07 - val_loss: 0.4254\n",
      "Epoch 1082/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 1.0779e-07 - val_loss: 0.4268\n",
      "Epoch 1083/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 1.0461e-07 - val_loss: 0.4262\n",
      "Epoch 1084/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 1.0434e-07 - val_loss: 0.4261\n",
      "Epoch 1085/5000\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 1.0496e-07 - val_loss: 0.4263\n",
      "Epoch 1086/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 1.0248e-07 - val_loss: 0.4255\n",
      "Epoch 1087/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 1.1548e-0 - 0s 50us/sample - loss: 1.0107e-07 - val_loss: 0.4269\n",
      "Epoch 1088/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 1.0098e-07 - val_loss: 0.4268\n",
      "Epoch 1089/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 9.6730e-08 - val_loss: 0.4277\n",
      "Epoch 1090/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 9.7969e-08 - val_loss: 0.4285\n",
      "Epoch 1091/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 9.7261e-08 - val_loss: 0.4292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1092/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 9.4695e-08 - val_loss: 0.4287\n",
      "Epoch 1093/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 9.5403e-08 - val_loss: 0.4266\n",
      "Epoch 1094/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 9.3987e-08 - val_loss: 0.4289\n",
      "Epoch 1095/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 9.2571e-08 - val_loss: 0.4273\n",
      "Epoch 1096/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 9.0712e-08 - val_loss: 0.4278\n",
      "Epoch 1097/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 9.0004e-08 - val_loss: 0.4275\n",
      "Epoch 1098/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 8.8057e-08 - val_loss: 0.4290\n",
      "Epoch 1099/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 1.0431e-0 - 0s 52us/sample - loss: 9.1066e-08 - val_loss: 0.4301\n",
      "Epoch 1100/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 8.6287e-08 - val_loss: 0.4304\n",
      "Epoch 1101/5000\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 8.4429e-08 - val_loss: 0.4292\n",
      "Epoch 1102/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 8.4517e-08 - val_loss: 0.4330\n",
      "Epoch 1103/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 8.4517e-08 - val_loss: 0.4296\n",
      "Epoch 1104/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 8.2924e-08 - val_loss: 0.4312\n",
      "Epoch 1105/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 8.2482e-08 - val_loss: 0.4314\n",
      "Epoch 1106/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 8.9031e-08 - val_loss: 0.4338\n",
      "Epoch 1107/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 8.0004e-08 - val_loss: 0.4335\n",
      "Epoch 1108/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 7.9650e-08 - val_loss: 0.4332\n",
      "Epoch 1109/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 7.9030e-08 - val_loss: 0.4323\n",
      "Epoch 1110/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 7.9650e-08 - val_loss: 0.4349\n",
      "Epoch 1111/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 7.7083e-08 - val_loss: 0.4329\n",
      "Epoch 1112/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 9.7084e-08 - val_loss: 0.4387\n",
      "Epoch 1113/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 2.6077e-0 - 0s 45us/sample - loss: 7.8588e-08 - val_loss: 0.4324\n",
      "Epoch 1114/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 7.4340e-08 - val_loss: 0.4341\n",
      "Epoch 1115/5000\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 8.1685e-08 - val_loss: 0.4365\n",
      "Epoch 1116/5000\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 7.2835e-08 - val_loss: 0.4363\n",
      "Epoch 1117/5000\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 7.4959e-08 - val_loss: 0.4361\n",
      "Epoch 1118/5000\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 7.0977e-08 - val_loss: 0.43650\n",
      "Epoch 1119/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 7.1065e-08 - val_loss: 0.4353\n",
      "Epoch 1120/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 7.3986e-08 - val_loss: 0.4378\n",
      "Epoch 1121/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 6.8410e-08 - val_loss: 0.4359\n",
      "Epoch 1122/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 7.1331e-08 - val_loss: 0.4387\n",
      "Epoch 1123/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.6729e-08 - val_loss: 0.4387\n",
      "Epoch 1124/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.6463e-08 - val_loss: 0.4390\n",
      "Epoch 1125/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 6.4605e-08 - val_loss: 0.4395\n",
      "Epoch 1126/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.5401e-08 - val_loss: 0.4380\n",
      "Epoch 1127/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 6.4693e-08 - val_loss: 0.4382\n",
      "Epoch 1128/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 6.2923e-08 - val_loss: 0.4393\n",
      "Epoch 1129/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 6.2304e-08 - val_loss: 0.4380\n",
      "Epoch 1130/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.3100e-08 - val_loss: 0.4390\n",
      "Epoch 1131/5000\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 6.1507e-08 - val_loss: 0.4400\n",
      "Epoch 1132/5000\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 6.1596e-08 - val_loss: 0.4382\n",
      "Epoch 1133/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.5224e-08 - val_loss: 0.4376\n",
      "Epoch 1134/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 6.1773e-08 - val_loss: 0.4405\n",
      "Epoch 1135/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 6.0357e-08 - val_loss: 0.4398\n",
      "Epoch 1136/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.1419e-08 - val_loss: 0.4391\n",
      "Epoch 1137/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.1419e-08 - val_loss: 0.4408\n",
      "Epoch 1138/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.8675e-08 - val_loss: 0.4396\n",
      "Epoch 1139/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 5.5224e-08 - val_loss: 0.4398\n",
      "Epoch 1140/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 5.6020e-08 - val_loss: 0.4412\n",
      "Epoch 1141/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 5.5666e-08 - val_loss: 0.4422\n",
      "Epoch 1142/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 5.5755e-08 - val_loss: 0.4407\n",
      "Epoch 1143/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.7702e-08 - val_loss: 0.4421\n",
      "Epoch 1144/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 5.5312e-08 - val_loss: 0.4407\n",
      "Epoch 1145/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.3631e-08 - val_loss: 0.4408\n",
      "Epoch 1146/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 5.4604e-08 - val_loss: 0.4414\n",
      "Epoch 1147/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.2569e-08 - val_loss: 0.4429\n",
      "Epoch 1148/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.2038e-08 - val_loss: 0.4443\n",
      "Epoch 1149/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 5.1330e-08 - val_loss: 0.4449\n",
      "Epoch 1150/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.3188e-08 - val_loss: 0.4444\n",
      "Epoch 1151/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.0356e-08 - val_loss: 0.4439\n",
      "Epoch 1152/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 5.1330e-08 - val_loss: 0.4444\n",
      "Epoch 1153/5000\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 4.9206e-08 - val_loss: 0.4456\n",
      "Epoch 1154/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 5.0091e-08 - val_loss: 0.4451\n",
      "Epoch 1155/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 4.9206e-08 - val_loss: 0.4468\n",
      "Epoch 1156/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 7.0780e-0 - 0s 50us/sample - loss: 4.9029e-08 - val_loss: 0.4463\n",
      "Epoch 1157/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 4.7790e-08 - val_loss: 0.4466\n",
      "Epoch 1158/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 4.8321e-08 - val_loss: 0.4467\n",
      "Epoch 1159/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 4.9560e-08 - val_loss: 0.4466\n",
      "Epoch 1160/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 4.6462e-08 - val_loss: 0.4466\n",
      "Epoch 1161/5000\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 4.5931e-08 - val_loss: 0.4455\n",
      "Epoch 1162/5000\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 4.6197e-08 - val_loss: 0.4458\n",
      "Epoch 1163/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1347/1347 [==============================] - 0s 64us/sample - loss: 4.4427e-08 - val_loss: 0.4466\n",
      "Epoch 1164/5000\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 4.6462e-08 - val_loss: 0.4462\n",
      "Epoch 1165/5000\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 4.3984e-08 - val_loss: 0.4460\n",
      "Epoch 1166/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.4338e-08 - val_loss: 0.4475\n",
      "Epoch 1167/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 4.3630e-08 - val_loss: 0.4468\n",
      "Epoch 1168/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 4.3365e-08 - val_loss: 0.4474\n",
      "Epoch 1169/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.1772e-08 - val_loss: 0.4461\n",
      "Epoch 1170/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.2214e-08 - val_loss: 0.4475\n",
      "Epoch 1171/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.1152e-08 - val_loss: 0.4483\n",
      "Epoch 1172/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.2922e-08 - val_loss: 0.4496\n",
      "Epoch 1173/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 4.1772e-08 - val_loss: 0.4489\n",
      "Epoch 1174/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.0975e-08 - val_loss: 0.4498\n",
      "Epoch 1175/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 4.2126e-08 - val_loss: 0.4494\n",
      "Epoch 1176/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.1595e-08 - val_loss: 0.4492\n",
      "Epoch 1177/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 3.9559e-08 - val_loss: 0.4496\n",
      "Epoch 1178/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 3.9117e-08 - val_loss: 0.4495\n",
      "Epoch 1179/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 3.8497e-08 - val_loss: 0.4500\n",
      "Epoch 1180/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 2.9802e-0 - 0s 48us/sample - loss: 3.9382e-08 - val_loss: 0.4488\n",
      "Epoch 1181/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 3.9294e-08 - val_loss: 0.4502\n",
      "Epoch 1182/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 3.7878e-08 - val_loss: 0.4513\n",
      "Epoch 1183/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 3.9382e-08 - val_loss: 0.4502\n",
      "Epoch 1184/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 3.7966e-08 - val_loss: 0.4504\n",
      "Epoch 1185/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.1772e-08 - val_loss: 0.4493\n",
      "Epoch 1186/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 3.8851e-08 - val_loss: 0.4509\n",
      "Epoch 1187/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 3.5931e-08 - val_loss: 0.4507\n",
      "Epoch 1188/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 3.6462e-08 - val_loss: 0.4520\n",
      "Epoch 1189/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 3.5223e-08 - val_loss: 0.4523\n",
      "Epoch 1190/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 3.5842e-08 - val_loss: 0.4518\n",
      "Epoch 1191/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 3.5488e-08 - val_loss: 0.4522\n",
      "Epoch 1192/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 3.5488e-08 - val_loss: 0.4527\n",
      "Epoch 1193/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 3.5488e-08 - val_loss: 0.4513\n",
      "Epoch 1194/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 3.4957e-08 - val_loss: 0.4537\n",
      "Epoch 1195/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 3.4603e-08 - val_loss: 0.4525\n",
      "Epoch 1196/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 3.5134e-08 - val_loss: 0.4518\n",
      "Epoch 1197/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 3.4603e-08 - val_loss: 0.4525\n",
      "Epoch 1198/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 3.3364e-08 - val_loss: 0.4524\n",
      "Epoch 1199/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 3.2745e-08 - val_loss: 0.4535\n",
      "Epoch 1200/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 3.3453e-08 - val_loss: 0.4532\n",
      "Epoch 1201/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 3.2833e-08 - val_loss: 0.4545\n",
      "Epoch 1202/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 3.2391e-08 - val_loss: 0.4540\n",
      "Epoch 1203/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 3.2568e-08 - val_loss: 0.4544\n",
      "Epoch 1204/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 3.2302e-08 - val_loss: 0.4551\n",
      "Epoch 1205/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 3.0886e-08 - val_loss: 0.4543\n",
      "Epoch 1206/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 3.1948e-08 - val_loss: 0.4559\n",
      "Epoch 1207/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 3.0267e-08 - val_loss: 0.4539\n",
      "Epoch 1208/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 3.0355e-08 - val_loss: 0.4555\n",
      "Epoch 1209/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 3.0444e-08 - val_loss: 0.4552\n",
      "Epoch 1210/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 3.0178e-08 - val_loss: 0.4548\n",
      "Epoch 1211/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 3.0886e-08 - val_loss: 0.4548\n",
      "Epoch 1212/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 3.0001e-08 - val_loss: 0.4560\n",
      "Epoch 1213/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 2.9913e-08 - val_loss: 0.4558\n",
      "Epoch 1214/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 2.8939e-08 - val_loss: 0.4564\n",
      "Epoch 1215/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 2.9913e-08 - val_loss: 0.4559\n",
      "Epoch 1216/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 2.9470e-08 - val_loss: 0.4554\n",
      "Epoch 1217/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 2.9736e-08 - val_loss: 0.4566\n",
      "Epoch 1218/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 2.9382e-08 - val_loss: 0.4565\n",
      "Epoch 1219/5000\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 2.8143e-08 - val_loss: 0.4564\n",
      "Epoch 1220/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 2.7877e-08 - val_loss: 0.4570\n",
      "Epoch 1221/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 2.9116e-08 - val_loss: 0.4549\n",
      "Epoch 1222/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 2.8939e-08 - val_loss: 0.4563\n",
      "Epoch 1223/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 4.0978e-0 - 0s 52us/sample - loss: 2.7435e-08 - val_loss: 0.4580\n",
      "Epoch 1224/5000\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 2.7258e-08 - val_loss: 0.4584\n",
      "Epoch 1225/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 2.6550e-08 - val_loss: 0.4570\n",
      "Epoch 1226/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 2.6992e-08 - val_loss: 0.4585\n",
      "Epoch 1227/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 2.6373e-08 - val_loss: 0.4579\n",
      "Epoch 1228/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 2.6196e-08 - val_loss: 0.4585\n",
      "Epoch 1229/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 2.6196e-08 - val_loss: 0.4576\n",
      "Epoch 1230/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 3.7253e-0 - 0s 45us/sample - loss: 2.7789e-08 - val_loss: 0.4578\n",
      "Epoch 1231/5000\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 2.6815e-08 - val_loss: 0.4578\n",
      "Epoch 1232/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 2.5842e-08 - val_loss: 0.4572\n",
      "Epoch 1233/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 2.5399e-08 - val_loss: 0.4580\n",
      "Epoch 1234/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1347/1347 [==============================] - 0s 46us/sample - loss: 2.6904e-08 - val_loss: 0.4590\n",
      "Epoch 1235/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 2.5399e-08 - val_loss: 0.4584\n",
      "Epoch 1236/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 2.4426e-08 - val_loss: 0.4598\n",
      "Epoch 1237/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 2.5311e-08 - val_loss: 0.4596\n",
      "Epoch 1238/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 2.5134e-08 - val_loss: 0.4601\n",
      "Epoch 1239/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 2.4249e-08 - val_loss: 0.4599\n",
      "Epoch 1240/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 2.4160e-08 - val_loss: 0.4601\n",
      "Epoch 1241/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 2.4957e-08 - val_loss: 0.4601\n",
      "Epoch 1242/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 2.3718e-08 - val_loss: 0.4614\n",
      "Epoch 1243/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 2.4691e-08 - val_loss: 0.4607\n",
      "Epoch 1244/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 2.4780e-08 - val_loss: 0.4611\n",
      "Epoch 1245/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 2.3895e-08 - val_loss: 0.4605\n",
      "Epoch 1246/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 2.3452e-08 - val_loss: 0.4603\n",
      "Epoch 1247/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 2.3629e-08 - val_loss: 0.4608\n",
      "Epoch 1248/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 2.3098e-08 - val_loss: 0.4622\n",
      "Epoch 1249/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 2.2833e-08 - val_loss: 0.4611\n",
      "Epoch 1250/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 2.2744e-08 - val_loss: 0.4609\n",
      "Epoch 1251/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 2.2833e-08 - val_loss: 0.4622\n",
      "Epoch 1252/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 2.3187e-08 - val_loss: 0.4614\n",
      "Epoch 1253/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 2.2302e-08 - val_loss: 0.4623\n",
      "Epoch 1254/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 2.3187e-08 - val_loss: 0.4630\n",
      "Epoch 1255/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 2.2036e-08 - val_loss: 0.4618\n",
      "Epoch 1256/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 2.2036e-08 - val_loss: 0.4622\n",
      "Epoch 1257/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 2.1240e-08 - val_loss: 0.4623\n",
      "Epoch 1258/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 2.1948e-08 - val_loss: 0.4620\n",
      "Epoch 1259/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 2.1328e-08 - val_loss: 0.4624\n",
      "Epoch 1260/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 2.1505e-08 - val_loss: 0.4636\n",
      "Epoch 1261/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 2.1682e-08 - val_loss: 0.4618\n",
      "Epoch 1262/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 2.1240e-08 - val_loss: 0.4622\n",
      "Epoch 1263/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 2.1240e-08 - val_loss: 0.4619\n",
      "Epoch 1264/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 2.2302e-08 - val_loss: 0.4635\n",
      "Epoch 1265/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 2.1505e-08 - val_loss: 0.4631\n",
      "Epoch 1266/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 2.2036e-08 - val_loss: 0.4637\n",
      "Epoch 1267/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 2.1417e-08 - val_loss: 0.4641\n",
      "Epoch 1268/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 2.1859e-08 - val_loss: 0.4653\n",
      "Epoch 1269/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 2.0974e-08 - val_loss: 0.4647\n",
      "Epoch 1270/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 2.0886e-08 - val_loss: 0.4644\n",
      "Epoch 1271/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 2.0178e-08 - val_loss: 0.4656\n",
      "Epoch 1272/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 2.0089e-08 - val_loss: 0.4652\n",
      "Epoch 1273/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.9381e-08 - val_loss: 0.4651\n",
      "Epoch 1274/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.9824e-08 - val_loss: 0.4653\n",
      "Epoch 1275/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.9647e-08 - val_loss: 0.4649\n",
      "Epoch 1276/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.9735e-08 - val_loss: 0.4646\n",
      "Epoch 1277/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.9647e-08 - val_loss: 0.4659\n",
      "Epoch 1278/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 1.9470e-08 - val_loss: 0.4659\n",
      "Epoch 1279/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 1.9381e-08 - val_loss: 0.4666\n",
      "Epoch 1280/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.9558e-08 - val_loss: 0.4668\n",
      "Epoch 1281/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.9558e-08 - val_loss: 0.4670\n",
      "Epoch 1282/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.9204e-08 - val_loss: 0.4664\n",
      "Epoch 1283/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 1.8939e-08 - val_loss: 0.4668\n",
      "Epoch 1284/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 1.8850e-08 - val_loss: 0.4658\n",
      "Epoch 1285/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 1.8850e-08 - val_loss: 0.4672\n",
      "Epoch 1286/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 1.9027e-08 - val_loss: 0.4678\n",
      "Epoch 1287/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 1.8054e-08 - val_loss: 0.4677\n",
      "Epoch 1288/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.8850e-08 - val_loss: 0.4681\n",
      "Epoch 1289/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 1.8585e-08 - val_loss: 0.4675\n",
      "Epoch 1290/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 1.8585e-08 - val_loss: 0.4680\n",
      "Epoch 1291/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 1.8408e-08 - val_loss: 0.4681\n",
      "Epoch 1292/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 1.8231e-08 - val_loss: 0.4676\n",
      "Epoch 1293/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.7523e-08 - val_loss: 0.4678\n",
      "Epoch 1294/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.7611e-08 - val_loss: 0.4677\n",
      "Epoch 1295/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.7346e-08 - val_loss: 0.4680\n",
      "Epoch 1296/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.7877e-08 - val_loss: 0.4688\n",
      "Epoch 1297/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.7700e-08 - val_loss: 0.4689\n",
      "Epoch 1298/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.8142e-08 - val_loss: 0.4683\n",
      "Epoch 1299/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.7434e-08 - val_loss: 0.4684\n",
      "Epoch 1300/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 1.7611e-08 - val_loss: 0.4694\n",
      "Epoch 1301/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.7434e-08 - val_loss: 0.4693\n",
      "Epoch 1302/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.7169e-08 - val_loss: 0.4697\n",
      "Epoch 1303/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 1.6815e-08 - val_loss: 0.4721\n",
      "Epoch 1304/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 1.7434e-08 - val_loss: 0.4719\n",
      "Epoch 1305/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 1.6992e-08 - val_loss: 0.4707\n",
      "Epoch 1306/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1347/1347 [==============================] - 0s 52us/sample - loss: 1.6638e-08 - val_loss: 0.4707\n",
      "Epoch 1307/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 1.6726e-08 - val_loss: 0.4700\n",
      "Epoch 1308/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 1.8231e-08 - val_loss: 0.4691\n",
      "Epoch 1309/5000\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 1.6903e-08 - val_loss: 0.4706\n",
      "Epoch 1310/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 1.6638e-08 - val_loss: 0.4727\n",
      "Epoch 1311/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 1.6018e-08 - val_loss: 0.4716\n",
      "Epoch 1312/5000\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 1.5841e-08 - val_loss: 0.4712\n",
      "Epoch 1313/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 1.6018e-08 - val_loss: 0.4715\n",
      "Epoch 1314/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 1.5930e-08 - val_loss: 0.4717\n",
      "Epoch 1315/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 1.6284e-08 - val_loss: 0.4717\n",
      "Epoch 1316/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 1.7169e-08 - val_loss: 0.4709\n",
      "Epoch 1317/5000\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 1.5753e-08 - val_loss: 0.4714\n",
      "Epoch 1318/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 1.5310e-08 - val_loss: 0.4723\n",
      "Epoch 1319/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 1.5310e-08 - val_loss: 0.4718\n",
      "Epoch 1320/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 1.5664e-08 - val_loss: 0.4721\n",
      "Epoch 1321/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 1.5576e-08 - val_loss: 0.4723\n",
      "Epoch 1322/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 1.6195e-08 - val_loss: 0.4723\n",
      "Epoch 1323/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 1.5399e-08 - val_loss: 0.4727\n",
      "Epoch 1324/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.5753e-08 - val_loss: 0.4723\n",
      "Epoch 1325/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 7.4506e-0 - 0s 45us/sample - loss: 1.5133e-08 - val_loss: 0.4725\n",
      "Epoch 1326/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.5045e-08 - val_loss: 0.4727\n",
      "Epoch 1327/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.5222e-08 - val_loss: 0.4723\n",
      "Epoch 1328/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.4602e-08 - val_loss: 0.4718\n",
      "Epoch 1329/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.4514e-08 - val_loss: 0.4719\n",
      "Epoch 1330/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 1.8626e-0 - 0s 49us/sample - loss: 1.4779e-08 - val_loss: 0.4719\n",
      "Epoch 1331/5000\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 1.4691e-08 - val_loss: 0.4720\n",
      "Epoch 1332/5000\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 1.5222e-08 - val_loss: 0.4724\n",
      "Epoch 1333/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 1.4691e-08 - val_loss: 0.4728\n",
      "Epoch 1334/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 1.4337e-08 - val_loss: 0.4721\n",
      "Epoch 1335/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 1.5045e-08 - val_loss: 0.4723\n",
      "Epoch 1336/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 1.3983e-08 - val_loss: 0.4720\n",
      "Epoch 1337/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 1.4071e-08 - val_loss: 0.4723\n",
      "Epoch 1338/5000\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 1.4248e-08 - val_loss: 0.4719\n",
      "Epoch 1339/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 1.4514e-08 - val_loss: 0.4720\n",
      "Epoch 1340/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 1.4160e-08 - val_loss: 0.4726\n",
      "Epoch 1341/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 1.4337e-08 - val_loss: 0.4721\n",
      "Epoch 1342/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 1.3894e-08 - val_loss: 0.4723\n",
      "Epoch 1343/5000\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 1.3540e-08 - val_loss: 0.4726\n",
      "Epoch 1344/5000\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 1.3983e-08 - val_loss: 0.47170\n",
      "Epoch 1345/5000\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 1.4248e-08 - val_loss: 0.4727\n",
      "Epoch 1346/5000\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 1.3894e-08 - val_loss: 0.4737\n",
      "Epoch 1347/5000\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 1.4071e-08 - val_loss: 0.4739\n",
      "Epoch 1348/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 1.3894e-08 - val_loss: 0.4736\n",
      "Epoch 1349/5000\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 1.4337e-08 - val_loss: 0.4748\n",
      "Epoch 1350/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 1.4248e-08 - val_loss: 0.4744\n",
      "Epoch 1351/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.3983e-08 - val_loss: 0.4750\n",
      "Epoch 1352/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 1.3983e-08 - val_loss: 0.4744\n",
      "Epoch 1353/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.3983e-08 - val_loss: 0.4748\n",
      "Epoch 1354/5000\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 1.3540e-08 - val_loss: 0.4747\n",
      "Epoch 1355/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 1.3806e-08 - val_loss: 0.4747\n",
      "Epoch 1356/5000\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 1.3098e-08 - val_loss: 0.4742\n",
      "Epoch 1357/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 1.3452e-08 - val_loss: 0.4747\n",
      "Epoch 1358/5000\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 1.3009e-08 - val_loss: 0.4750\n",
      "Epoch 1359/5000\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 1.3363e-08 - val_loss: 0.4741\n",
      "Epoch 1360/5000\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 1.3275e-08 - val_loss: 0.4755\n",
      "Epoch 1361/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 1.3009e-08 - val_loss: 0.4757\n",
      "Epoch 1362/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 1.3363e-08 - val_loss: 0.4772\n",
      "Epoch 1363/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 1.3363e-08 - val_loss: 0.4771\n",
      "Epoch 1364/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 1.2921e-08 - val_loss: 0.4765\n",
      "Epoch 1365/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 1.3098e-08 - val_loss: 0.4766\n",
      "Epoch 1366/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 1.1176e-0 - 0s 45us/sample - loss: 1.2301e-08 - val_loss: 0.4767\n",
      "Epoch 1367/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.2921e-08 - val_loss: 0.4763\n",
      "Epoch 1368/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 1.2744e-08 - val_loss: 0.4760\n",
      "Epoch 1369/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 1.2744e-08 - val_loss: 0.4762\n",
      "Epoch 1370/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 1.2124e-08 - val_loss: 0.4753\n",
      "Epoch 1371/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 1.2567e-08 - val_loss: 0.4759\n",
      "Epoch 1372/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 1.2301e-08 - val_loss: 0.4760\n",
      "Epoch 1373/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 1.2567e-08 - val_loss: 0.4762\n",
      "Epoch 1374/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 1.2478e-08 - val_loss: 0.4762\n",
      "Epoch 1375/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.2478e-08 - val_loss: 0.4766\n",
      "Epoch 1376/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 1.2567e-08 - val_loss: 0.4770\n",
      "Epoch 1377/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.3009e-08 - val_loss: 0.4773\n",
      "Epoch 1378/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 1.4901e-0 - 0s 45us/sample - loss: 1.2301e-08 - val_loss: 0.4768\n",
      "Epoch 1379/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.2036e-08 - val_loss: 0.4770\n",
      "Epoch 1380/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 1.2390e-08 - val_loss: 0.4756\n",
      "Epoch 1381/5000\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 1.2124e-08 - val_loss: 0.4766\n",
      "Epoch 1382/5000\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 1.2567e-08 - val_loss: 0.4762\n",
      "Epoch 1383/5000\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 1.1859e-08 - val_loss: 0.4774\n",
      "Epoch 1384/5000\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 1.2036e-08 - val_loss: 0.4761\n",
      "Epoch 1385/5000\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 1.2124e-08 - val_loss: 0.4770\n",
      "Epoch 1386/5000\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 1.2567e-08 - val_loss: 0.4757\n",
      "Epoch 1387/5000\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 1.3186e-08 - val_loss: 0.4771\n",
      "Epoch 1388/5000\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 1.1770e-08 - val_loss: 0.4766\n",
      "Epoch 1389/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 1.1416e-08 - val_loss: 0.4768\n",
      "Epoch 1390/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 1.1593e-08 - val_loss: 0.4772\n",
      "Epoch 1391/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 1.2390e-08 - val_loss: 0.4770\n",
      "Epoch 1392/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 1.8626e-0 - 0s 44us/sample - loss: 1.1859e-08 - val_loss: 0.4767\n",
      "Epoch 1393/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.1505e-08 - val_loss: 0.4770\n",
      "Epoch 1394/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 1.1682e-08 - val_loss: 0.4764\n",
      "Epoch 1395/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.1682e-08 - val_loss: 0.4781\n",
      "Epoch 1396/5000\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 1.1770e-08 - val_loss: 0.4784\n",
      "Epoch 1397/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 1.1505e-08 - val_loss: 0.4768\n",
      "Epoch 1398/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 1.1328e-08 - val_loss: 0.4773\n",
      "Epoch 1399/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 1.2390e-08 - val_loss: 0.4779\n",
      "Epoch 1400/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 1.1328e-08 - val_loss: 0.4779\n",
      "Epoch 1401/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 1.0531e-08 - val_loss: 0.4783\n",
      "Epoch 1402/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.2390e-08 - val_loss: 0.4801\n",
      "Epoch 1403/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.1593e-08 - val_loss: 0.4783\n",
      "Epoch 1404/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 1.0797e-08 - val_loss: 0.4782\n",
      "Epoch 1405/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.1770e-08 - val_loss: 0.4785\n",
      "Epoch 1406/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 1.1062e-08 - val_loss: 0.4784\n",
      "Epoch 1407/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 3.7253e-0 - 0s 45us/sample - loss: 1.0885e-08 - val_loss: 0.4781\n",
      "Epoch 1408/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.1593e-08 - val_loss: 0.4783\n",
      "Epoch 1409/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.0531e-08 - val_loss: 0.4805\n",
      "Epoch 1410/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.2921e-08 - val_loss: 0.4817\n",
      "Epoch 1411/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.1593e-08 - val_loss: 0.4810\n",
      "Epoch 1412/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.0885e-08 - val_loss: 0.4807\n",
      "Epoch 1413/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.0885e-08 - val_loss: 0.4803\n",
      "Epoch 1414/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 1.0531e-08 - val_loss: 0.4797\n",
      "Epoch 1415/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.0177e-08 - val_loss: 0.4807\n",
      "Epoch 1416/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.1062e-08 - val_loss: 0.4806\n",
      "Epoch 1417/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.1062e-08 - val_loss: 0.4788\n",
      "Epoch 1418/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.0620e-08 - val_loss: 0.4803\n",
      "Epoch 1419/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 9.8235e-09 - val_loss: 0.4792\n",
      "Epoch 1420/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.0885e-08 - val_loss: 0.4791\n",
      "Epoch 1421/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.0443e-08 - val_loss: 0.4805\n",
      "Epoch 1422/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 1.0708e-08 - val_loss: 0.4809\n",
      "Epoch 1423/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 9.8235e-09 - val_loss: 0.4797\n",
      "Epoch 1424/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 1.0443e-08 - val_loss: 0.4804\n",
      "Epoch 1425/5000\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 1.0177e-08 - val_loss: 0.4803\n",
      "Epoch 1426/5000\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 1.0000e-08 - val_loss: 0.4791\n",
      "Epoch 1427/5000\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 9.9120e-09 - val_loss: 0.4803\n",
      "Epoch 1428/5000\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 9.9120e-09 - val_loss: 0.4800\n",
      "Epoch 1429/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 9.4695e-09 - val_loss: 0.4799\n",
      "Epoch 1430/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 1.0354e-08 - val_loss: 0.4800\n",
      "Epoch 1431/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 9.9120e-09 - val_loss: 0.4801\n",
      "Epoch 1432/5000\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 9.8235e-09 - val_loss: 0.4796\n",
      "Epoch 1433/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 1.0177e-08 - val_loss: 0.4796\n",
      "Epoch 1434/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 9.6465e-09 - val_loss: 0.4800\n",
      "Epoch 1435/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 1.0089e-08 - val_loss: 0.4798\n",
      "Epoch 1436/5000\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 1.0266e-08 - val_loss: 0.4796\n",
      "Epoch 1437/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 9.4695e-09 - val_loss: 0.4796\n",
      "Epoch 1438/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 9.5580e-09 - val_loss: 0.4793\n",
      "Epoch 1439/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 9.5580e-09 - val_loss: 0.4811\n",
      "Epoch 1440/5000\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 9.5580e-09 - val_loss: 0.4802\n",
      "Epoch 1441/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 8.5845e-09 - val_loss: 0.4800\n",
      "Epoch 1442/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 3.7253e-0 - 0s 47us/sample - loss: 9.8235e-09 - val_loss: 0.4799\n",
      "Epoch 1443/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 1.1176e-0 - 0s 45us/sample - loss: 9.7350e-09 - val_loss: 0.4799\n",
      "Epoch 1444/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 9.3810e-09 - val_loss: 0.4796\n",
      "Epoch 1445/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 9.3810e-09 - val_loss: 0.4798\n",
      "Epoch 1446/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 0.0000e+0 - 0s 48us/sample - loss: 9.5580e-09 - val_loss: 0.4809\n",
      "Epoch 1447/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1347/1347 [==============================] - 0s 45us/sample - loss: 9.7350e-09 - val_loss: 0.4795\n",
      "Epoch 1448/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 9.3810e-09 - val_loss: 0.4800\n",
      "Epoch 1449/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 9.6465e-09 - val_loss: 0.4805\n",
      "Epoch 1450/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 9.3810e-09 - val_loss: 0.4802\n",
      "Epoch 1451/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 9.5580e-09 - val_loss: 0.4807\n",
      "Epoch 1452/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 9.4695e-09 - val_loss: 0.4801\n",
      "Epoch 1453/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 9.3810e-09 - val_loss: 0.4800\n",
      "Epoch 1454/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 9.2040e-09 - val_loss: 0.4801\n",
      "Epoch 1455/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 1.0000e-08 - val_loss: 0.4803\n",
      "Epoch 1456/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 8.8500e-09 - val_loss: 0.4805\n",
      "Epoch 1457/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 9.3810e-09 - val_loss: 0.4805\n",
      "Epoch 1458/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 9.5580e-09 - val_loss: 0.4810\n",
      "Epoch 1459/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 9.4695e-09 - val_loss: 0.4800\n",
      "Epoch 1460/5000\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 9.7350e-09 - val_loss: 0.4804\n",
      "Epoch 1461/5000\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 8.5845e-09 - val_loss: 0.4799\n",
      "Epoch 1462/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 8.8500e-09 - val_loss: 0.4801\n",
      "Epoch 1463/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 9.4695e-09 - val_loss: 0.4806\n",
      "Epoch 1464/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 9.1155e-09 - val_loss: 0.4800\n",
      "Epoch 1465/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 9.0270e-09 - val_loss: 0.4805\n",
      "Epoch 1466/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 8.8500e-09 - val_loss: 0.4802\n",
      "Epoch 1467/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 9.0270e-09 - val_loss: 0.4803\n",
      "Epoch 1468/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 8.8500e-09 - val_loss: 0.4808\n",
      "Epoch 1469/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 8.9385e-09 - val_loss: 0.4804\n",
      "Epoch 1470/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 8.4075e-09 - val_loss: 0.4818\n",
      "Epoch 1471/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 9.2040e-09 - val_loss: 0.4808\n",
      "Epoch 1472/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 9.0270e-09 - val_loss: 0.4806\n",
      "Epoch 1473/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 8.6730e-09 - val_loss: 0.4811\n",
      "Epoch 1474/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 3.7253e-0 - 0s 46us/sample - loss: 8.9385e-09 - val_loss: 0.4805\n",
      "Epoch 1475/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 2.2352e-0 - 0s 45us/sample - loss: 1.0000e-08 - val_loss: 0.4801\n",
      "Epoch 1476/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 8.4960e-09 - val_loss: 0.4814\n",
      "Epoch 1477/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 8.6730e-09 - val_loss: 0.4805\n",
      "Epoch 1478/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 8.8500e-09 - val_loss: 0.4810\n",
      "Epoch 1479/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 9.0270e-09 - val_loss: 0.4812\n",
      "Epoch 1480/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 8.7615e-09 - val_loss: 0.4811\n",
      "Epoch 1481/5000\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 8.4075e-09 - val_loss: 0.4805\n",
      "Epoch 1482/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 8.1420e-09 - val_loss: 0.4808\n",
      "Epoch 1483/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 8.9385e-09 - val_loss: 0.4822\n",
      "Epoch 1484/5000\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 7.6110e-09 - val_loss: 0.4813\n",
      "Epoch 1485/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 8.5845e-09 - val_loss: 0.4813\n",
      "Epoch 1486/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 8.1420e-09 - val_loss: 0.4816\n",
      "Epoch 1487/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 8.3190e-09 - val_loss: 0.4815\n",
      "Epoch 1488/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 8.4075e-09 - val_loss: 0.4836\n",
      "Epoch 1489/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 9.1155e-09 - val_loss: 0.4824\n",
      "Epoch 1490/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 8.3190e-09 - val_loss: 0.4832\n",
      "Epoch 1491/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 8.0535e-09 - val_loss: 0.4822\n",
      "Epoch 1492/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 7.7880e-09 - val_loss: 0.4821\n",
      "Epoch 1493/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 8.0535e-09 - val_loss: 0.4829\n",
      "Epoch 1494/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 7.9650e-09 - val_loss: 0.4823\n",
      "Epoch 1495/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 7.8765e-09 - val_loss: 0.4825\n",
      "Epoch 1496/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 8.0535e-09 - val_loss: 0.4829\n",
      "Epoch 1497/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 7.4340e-09 - val_loss: 0.4831\n",
      "Epoch 1498/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 8.1420e-09 - val_loss: 0.4828\n",
      "Epoch 1499/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 8.2305e-09 - val_loss: 0.4818\n",
      "Epoch 1500/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 7.6995e-09 - val_loss: 0.4818\n",
      "Epoch 1501/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 7.7880e-09 - val_loss: 0.4826\n",
      "Epoch 1502/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 8.3190e-09 - val_loss: 0.4818\n",
      "Epoch 1503/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 7.2570e-09 - val_loss: 0.4825\n",
      "Epoch 1504/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 7.5225e-09 - val_loss: 0.4830\n",
      "Epoch 1505/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 7.5225e-09 - val_loss: 0.4827\n",
      "Epoch 1506/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 8.2305e-09 - val_loss: 0.4832\n",
      "Epoch 1507/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 7.6995e-09 - val_loss: 0.4835\n",
      "Epoch 1508/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 7.4506e-0 - 0s 46us/sample - loss: 6.9915e-09 - val_loss: 0.4830\n",
      "Epoch 1509/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 7.8765e-09 - val_loss: 0.4827\n",
      "Epoch 1510/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 8.0535e-09 - val_loss: 0.4819\n",
      "Epoch 1511/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 7.5225e-09 - val_loss: 0.4827\n",
      "Epoch 1512/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 0.0000e+0 - 0s 52us/sample - loss: 7.5225e-09 - val_loss: 0.4827\n",
      "Epoch 1513/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 7.4340e-09 - val_loss: 0.4827\n",
      "Epoch 1514/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 7.6110e-09 - val_loss: 0.4828\n",
      "Epoch 1515/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 8.2305e-09 - val_loss: 0.4828\n",
      "Epoch 1516/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 7.3455e-09 - val_loss: 0.4827\n",
      "Epoch 1517/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 7.0800e-09 - val_loss: 0.4826\n",
      "Epoch 1518/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1347/1347 [==============================] - 0s 47us/sample - loss: 7.1685e-09 - val_loss: 0.4831\n",
      "Epoch 1519/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 7.7880e-09 - val_loss: 0.4832\n",
      "Epoch 1520/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 7.9650e-09 - val_loss: 0.4810\n",
      "Epoch 1521/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 7.2570e-09 - val_loss: 0.4827\n",
      "Epoch 1522/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 7.7880e-09 - val_loss: 0.4850\n",
      "Epoch 1523/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 7.0800e-09 - val_loss: 0.4839\n",
      "Epoch 1524/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.7260e-09 - val_loss: 0.4833\n",
      "Epoch 1525/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 6.8145e-09 - val_loss: 0.4838\n",
      "Epoch 1526/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 7.0800e-09 - val_loss: 0.4830\n",
      "Epoch 1527/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 3.7253e-0 - 0s 44us/sample - loss: 7.2570e-09 - val_loss: 0.4830\n",
      "Epoch 1528/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 7.3455e-09 - val_loss: 0.4827\n",
      "Epoch 1529/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.9915e-09 - val_loss: 0.4836\n",
      "Epoch 1530/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 7.3455e-09 - val_loss: 0.4843\n",
      "Epoch 1531/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.8145e-09 - val_loss: 0.4832\n",
      "Epoch 1532/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.6375e-09 - val_loss: 0.4835\n",
      "Epoch 1533/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.9915e-09 - val_loss: 0.4834\n",
      "Epoch 1534/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 7.7880e-09 - val_loss: 0.4828\n",
      "Epoch 1535/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 7.3455e-09 - val_loss: 0.4834\n",
      "Epoch 1536/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 7.3455e-09 - val_loss: 0.4836\n",
      "Epoch 1537/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 3.7253e-0 - 0s 45us/sample - loss: 6.9915e-09 - val_loss: 0.4835\n",
      "Epoch 1538/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 7.2570e-09 - val_loss: 0.4850\n",
      "Epoch 1539/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 7.4340e-09 - val_loss: 0.4843\n",
      "Epoch 1540/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.7260e-09 - val_loss: 0.4846\n",
      "Epoch 1541/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 7.6995e-09 - val_loss: 0.4839\n",
      "Epoch 1542/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.9915e-09 - val_loss: 0.4841\n",
      "Epoch 1543/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.9915e-09 - val_loss: 0.4841\n",
      "Epoch 1544/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 7.1685e-09 - val_loss: 0.4844\n",
      "Epoch 1545/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 7.1685e-09 - val_loss: 0.4838\n",
      "Epoch 1546/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 7.4340e-09 - val_loss: 0.4851\n",
      "Epoch 1547/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 6.7260e-09 - val_loss: 0.4845\n",
      "Epoch 1548/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 7.5225e-09 - val_loss: 0.4844\n",
      "Epoch 1549/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 7.4506e-0 - 0s 45us/sample - loss: 7.2570e-09 - val_loss: 0.4845\n",
      "Epoch 1550/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 6.9915e-09 - val_loss: 0.4839\n",
      "Epoch 1551/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.8145e-09 - val_loss: 0.4835\n",
      "Epoch 1552/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 7.9650e-09 - val_loss: 0.4835\n",
      "Epoch 1553/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 7.5225e-09 - val_loss: 0.4841\n",
      "Epoch 1554/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 7.0800e-09 - val_loss: 0.4840\n",
      "Epoch 1555/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.3720e-09 - val_loss: 0.4834\n",
      "Epoch 1556/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.4605e-09 - val_loss: 0.4834\n",
      "Epoch 1557/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.9030e-09 - val_loss: 0.4842\n",
      "Epoch 1558/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.7260e-09 - val_loss: 0.4844\n",
      "Epoch 1559/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 7.9650e-09 - val_loss: 0.4848\n",
      "Epoch 1560/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 7.0800e-09 - val_loss: 0.4841\n",
      "Epoch 1561/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 7.1685e-09 - val_loss: 0.4845\n",
      "Epoch 1562/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 7.3455e-09 - val_loss: 0.4847\n",
      "Epoch 1563/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 6.7260e-09 - val_loss: 0.4839\n",
      "Epoch 1564/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.7260e-09 - val_loss: 0.4840\n",
      "Epoch 1565/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.9030e-09 - val_loss: 0.4838\n",
      "Epoch 1566/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.8145e-09 - val_loss: 0.4841\n",
      "Epoch 1567/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 0.0000e+0 - 0s 45us/sample - loss: 6.9030e-09 - val_loss: 0.4836\n",
      "Epoch 1568/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 7.1685e-09 - val_loss: 0.4840\n",
      "Epoch 1569/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.5490e-09 - val_loss: 0.4845\n",
      "Epoch 1570/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 3.7253e-0 - 0s 44us/sample - loss: 7.0800e-09 - val_loss: 0.4842\n",
      "Epoch 1571/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.6375e-09 - val_loss: 0.4840\n",
      "Epoch 1572/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.6375e-09 - val_loss: 0.4840\n",
      "Epoch 1573/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 6.6375e-09 - val_loss: 0.4837\n",
      "Epoch 1574/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.7260e-09 - val_loss: 0.4842\n",
      "Epoch 1575/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.7260e-09 - val_loss: 0.4842\n",
      "Epoch 1576/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.5490e-09 - val_loss: 0.4837\n",
      "Epoch 1577/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 7.2570e-09 - val_loss: 0.4842\n",
      "Epoch 1578/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 6.6375e-09 - val_loss: 0.4842\n",
      "Epoch 1579/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 6.8145e-09 - val_loss: 0.4838\n",
      "Epoch 1580/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 6.5490e-09 - val_loss: 0.4843\n",
      "Epoch 1581/5000\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 6.9030e-09 - val_loss: 0.4843\n",
      "Epoch 1582/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 6.3720e-09 - val_loss: 0.4852\n",
      "Epoch 1583/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 6.9915e-09 - val_loss: 0.4844\n",
      "Epoch 1584/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 6.4605e-09 - val_loss: 0.4843\n",
      "Epoch 1585/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 6.9030e-09 - val_loss: 0.4841\n",
      "Epoch 1586/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.2835e-09 - val_loss: 0.4847\n",
      "Epoch 1587/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.6375e-09 - val_loss: 0.4843\n",
      "Epoch 1588/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.7260e-09 - val_loss: 0.4844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1589/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.4605e-09 - val_loss: 0.4832\n",
      "Epoch 1590/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 6.7260e-09 - val_loss: 0.4839\n",
      "Epoch 1591/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 7.0800e-09 - val_loss: 0.4843\n",
      "Epoch 1592/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.9915e-09 - val_loss: 0.4844\n",
      "Epoch 1593/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 7.0800e-09 - val_loss: 0.4844\n",
      "Epoch 1594/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.9915e-09 - val_loss: 0.4843\n",
      "Epoch 1595/5000\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 6.5490e-09 - val_loss: 0.4846\n",
      "Epoch 1596/5000\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 6.5490e-09 - val_loss: 0.4846\n",
      "Epoch 1597/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 6.9030e-09 - val_loss: 0.4850\n",
      "Epoch 1598/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 6.8145e-09 - val_loss: 0.4850\n",
      "Epoch 1599/5000\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 6.8145e-09 - val_loss: 0.4846\n",
      "Epoch 1600/5000\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 6.2835e-09 - val_loss: 0.4850\n",
      "Epoch 1601/5000\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 6.2835e-09 - val_loss: 0.4849\n",
      "Epoch 1602/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 6.6375e-09 - val_loss: 0.4853\n",
      "Epoch 1603/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 6.1950e-09 - val_loss: 0.4848\n",
      "Epoch 1604/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 7.3455e-09 - val_loss: 0.4844\n",
      "Epoch 1605/5000\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 6.7260e-09 - val_loss: 0.4849\n",
      "Epoch 1606/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 6.9030e-09 - val_loss: 0.4844\n",
      "Epoch 1607/5000\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 6.1950e-09 - val_loss: 0.4844\n",
      "Epoch 1608/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 6.4605e-09 - val_loss: 0.4842\n",
      "Epoch 1609/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 7.2570e-09 - val_loss: 0.4847\n",
      "Epoch 1610/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 6.3720e-09 - val_loss: 0.4847\n",
      "Epoch 1611/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 6.4605e-09 - val_loss: 0.4846\n",
      "Epoch 1612/5000\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 6.8145e-09 - val_loss: 0.4847\n",
      "Epoch 1613/5000\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 6.4605e-09 - val_loss: 0.4848\n",
      "Epoch 1614/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 7.0800e-09 - val_loss: 0.4851\n",
      "Epoch 1615/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 6.9030e-09 - val_loss: 0.4855\n",
      "Epoch 1616/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 6.2835e-09 - val_loss: 0.4849\n",
      "Epoch 1617/5000\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 6.4605e-09 - val_loss: 0.4851\n",
      "Epoch 1618/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 6.5490e-09 - val_loss: 0.4851\n",
      "Epoch 1619/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 6.5490e-09 - val_loss: 0.4844\n",
      "Epoch 1620/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 6.5490e-09 - val_loss: 0.4846\n",
      "Epoch 1621/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 6.4605e-09 - val_loss: 0.4843\n",
      "Epoch 1622/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 6.2835e-09 - val_loss: 0.4837\n",
      "Epoch 1623/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 6.4605e-09 - val_loss: 0.4843\n",
      "Epoch 1624/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.1950e-09 - val_loss: 0.4847\n",
      "Epoch 1625/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 6.6375e-09 - val_loss: 0.4849\n",
      "Epoch 1626/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.1950e-09 - val_loss: 0.4848\n",
      "Epoch 1627/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 6.3720e-09 - val_loss: 0.4854\n",
      "Epoch 1628/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.5490e-09 - val_loss: 0.4848\n",
      "Epoch 1629/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.5490e-09 - val_loss: 0.4857\n",
      "Epoch 1630/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 6.7260e-09 - val_loss: 0.4859\n",
      "Epoch 1631/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.8145e-09 - val_loss: 0.4855\n",
      "Epoch 1632/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 6.4605e-09 - val_loss: 0.4850\n",
      "Epoch 1633/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.4605e-09 - val_loss: 0.4847\n",
      "Epoch 1634/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 6.5490e-09 - val_loss: 0.4853\n",
      "Epoch 1635/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 6.3720e-09 - val_loss: 0.4851\n",
      "Epoch 1636/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.7260e-09 - val_loss: 0.4854\n",
      "Epoch 1637/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.7260e-09 - val_loss: 0.4857\n",
      "Epoch 1638/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 6.3720e-09 - val_loss: 0.4849\n",
      "Epoch 1639/5000\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 6.9030e-09 - val_loss: 0.4850\n",
      "Epoch 1640/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.5490e-09 - val_loss: 0.4856\n",
      "Epoch 1641/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.5490e-09 - val_loss: 0.4857\n",
      "Epoch 1642/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 6.5490e-09 - val_loss: 0.4863\n",
      "Epoch 1643/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 6.2835e-09 - val_loss: 0.4852\n",
      "Epoch 1644/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 6.8145e-09 - val_loss: 0.4859\n",
      "Epoch 1645/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.7260e-09 - val_loss: 0.4859\n",
      "Epoch 1646/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.6375e-09 - val_loss: 0.4865\n",
      "Epoch 1647/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.3720e-09 - val_loss: 0.4864\n",
      "Epoch 1648/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.3720e-09 - val_loss: 0.4861\n",
      "Epoch 1649/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 6.3720e-09 - val_loss: 0.4865\n",
      "Epoch 1650/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.4605e-09 - val_loss: 0.4862\n",
      "Epoch 1651/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.9295e-09 - val_loss: 0.4858\n",
      "Epoch 1652/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 6.1950e-09 - val_loss: 0.4857\n",
      "Epoch 1653/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 6.2835e-09 - val_loss: 0.4852\n",
      "Epoch 1654/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 6.2835e-09 - val_loss: 0.4856\n",
      "Epoch 1655/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 6.5490e-09 - val_loss: 0.4855\n",
      "Epoch 1656/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 6.6375e-09 - val_loss: 0.4859\n",
      "Epoch 1657/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.8410e-09 - val_loss: 0.4863\n",
      "Epoch 1658/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 6.6375e-09 - val_loss: 0.4867\n",
      "Epoch 1659/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.7260e-09 - val_loss: 0.4867\n",
      "Epoch 1660/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 6.3720e-09 - val_loss: 0.4860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1661/5000\n",
      "1347/1347 [==============================] - 0s 69us/sample - loss: 6.1065e-09 - val_loss: 0.4864\n",
      "Epoch 1662/5000\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 6.4605e-09 - val_loss: 0.4865\n",
      "Epoch 1663/5000\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 6.5490e-09 - val_loss: 0.4870\n",
      "Epoch 1664/5000\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 6.5490e-09 - val_loss: 0.4862\n",
      "Epoch 1665/5000\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 6.1065e-09 - val_loss: 0.4865\n",
      "Epoch 1666/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 6.2835e-09 - val_loss: 0.4868\n",
      "Epoch 1667/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 6.1950e-09 - val_loss: 0.4864\n",
      "Epoch 1668/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.9295e-09 - val_loss: 0.4868\n",
      "Epoch 1669/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.5490e-09 - val_loss: 0.4863\n",
      "Epoch 1670/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 6.1950e-09 - val_loss: 0.4870\n",
      "Epoch 1671/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.2835e-09 - val_loss: 0.4872\n",
      "Epoch 1672/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 6.3720e-09 - val_loss: 0.4870\n",
      "Epoch 1673/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 6.2835e-09 - val_loss: 0.4870\n",
      "Epoch 1674/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 6.8145e-09 - val_loss: 0.4887\n",
      "Epoch 1675/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.3720e-09 - val_loss: 0.4876\n",
      "Epoch 1676/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.8410e-09 - val_loss: 0.4876\n",
      "Epoch 1677/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.1950e-09 - val_loss: 0.4871\n",
      "Epoch 1678/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 6.4605e-09 - val_loss: 0.4875\n",
      "Epoch 1679/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.7525e-09 - val_loss: 0.4869\n",
      "Epoch 1680/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.1065e-09 - val_loss: 0.4866\n",
      "Epoch 1681/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.0180e-09 - val_loss: 0.4868\n",
      "Epoch 1682/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 6.4605e-09 - val_loss: 0.4872\n",
      "Epoch 1683/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 6.1065e-09 - val_loss: 0.4865\n",
      "Epoch 1684/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 6.1065e-09 - val_loss: 0.4861\n",
      "Epoch 1685/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 6.6375e-09 - val_loss: 0.4873\n",
      "Epoch 1686/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.8410e-09 - val_loss: 0.4872\n",
      "Epoch 1687/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.9295e-09 - val_loss: 0.4864\n",
      "Epoch 1688/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.1950e-09 - val_loss: 0.4864\n",
      "Epoch 1689/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 6.0180e-09 - val_loss: 0.4870\n",
      "Epoch 1690/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.0180e-09 - val_loss: 0.4870\n",
      "Epoch 1691/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.7525e-09 - val_loss: 0.4874\n",
      "Epoch 1692/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.8410e-09 - val_loss: 0.4868\n",
      "Epoch 1693/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.3720e-09 - val_loss: 0.4877\n",
      "Epoch 1694/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.1950e-09 - val_loss: 0.4870\n",
      "Epoch 1695/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.0180e-09 - val_loss: 0.4870\n",
      "Epoch 1696/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.8410e-09 - val_loss: 0.4873\n",
      "Epoch 1697/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.9295e-09 - val_loss: 0.4878\n",
      "Epoch 1698/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 6.1950e-09 - val_loss: 0.4878\n",
      "Epoch 1699/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 7.1685e-09 - val_loss: 0.4891\n",
      "Epoch 1700/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.1065e-09 - val_loss: 0.4878\n",
      "Epoch 1701/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.9295e-09 - val_loss: 0.4877\n",
      "Epoch 1702/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 6.1950e-09 - val_loss: 0.4879\n",
      "Epoch 1703/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 6.3720e-09 - val_loss: 0.4863\n",
      "Epoch 1704/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 5.8410e-09 - val_loss: 0.4872\n",
      "Epoch 1705/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.2835e-09 - val_loss: 0.4881\n",
      "Epoch 1706/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.8410e-09 - val_loss: 0.4882\n",
      "Epoch 1707/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.9295e-09 - val_loss: 0.4886\n",
      "Epoch 1708/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 5.8410e-09 - val_loss: 0.4876\n",
      "Epoch 1709/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 5.8410e-09 - val_loss: 0.4881\n",
      "Epoch 1710/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 5.8410e-09 - val_loss: 0.4866\n",
      "Epoch 1711/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 6.2835e-09 - val_loss: 0.4877\n",
      "Epoch 1712/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 6.1950e-09 - val_loss: 0.4870\n",
      "Epoch 1713/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 5.8410e-09 - val_loss: 0.4862\n",
      "Epoch 1714/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 6.0180e-09 - val_loss: 0.4877\n",
      "Epoch 1715/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 6.2835e-09 - val_loss: 0.4875\n",
      "Epoch 1716/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 5.9295e-09 - val_loss: 0.4870\n",
      "Epoch 1717/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 5.9295e-09 - val_loss: 0.4874\n",
      "Epoch 1718/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 6.0180e-09 - val_loss: 0.4879\n",
      "Epoch 1719/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.9295e-09 - val_loss: 0.4884\n",
      "Epoch 1720/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.2215e-09 - val_loss: 0.4874\n",
      "Epoch 1721/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.2835e-09 - val_loss: 0.4878\n",
      "Epoch 1722/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.9295e-09 - val_loss: 0.4890\n",
      "Epoch 1723/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 5.5755e-09 - val_loss: 0.4883\n",
      "Epoch 1724/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 6.1065e-09 - val_loss: 0.4882\n",
      "Epoch 1725/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.3100e-09 - val_loss: 0.4881\n",
      "Epoch 1726/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 6.2835e-09 - val_loss: 0.4886\n",
      "Epoch 1727/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.9295e-09 - val_loss: 0.4886\n",
      "Epoch 1728/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.5755e-09 - val_loss: 0.4885\n",
      "Epoch 1729/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.4605e-09 - val_loss: 0.4888\n",
      "Epoch 1730/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.8410e-09 - val_loss: 0.4884\n",
      "Epoch 1731/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.0180e-09 - val_loss: 0.4890\n",
      "Epoch 1732/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.4605e-09 - val_loss: 0.4894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1733/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.4870e-09 - val_loss: 0.4891\n",
      "Epoch 1734/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.8410e-09 - val_loss: 0.4885\n",
      "Epoch 1735/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.7525e-09 - val_loss: 0.4882\n",
      "Epoch 1736/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.6640e-09 - val_loss: 0.4879\n",
      "Epoch 1737/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.8410e-09 - val_loss: 0.4887\n",
      "Epoch 1738/5000\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 5.8410e-09 - val_loss: 0.4884\n",
      "Epoch 1739/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 5.5755e-09 - val_loss: 0.4887\n",
      "Epoch 1740/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 5.6640e-09 - val_loss: 0.4882\n",
      "Epoch 1741/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 5.5755e-09 - val_loss: 0.4889\n",
      "Epoch 1742/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.7525e-09 - val_loss: 0.4892\n",
      "Epoch 1743/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.5755e-09 - val_loss: 0.4904\n",
      "Epoch 1744/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.7525e-09 - val_loss: 0.4888\n",
      "Epoch 1745/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 5.5755e-09 - val_loss: 0.4894\n",
      "Epoch 1746/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.5755e-09 - val_loss: 0.4892\n",
      "Epoch 1747/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.3100e-09 - val_loss: 0.4882\n",
      "Epoch 1748/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.1950e-09 - val_loss: 0.4892\n",
      "Epoch 1749/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.8410e-09 - val_loss: 0.4890\n",
      "Epoch 1750/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.5490e-09 - val_loss: 0.4892\n",
      "Epoch 1751/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.3100e-09 - val_loss: 0.4902\n",
      "Epoch 1752/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.7525e-09 - val_loss: 0.4894\n",
      "Epoch 1753/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.7525e-09 - val_loss: 0.4890\n",
      "Epoch 1754/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.3100e-09 - val_loss: 0.4896\n",
      "Epoch 1755/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 5.4870e-09 - val_loss: 0.4888\n",
      "Epoch 1756/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.0180e-09 - val_loss: 0.4893\n",
      "Epoch 1757/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 5.7525e-09 - val_loss: 0.4893\n",
      "Epoch 1758/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.4870e-09 - val_loss: 0.4886\n",
      "Epoch 1759/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 5.9295e-09 - val_loss: 0.4892\n",
      "Epoch 1760/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 5.3985e-09 - val_loss: 0.4897\n",
      "Epoch 1761/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 5.6640e-09 - val_loss: 0.4904\n",
      "Epoch 1762/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.0180e-09 - val_loss: 0.4893\n",
      "Epoch 1763/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.9295e-09 - val_loss: 0.4892\n",
      "Epoch 1764/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 5.0445e-09 - val_loss: 0.4891\n",
      "Epoch 1765/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 5.3985e-09 - val_loss: 0.4890\n",
      "Epoch 1766/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 5.8410e-09 - val_loss: 0.4897\n",
      "Epoch 1767/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 6.1065e-09 - val_loss: 0.4900\n",
      "Epoch 1768/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 5.8410e-09 - val_loss: 0.4896\n",
      "Epoch 1769/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 5.8410e-09 - val_loss: 0.4898\n",
      "Epoch 1770/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 5.9295e-09 - val_loss: 0.4903\n",
      "Epoch 1771/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.9295e-09 - val_loss: 0.4907\n",
      "Epoch 1772/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 5.9295e-09 - val_loss: 0.4902\n",
      "Epoch 1773/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 5.5755e-09 - val_loss: 0.4904\n",
      "Epoch 1774/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.8410e-09 - val_loss: 0.4902\n",
      "Epoch 1775/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 6.0180e-09 - val_loss: 0.4904\n",
      "Epoch 1776/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.7525e-09 - val_loss: 0.4905\n",
      "Epoch 1777/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 5.3100e-09 - val_loss: 0.4905\n",
      "Epoch 1778/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 5.2215e-09 - val_loss: 0.4909\n",
      "Epoch 1779/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.0445e-09 - val_loss: 0.4906\n",
      "Epoch 1780/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 5.8410e-09 - val_loss: 0.4908\n",
      "Epoch 1781/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 5.4870e-09 - val_loss: 0.4907\n",
      "Epoch 1782/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.8410e-09 - val_loss: 0.4927\n",
      "Epoch 1783/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 6.0180e-09 - val_loss: 0.4920\n",
      "Epoch 1784/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 7.4506e-0 - 0s 48us/sample - loss: 5.4870e-09 - val_loss: 0.4906\n",
      "Epoch 1785/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 6.1065e-09 - val_loss: 0.4905\n",
      "Epoch 1786/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.3100e-09 - val_loss: 0.4916\n",
      "Epoch 1787/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.1065e-09 - val_loss: 0.4931\n",
      "Epoch 1788/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.9295e-09 - val_loss: 0.4916\n",
      "Epoch 1789/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.6640e-09 - val_loss: 0.4920\n",
      "Epoch 1790/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 6.2835e-09 - val_loss: 0.4918\n",
      "Epoch 1791/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.2215e-09 - val_loss: 0.4919\n",
      "Epoch 1792/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.7525e-09 - val_loss: 0.4915\n",
      "Epoch 1793/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.8410e-09 - val_loss: 0.4917\n",
      "Epoch 1794/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 5.2215e-09 - val_loss: 0.4925\n",
      "Epoch 1795/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.1950e-09 - val_loss: 0.4919\n",
      "Epoch 1796/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 5.7525e-09 - val_loss: 0.4913\n",
      "Epoch 1797/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 5.8410e-09 - val_loss: 0.4910\n",
      "Epoch 1798/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 6.1950e-09 - val_loss: 0.4915\n",
      "Epoch 1799/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 5.6640e-09 - val_loss: 0.4913\n",
      "Epoch 1800/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 5.5755e-09 - val_loss: 0.4922\n",
      "Epoch 1801/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 5.3985e-09 - val_loss: 0.4914\n",
      "Epoch 1802/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.6640e-09 - val_loss: 0.4916\n",
      "Epoch 1803/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.7525e-09 - val_loss: 0.4921\n",
      "Epoch 1804/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1347/1347 [==============================] - 0s 49us/sample - loss: 5.1330e-09 - val_loss: 0.4915\n",
      "Epoch 1805/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.4870e-09 - val_loss: 0.4918\n",
      "Epoch 1806/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 5.5755e-09 - val_loss: 0.4916\n",
      "Epoch 1807/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.8410e-09 - val_loss: 0.4915\n",
      "Epoch 1808/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 6.0180e-09 - val_loss: 0.4921\n",
      "Epoch 1809/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 4.5135e-09 - val_loss: 0.4920\n",
      "Epoch 1810/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.4870e-09 - val_loss: 0.4918\n",
      "Epoch 1811/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 5.6640e-09 - val_loss: 0.4922\n",
      "Epoch 1812/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 5.1330e-09 - val_loss: 0.4925\n",
      "Epoch 1813/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 6.0180e-09 - val_loss: 0.4921\n",
      "Epoch 1814/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 5.3985e-09 - val_loss: 0.4918\n",
      "Epoch 1815/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.4870e-09 - val_loss: 0.4913\n",
      "Epoch 1816/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.0445e-09 - val_loss: 0.4917\n",
      "Epoch 1817/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.1330e-09 - val_loss: 0.4922\n",
      "Epoch 1818/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 5.3100e-09 - val_loss: 0.4918\n",
      "Epoch 1819/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 5.7525e-09 - val_loss: 0.4919\n",
      "Epoch 1820/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.4870e-09 - val_loss: 0.4922\n",
      "Epoch 1821/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 4.7790e-09 - val_loss: 0.4920\n",
      "Epoch 1822/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 6.0180e-09 - val_loss: 0.4920\n",
      "Epoch 1823/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 5.3100e-09 - val_loss: 0.4931\n",
      "Epoch 1824/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 5.6640e-09 - val_loss: 0.4928\n",
      "Epoch 1825/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 5.2215e-09 - val_loss: 0.4921\n",
      "Epoch 1826/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.7525e-09 - val_loss: 0.4921\n",
      "Epoch 1827/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.7525e-09 - val_loss: 0.4926\n",
      "Epoch 1828/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 5.3100e-09 - val_loss: 0.4934\n",
      "Epoch 1829/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 5.5755e-09 - val_loss: 0.4934\n",
      "Epoch 1830/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 4.8675e-09 - val_loss: 0.4926\n",
      "Epoch 1831/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 6.1065e-09 - val_loss: 0.4916\n",
      "Epoch 1832/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 5.4870e-09 - val_loss: 0.4931\n",
      "Epoch 1833/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 5.3985e-09 - val_loss: 0.4937\n",
      "Epoch 1834/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 5.0445e-09 - val_loss: 0.4929\n",
      "Epoch 1835/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.4870e-09 - val_loss: 0.4937\n",
      "Epoch 1836/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.0445e-09 - val_loss: 0.4932\n",
      "Epoch 1837/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 5.1330e-09 - val_loss: 0.4934\n",
      "Epoch 1838/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.1330e-09 - val_loss: 0.4936\n",
      "Epoch 1839/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.5755e-09 - val_loss: 0.4933\n",
      "Epoch 1840/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.6640e-09 - val_loss: 0.4929\n",
      "Epoch 1841/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 5.8410e-09 - val_loss: 0.4927\n",
      "Epoch 1842/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 5.3985e-09 - val_loss: 0.4938\n",
      "Epoch 1843/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.1330e-09 - val_loss: 0.4931\n",
      "Epoch 1844/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 6.1065e-09 - val_loss: 0.4935\n",
      "Epoch 1845/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.5755e-09 - val_loss: 0.4934\n",
      "Epoch 1846/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 5.6640e-09 - val_loss: 0.4934\n",
      "Epoch 1847/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 5.3985e-09 - val_loss: 0.4937\n",
      "Epoch 1848/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.5135e-09 - val_loss: 0.4936\n",
      "Epoch 1849/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.3100e-09 - val_loss: 0.4937\n",
      "Epoch 1850/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 5.8410e-09 - val_loss: 0.4940\n",
      "Epoch 1851/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.2215e-09 - val_loss: 0.4938\n",
      "Epoch 1852/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.3100e-09 - val_loss: 0.4935\n",
      "Epoch 1853/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 5.4870e-09 - val_loss: 0.4948\n",
      "Epoch 1854/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 5.8410e-09 - val_loss: 0.4947\n",
      "Epoch 1855/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.7525e-09 - val_loss: 0.4942\n",
      "Epoch 1856/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.1330e-09 - val_loss: 0.4940\n",
      "Epoch 1857/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.0445e-09 - val_loss: 0.4936\n",
      "Epoch 1858/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 5.1330e-09 - val_loss: 0.4937\n",
      "Epoch 1859/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.3985e-09 - val_loss: 0.4939\n",
      "Epoch 1860/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.6640e-09 - val_loss: 0.4940\n",
      "Epoch 1861/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 6.0180e-09 - val_loss: 0.4949\n",
      "Epoch 1862/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.3100e-09 - val_loss: 0.4940\n",
      "Epoch 1863/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 0.0000e+0 - 0s 46us/sample - loss: 5.2215e-09 - val_loss: 0.4942\n",
      "Epoch 1864/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 4.8675e-09 - val_loss: 0.4943\n",
      "Epoch 1865/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 4.9560e-09 - val_loss: 0.4941\n",
      "Epoch 1866/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.5755e-09 - val_loss: 0.4936\n",
      "Epoch 1867/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.3985e-09 - val_loss: 0.4943\n",
      "Epoch 1868/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.8675e-09 - val_loss: 0.4940\n",
      "Epoch 1869/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.3985e-09 - val_loss: 0.4957\n",
      "Epoch 1870/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.5755e-09 - val_loss: 0.4951\n",
      "Epoch 1871/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.3985e-09 - val_loss: 0.4949\n",
      "Epoch 1872/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.2215e-09 - val_loss: 0.4950\n",
      "Epoch 1873/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.2215e-09 - val_loss: 0.4951\n",
      "Epoch 1874/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.0445e-09 - val_loss: 0.4941\n",
      "Epoch 1875/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.5755e-09 - val_loss: 0.4936\n",
      "Epoch 1876/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.7790e-09 - val_loss: 0.4943\n",
      "Epoch 1877/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.2215e-09 - val_loss: 0.4940\n",
      "Epoch 1878/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 5.6640e-09 - val_loss: 0.4942\n",
      "Epoch 1879/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 5.7525e-09 - val_loss: 0.4937\n",
      "Epoch 1880/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.2215e-09 - val_loss: 0.4945\n",
      "Epoch 1881/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 5.4870e-09 - val_loss: 0.4936\n",
      "Epoch 1882/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 5.3100e-09 - val_loss: 0.4938\n",
      "Epoch 1883/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.3100e-09 - val_loss: 0.4938\n",
      "Epoch 1884/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 4.9560e-09 - val_loss: 0.4950\n",
      "Epoch 1885/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 5.0445e-09 - val_loss: 0.4952\n",
      "Epoch 1886/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 4.9560e-09 - val_loss: 0.4946\n",
      "Epoch 1887/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 4.5135e-09 - val_loss: 0.4950\n",
      "Epoch 1888/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.3985e-09 - val_loss: 0.4943\n",
      "Epoch 1889/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 5.2215e-09 - val_loss: 0.4951\n",
      "Epoch 1890/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 4.9560e-09 - val_loss: 0.4950\n",
      "Epoch 1891/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 5.1330e-09 - val_loss: 0.4944\n",
      "Epoch 1892/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.3100e-09 - val_loss: 0.4956\n",
      "Epoch 1893/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.0180e-09 - val_loss: 0.4958\n",
      "Epoch 1894/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 5.2215e-09 - val_loss: 0.4953\n",
      "Epoch 1895/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.1330e-09 - val_loss: 0.4952\n",
      "Epoch 1896/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.6640e-09 - val_loss: 0.4955\n",
      "Epoch 1897/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 4.7790e-09 - val_loss: 0.4953\n",
      "Epoch 1898/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 4.9560e-09 - val_loss: 0.4957\n",
      "Epoch 1899/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 4.6905e-09 - val_loss: 0.4957\n",
      "Epoch 1900/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 5.6640e-09 - val_loss: 0.4954\n",
      "Epoch 1901/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 4.7790e-09 - val_loss: 0.4952\n",
      "Epoch 1902/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.1330e-09 - val_loss: 0.4955\n",
      "Epoch 1903/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 4.6020e-09 - val_loss: 0.4964\n",
      "Epoch 1904/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 5.0445e-09 - val_loss: 0.4962\n",
      "Epoch 1905/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.6905e-09 - val_loss: 0.4956\n",
      "Epoch 1906/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 5.1330e-09 - val_loss: 0.4965\n",
      "Epoch 1907/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 4.8675e-09 - val_loss: 0.4958\n",
      "Epoch 1908/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 4.6905e-09 - val_loss: 0.4964\n",
      "Epoch 1909/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 4.9560e-09 - val_loss: 0.4962\n",
      "Epoch 1910/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 5.0445e-09 - val_loss: 0.4969\n",
      "Epoch 1911/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 3.7253e-0 - 0s 44us/sample - loss: 5.0445e-09 - val_loss: 0.4963\n",
      "Epoch 1912/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.7790e-09 - val_loss: 0.4964\n",
      "Epoch 1913/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 4.6905e-09 - val_loss: 0.4966\n",
      "Epoch 1914/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.5755e-09 - val_loss: 0.4967\n",
      "Epoch 1915/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 4.9560e-09 - val_loss: 0.4967\n",
      "Epoch 1916/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 6.0180e-09 - val_loss: 0.4963\n",
      "Epoch 1917/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 5.3985e-09 - val_loss: 0.4957\n",
      "Epoch 1918/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.5135e-09 - val_loss: 0.4967\n",
      "Epoch 1919/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 5.0445e-09 - val_loss: 0.4963\n",
      "Epoch 1920/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 5.0445e-09 - val_loss: 0.4961\n",
      "Epoch 1921/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.6905e-09 - val_loss: 0.4958\n",
      "Epoch 1922/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.9560e-09 - val_loss: 0.4965\n",
      "Epoch 1923/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.7790e-09 - val_loss: 0.4973\n",
      "Epoch 1924/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 5.2215e-09 - val_loss: 0.4973\n",
      "Epoch 1925/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 0.0000e+0 - 0s 44us/sample - loss: 5.1330e-09 - val_loss: 0.4966\n",
      "Epoch 1926/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 4.9560e-09 - val_loss: 0.4963\n",
      "Epoch 1927/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 5.0445e-09 - val_loss: 0.4975\n",
      "Epoch 1928/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 5.3100e-09 - val_loss: 0.4982\n",
      "Epoch 1929/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.3100e-09 - val_loss: 0.4978\n",
      "Epoch 1930/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 5.3100e-09 - val_loss: 0.4976\n",
      "Epoch 1931/5000\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 5.3100e-09 - val_loss: 0.4969\n",
      "Epoch 1932/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 4.6905e-09 - val_loss: 0.4974\n",
      "Epoch 1933/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 5.3100e-09 - val_loss: 0.4975\n",
      "Epoch 1934/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 5.4870e-09 - val_loss: 0.4978\n",
      "Epoch 1935/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.3100e-09 - val_loss: 0.4981\n",
      "Epoch 1936/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.8675e-09 - val_loss: 0.4982\n",
      "Epoch 1937/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.8675e-09 - val_loss: 0.4982\n",
      "Epoch 1938/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 5.1330e-09 - val_loss: 0.4976\n",
      "Epoch 1939/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 4.6905e-09 - val_loss: 0.4977\n",
      "Epoch 1940/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 5.0445e-09 - val_loss: 0.4985\n",
      "Epoch 1941/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 5.0445e-09 - val_loss: 0.4983\n",
      "Epoch 1942/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 4.6020e-09 - val_loss: 0.4981\n",
      "Epoch 1943/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 5.0445e-09 - val_loss: 0.4984\n",
      "Epoch 1944/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 4.8675e-09 - val_loss: 0.4991\n",
      "Epoch 1945/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 5.5755e-09 - val_loss: 0.4987\n",
      "Epoch 1946/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1347/1347 [==============================] - 0s 49us/sample - loss: 5.5755e-09 - val_loss: 0.4988\n",
      "Epoch 1947/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 4.6020e-09 - val_loss: 0.4990\n",
      "Epoch 1948/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.6905e-09 - val_loss: 0.4981\n",
      "Epoch 1949/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.5755e-09 - val_loss: 0.4983\n",
      "Epoch 1950/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 4.6020e-09 - val_loss: 0.4991\n",
      "Epoch 1951/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 5.3100e-09 - val_loss: 0.4993\n",
      "Epoch 1952/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.2215e-09 - val_loss: 0.4994\n",
      "Epoch 1953/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 3.7253e-0 - 0s 45us/sample - loss: 4.9560e-09 - val_loss: 0.4990\n",
      "Epoch 1954/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.5755e-09 - val_loss: 0.5001\n",
      "Epoch 1955/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 0.0000e+0 - 0s 45us/sample - loss: 5.2215e-09 - val_loss: 0.5001\n",
      "Epoch 1956/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.9560e-09 - val_loss: 0.4999\n",
      "Epoch 1957/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 5.5755e-09 - val_loss: 0.4990\n",
      "Epoch 1958/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 5.0445e-09 - val_loss: 0.4996\n",
      "Epoch 1959/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.3100e-09 - val_loss: 0.4998\n",
      "Epoch 1960/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 4.8675e-09 - val_loss: 0.4999\n",
      "Epoch 1961/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 5.2215e-09 - val_loss: 0.4991\n",
      "Epoch 1962/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 4.6020e-09 - val_loss: 0.4994\n",
      "Epoch 1963/5000\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 5.0445e-09 - val_loss: 0.5013\n",
      "Epoch 1964/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 5.3985e-09 - val_loss: 0.5003\n",
      "Epoch 1965/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 5.0445e-09 - val_loss: 0.5003\n",
      "Epoch 1966/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 4.7790e-09 - val_loss: 0.5007\n",
      "Epoch 1967/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.0445e-09 - val_loss: 0.5006\n",
      "Epoch 1968/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 4.7790e-09 - val_loss: 0.4998\n",
      "Epoch 1969/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 4.8675e-09 - val_loss: 0.5002\n",
      "Epoch 1970/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.3100e-09 - val_loss: 0.4997\n",
      "Epoch 1971/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 5.0445e-09 - val_loss: 0.5006\n",
      "Epoch 1972/5000\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 4.5135e-09 - val_loss: 0.5011\n",
      "Epoch 1973/5000\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 4.2480e-09 - val_loss: 0.5011\n",
      "Epoch 1974/5000\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 4.9560e-09 - val_loss: 0.5004\n",
      "Epoch 1975/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 5.3608e-0 - 0s 59us/sample - loss: 5.3985e-09 - val_loss: 0.5013\n",
      "Epoch 1976/5000\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 4.6020e-09 - val_loss: 0.5006\n",
      "Epoch 1977/5000\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 5.5755e-09 - val_loss: 0.5006\n",
      "Epoch 1978/5000\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 5.4870e-09 - val_loss: 0.5011\n",
      "Epoch 1979/5000\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 5.0445e-09 - val_loss: 0.5016\n",
      "Epoch 1980/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 4.8675e-09 - val_loss: 0.5003\n",
      "Epoch 1981/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.6020e-09 - val_loss: 0.5009\n",
      "Epoch 1982/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 5.3985e-09 - val_loss: 0.5019\n",
      "Epoch 1983/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 5.5755e-09 - val_loss: 0.5016\n",
      "Epoch 1984/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 4.9560e-09 - val_loss: 0.5021\n",
      "Epoch 1985/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 5.0445e-09 - val_loss: 0.5013\n",
      "Epoch 1986/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.1330e-09 - val_loss: 0.5014\n",
      "Epoch 1987/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 5.0445e-09 - val_loss: 0.5017\n",
      "Epoch 1988/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 4.9560e-09 - val_loss: 0.5019\n",
      "Epoch 1989/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 4.8675e-09 - val_loss: 0.5017\n",
      "Epoch 1990/5000\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 5.3100e-09 - val_loss: 0.5026\n",
      "Epoch 1991/5000\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 4.8675e-09 - val_loss: 0.5011\n",
      "Epoch 1992/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 4.4250e-09 - val_loss: 0.5021\n",
      "Epoch 1993/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 5.2215e-09 - val_loss: 0.5023\n",
      "Epoch 1994/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 4.7790e-09 - val_loss: 0.5017\n",
      "Epoch 1995/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 4.8675e-09 - val_loss: 0.5020\n",
      "Epoch 1996/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 5.0445e-09 - val_loss: 0.5021\n",
      "Epoch 1997/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 4.9560e-09 - val_loss: 0.5023\n",
      "Epoch 1998/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.1330e-09 - val_loss: 0.5018\n",
      "Epoch 1999/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 4.9560e-09 - val_loss: 0.5027\n",
      "Epoch 2000/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.8675e-09 - val_loss: 0.5026\n",
      "Epoch 2001/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.8675e-09 - val_loss: 0.5027\n",
      "Epoch 2002/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 5.1330e-09 - val_loss: 0.5027\n",
      "Epoch 2003/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 4.8675e-09 - val_loss: 0.5035\n",
      "Epoch 2004/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 4.8675e-09 - val_loss: 0.5031\n",
      "Epoch 2005/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 4.6905e-09 - val_loss: 0.5031\n",
      "Epoch 2006/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.8675e-09 - val_loss: 0.5023\n",
      "Epoch 2007/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.1330e-09 - val_loss: 0.5029\n",
      "Epoch 2008/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 4.6020e-09 - val_loss: 0.5029\n",
      "Epoch 2009/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.8675e-09 - val_loss: 0.5034\n",
      "Epoch 2010/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.0445e-09 - val_loss: 0.5033\n",
      "Epoch 2011/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.1330e-09 - val_loss: 0.5034\n",
      "Epoch 2012/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 4.8784e-0 - 0s 55us/sample - loss: 4.8675e-09 - val_loss: 0.5033\n",
      "Epoch 2013/5000\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 4.7790e-09 - val_loss: 0.5043\n",
      "Epoch 2014/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 5.3491e-0 - 0s 59us/sample - loss: 5.3100e-09 - val_loss: 0.5044\n",
      "Epoch 2015/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 4.8675e-09 - val_loss: 0.5047\n",
      "Epoch 2016/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1347/1347 [==============================] - 0s 55us/sample - loss: 4.9560e-09 - val_loss: 0.50350\n",
      "Epoch 2017/5000\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 4.9560e-09 - val_loss: 0.5042\n",
      "Epoch 2018/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 4.5135e-09 - val_loss: 0.5043\n",
      "Epoch 2019/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.2215e-09 - val_loss: 0.5044\n",
      "Epoch 2020/5000\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 4.5135e-09 - val_loss: 0.5043\n",
      "Epoch 2021/5000\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 4.9560e-09 - val_loss: 0.5042\n",
      "Epoch 2022/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 5.3100e-09 - val_loss: 0.5043\n",
      "Epoch 2023/5000\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 5.0445e-09 - val_loss: 0.5041\n",
      "Epoch 2024/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 4.4250e-09 - val_loss: 0.5037\n",
      "Epoch 2025/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 5.1330e-09 - val_loss: 0.5040\n",
      "Epoch 2026/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.6905e-09 - val_loss: 0.5040\n",
      "Epoch 2027/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.3100e-09 - val_loss: 0.5038\n",
      "Epoch 2028/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.5135e-09 - val_loss: 0.5042\n",
      "Epoch 2029/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 4.7790e-09 - val_loss: 0.5038\n",
      "Epoch 2030/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 5.0445e-09 - val_loss: 0.5040\n",
      "Epoch 2031/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 4.3365e-09 - val_loss: 0.5038\n",
      "Epoch 2032/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.2215e-09 - val_loss: 0.5042\n",
      "Epoch 2033/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 4.7790e-09 - val_loss: 0.5044\n",
      "Epoch 2034/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 4.4250e-09 - val_loss: 0.5035\n",
      "Epoch 2035/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.2215e-09 - val_loss: 0.5044\n",
      "Epoch 2036/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 5.1330e-09 - val_loss: 0.5033\n",
      "Epoch 2037/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 5.4870e-09 - val_loss: 0.5035\n",
      "Epoch 2038/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 4.6905e-09 - val_loss: 0.5033\n",
      "Epoch 2039/5000\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 5.2215e-09 - val_loss: 0.5038\n",
      "Epoch 2040/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 4.6020e-09 - val_loss: 0.5039\n",
      "Epoch 2041/5000\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 5.2215e-09 - val_loss: 0.5038\n",
      "Epoch 2042/5000\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 5.0445e-09 - val_loss: 0.5041\n",
      "Epoch 2043/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 4.7790e-09 - val_loss: 0.5042\n",
      "Epoch 2044/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 5.0445e-09 - val_loss: 0.5043\n",
      "Epoch 2045/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 4.7790e-09 - val_loss: 0.5052\n",
      "Epoch 2046/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.1330e-09 - val_loss: 0.5047\n",
      "Epoch 2047/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 4.0710e-09 - val_loss: 0.5047\n",
      "Epoch 2048/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.9560e-09 - val_loss: 0.5041\n",
      "Epoch 2049/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.2215e-09 - val_loss: 0.5047\n",
      "Epoch 2050/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.5135e-09 - val_loss: 0.5044\n",
      "Epoch 2051/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.2215e-09 - val_loss: 0.5051\n",
      "Epoch 2052/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 5.2215e-09 - val_loss: 0.5044\n",
      "Epoch 2053/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.7790e-09 - val_loss: 0.5046\n",
      "Epoch 2054/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.8675e-09 - val_loss: 0.5047\n",
      "Epoch 2055/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 4.8675e-09 - val_loss: 0.5051\n",
      "Epoch 2056/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 4.8675e-09 - val_loss: 0.5044\n",
      "Epoch 2057/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 4.7790e-09 - val_loss: 0.5046\n",
      "Epoch 2058/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 4.7790e-09 - val_loss: 0.5049\n",
      "Epoch 2059/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.3100e-09 - val_loss: 0.5043\n",
      "Epoch 2060/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 4.8675e-09 - val_loss: 0.5042\n",
      "Epoch 2061/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 4.6020e-09 - val_loss: 0.5045\n",
      "Epoch 2062/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 4.7790e-09 - val_loss: 0.5042\n",
      "Epoch 2063/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 4.8675e-09 - val_loss: 0.5034\n",
      "Epoch 2064/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 4.7790e-09 - val_loss: 0.5031\n",
      "Epoch 2065/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 4.6905e-09 - val_loss: 0.5042\n",
      "Epoch 2066/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 5.1330e-09 - val_loss: 0.5042\n",
      "Epoch 2067/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 4.6905e-09 - val_loss: 0.5038\n",
      "Epoch 2068/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 4.6905e-09 - val_loss: 0.5042\n",
      "Epoch 2069/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 4.6905e-09 - val_loss: 0.5048\n",
      "Epoch 2070/5000\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 4.8675e-09 - val_loss: 0.5046\n",
      "Epoch 2071/5000\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 4.7790e-09 - val_loss: 0.5046\n",
      "Epoch 2072/5000\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 4.6020e-09 - val_loss: 0.5040\n",
      "Epoch 2073/5000\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 4.6905e-09 - val_loss: 0.5043\n",
      "Epoch 2074/5000\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 4.6905e-09 - val_loss: 0.5048\n",
      "Epoch 2075/5000\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 4.7790e-09 - val_loss: 0.5039\n",
      "Epoch 2076/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 4.7790e-09 - val_loss: 0.5049\n",
      "Epoch 2077/5000\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 3.8055e-09 - val_loss: 0.5047\n",
      "Epoch 2078/5000\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 5.6640e-09 - val_loss: 0.5045\n",
      "Epoch 2079/5000\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 4.9560e-09 - val_loss: 0.5048\n",
      "Epoch 2080/5000\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 4.9560e-09 - val_loss: 0.5049\n",
      "Epoch 2081/5000\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 4.9560e-09 - val_loss: 0.5047\n",
      "Epoch 2082/5000\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 4.6020e-09 - val_loss: 0.5054\n",
      "Epoch 2083/5000\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 4.7790e-09 - val_loss: 0.5052\n",
      "Epoch 2084/5000\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 4.8675e-09 - val_loss: 0.5054\n",
      "Epoch 2085/5000\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 4.5135e-09 - val_loss: 0.5056\n",
      "Epoch 2086/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 4.6905e-09 - val_loss: 0.5053\n",
      "Epoch 2087/5000\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 4.8675e-09 - val_loss: 0.5059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2088/5000\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 4.9560e-09 - val_loss: 0.50520\n",
      "Epoch 2089/5000\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 4.3365e-09 - val_loss: 0.5049\n",
      "Epoch 2090/5000\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 4.8675e-09 - val_loss: 0.5057\n",
      "Epoch 2091/5000\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 4.8675e-09 - val_loss: 0.5053\n",
      "Epoch 2092/5000\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 4.7790e-09 - val_loss: 0.5048\n",
      "Epoch 2093/5000\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 4.8675e-09 - val_loss: 0.5045\n",
      "Epoch 2094/5000\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 4.9560e-09 - val_loss: 0.5053\n",
      "Epoch 2095/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 4.8675e-09 - val_loss: 0.5056\n",
      "Epoch 2096/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 4.9560e-09 - val_loss: 0.5053\n",
      "Epoch 2097/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 5.1330e-09 - val_loss: 0.5055\n",
      "Epoch 2098/5000\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 4.5135e-09 - val_loss: 0.5056\n",
      "Epoch 2099/5000\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 4.6020e-09 - val_loss: 0.5055\n",
      "Epoch 2100/5000\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 5.0445e-09 - val_loss: 0.5048\n",
      "Epoch 2101/5000\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 4.8675e-09 - val_loss: 0.5055\n",
      "Epoch 2102/5000\n",
      "1347/1347 [==============================] - 0s 64us/sample - loss: 4.5135e-09 - val_loss: 0.5056\n",
      "Epoch 2103/5000\n",
      "1347/1347 [==============================] - 0s 68us/sample - loss: 4.6905e-09 - val_loss: 0.5054\n",
      "Epoch 2104/5000\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 4.7790e-09 - val_loss: 0.5054\n",
      "Epoch 2105/5000\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 4.5135e-09 - val_loss: 0.5058\n",
      "Epoch 2106/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 4.6905e-09 - val_loss: 0.5052\n",
      "Epoch 2107/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 4.5135e-09 - val_loss: 0.5050\n",
      "Epoch 2108/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 4.7790e-09 - val_loss: 0.5050\n",
      "Epoch 2109/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 4.8675e-09 - val_loss: 0.5047\n",
      "Epoch 2110/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.2215e-09 - val_loss: 0.5049\n",
      "Epoch 2111/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.0445e-09 - val_loss: 0.5054\n",
      "Epoch 2112/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.4250e-09 - val_loss: 0.5061\n",
      "Epoch 2113/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.5135e-09 - val_loss: 0.5055\n",
      "Epoch 2114/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 4.4250e-09 - val_loss: 0.5053\n",
      "Epoch 2115/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 5.2215e-09 - val_loss: 0.5056\n",
      "Epoch 2116/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 5.3100e-09 - val_loss: 0.5055\n",
      "Epoch 2117/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 4.7790e-09 - val_loss: 0.5060\n",
      "Epoch 2118/5000\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 5.3985e-09 - val_loss: 0.5058\n",
      "Epoch 2119/5000\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 4.5135e-09 - val_loss: 0.5051\n",
      "Epoch 2120/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 4.9560e-09 - val_loss: 0.5043\n",
      "Epoch 2121/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 4.8675e-09 - val_loss: 0.5062\n",
      "Epoch 2122/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 3.7253e-0 - 0s 47us/sample - loss: 4.8675e-09 - val_loss: 0.5059\n",
      "Epoch 2123/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 4.6905e-09 - val_loss: 0.5055\n",
      "Epoch 2124/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 5.1330e-09 - val_loss: 0.5062\n",
      "Epoch 2125/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 5.0445e-09 - val_loss: 0.5055\n",
      "Epoch 2126/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 7.4506e-0 - 0s 51us/sample - loss: 5.9295e-09 - val_loss: 0.5054\n",
      "Epoch 2127/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 4.3365e-09 - val_loss: 0.5056\n",
      "Epoch 2128/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 4.8675e-09 - val_loss: 0.5064\n",
      "Epoch 2129/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 5.1330e-09 - val_loss: 0.5060\n",
      "Epoch 2130/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 4.7790e-09 - val_loss: 0.5052\n",
      "Epoch 2131/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 5.0445e-09 - val_loss: 0.5055\n",
      "Epoch 2132/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 4.7790e-09 - val_loss: 0.5056\n",
      "Epoch 2133/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 4.5135e-09 - val_loss: 0.5053\n",
      "Epoch 2134/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 4.5135e-09 - val_loss: 0.5059\n",
      "Epoch 2135/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 4.6020e-09 - val_loss: 0.5050\n",
      "Epoch 2136/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.5135e-09 - val_loss: 0.5055\n",
      "Epoch 2137/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.9560e-09 - val_loss: 0.5048\n",
      "Epoch 2138/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.8675e-09 - val_loss: 0.5051\n",
      "Epoch 2139/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.6905e-09 - val_loss: 0.5049\n",
      "Epoch 2140/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 5.0445e-09 - val_loss: 0.5050\n",
      "Epoch 2141/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.8675e-09 - val_loss: 0.5052\n",
      "Epoch 2142/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.7790e-09 - val_loss: 0.5052\n",
      "Epoch 2143/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.3365e-09 - val_loss: 0.5060\n",
      "Epoch 2144/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 5.1330e-09 - val_loss: 0.5060\n",
      "Epoch 2145/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.6905e-09 - val_loss: 0.5056\n",
      "Epoch 2146/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.2480e-09 - val_loss: 0.5052\n",
      "Epoch 2147/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.2215e-09 - val_loss: 0.5058\n",
      "Epoch 2148/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.9560e-09 - val_loss: 0.5055\n",
      "Epoch 2149/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.3365e-09 - val_loss: 0.5054\n",
      "Epoch 2150/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 5.4870e-09 - val_loss: 0.5041\n",
      "Epoch 2151/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.4250e-09 - val_loss: 0.5047\n",
      "Epoch 2152/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 5.2215e-09 - val_loss: 0.5047\n",
      "Epoch 2153/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.6905e-09 - val_loss: 0.5042\n",
      "Epoch 2154/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 4.5135e-09 - val_loss: 0.5049\n",
      "Epoch 2155/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 4.8675e-09 - val_loss: 0.5045\n",
      "Epoch 2156/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.8675e-09 - val_loss: 0.5045\n",
      "Epoch 2157/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.8675e-09 - val_loss: 0.5045\n",
      "Epoch 2158/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.7790e-09 - val_loss: 0.5048\n",
      "Epoch 2159/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.6905e-09 - val_loss: 0.5039\n",
      "Epoch 2160/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.9560e-09 - val_loss: 0.5036\n",
      "Epoch 2161/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 4.9560e-09 - val_loss: 0.5043\n",
      "Epoch 2162/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 4.6905e-09 - val_loss: 0.5040\n",
      "Epoch 2163/5000\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 4.2480e-09 - val_loss: 0.5051\n",
      "Epoch 2164/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 4.6020e-09 - val_loss: 0.5046\n",
      "Epoch 2165/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 4.6020e-09 - val_loss: 0.5044\n",
      "Epoch 2166/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.5135e-09 - val_loss: 0.5048\n",
      "Epoch 2167/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 4.7790e-09 - val_loss: 0.5049\n",
      "Epoch 2168/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 5.2215e-09 - val_loss: 0.5046\n",
      "Epoch 2169/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.6905e-09 - val_loss: 0.5041\n",
      "Epoch 2170/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 5.1330e-09 - val_loss: 0.5041\n",
      "Epoch 2171/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.6020e-09 - val_loss: 0.5047\n",
      "Epoch 2172/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 4.8675e-09 - val_loss: 0.5049\n",
      "Epoch 2173/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.8675e-09 - val_loss: 0.5036\n",
      "Epoch 2174/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.7790e-09 - val_loss: 0.5032\n",
      "Epoch 2175/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 5.3100e-09 - val_loss: 0.5040\n",
      "Epoch 2176/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 5.1330e-09 - val_loss: 0.5048\n",
      "Epoch 2177/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.1595e-09 - val_loss: 0.5034\n",
      "Epoch 2178/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 0.0000e+0 - 0s 44us/sample - loss: 4.4250e-09 - val_loss: 0.5041\n",
      "Epoch 2179/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 3.7253e-0 - 0s 43us/sample - loss: 4.8675e-09 - val_loss: 0.5040\n",
      "Epoch 2180/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 4.9560e-09 - val_loss: 0.5039\n",
      "Epoch 2181/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 5.3100e-09 - val_loss: 0.5042\n",
      "Epoch 2182/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.6905e-09 - val_loss: 0.5037\n",
      "Epoch 2183/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.6020e-09 - val_loss: 0.5053\n",
      "Epoch 2184/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.5135e-09 - val_loss: 0.5037\n",
      "Epoch 2185/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 4.5135e-09 - val_loss: 0.5039\n",
      "Epoch 2186/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 4.5135e-09 - val_loss: 0.5039\n",
      "Epoch 2187/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 4.7790e-09 - val_loss: 0.5037\n",
      "Epoch 2188/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 4.6020e-09 - val_loss: 0.5037\n",
      "Epoch 2189/5000\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 0.0000e+0 - 0s 43us/sample - loss: 5.0445e-09 - val_loss: 0.5035\n",
      "Epoch 2190/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 4.0710e-09 - val_loss: 0.5041\n",
      "Epoch 2191/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 4.9560e-09 - val_loss: 0.5035\n",
      "Epoch 2192/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 4.3365e-09 - val_loss: 0.5035\n",
      "Epoch 2193/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.5135e-09 - val_loss: 0.5034\n",
      "Epoch 2194/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 4.3365e-09 - val_loss: 0.5038\n",
      "Epoch 2195/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 4.6020e-09 - val_loss: 0.5035\n",
      "Epoch 2196/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 4.6020e-09 - val_loss: 0.5038\n",
      "Epoch 2197/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 4.6020e-09 - val_loss: 0.5037\n",
      "Epoch 2198/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.8675e-09 - val_loss: 0.5029\n",
      "Epoch 2199/5000\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 5.1330e-09 - val_loss: 0.5033\n",
      "Epoch 2200/5000\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 4.1595e-09 - val_loss: 0.5038\n",
      "Epoch 2201/5000\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 5.4870e-09 - val_loss: 0.5045\n",
      "Epoch 2202/5000\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 4.4250e-09 - val_loss: 0.5038\n",
      "Epoch 2203/5000\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 4.6905e-09 - val_loss: 0.5039\n",
      "Epoch 2204/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 4.6905e-09 - val_loss: 0.5036\n",
      "Epoch 2205/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 4.2480e-09 - val_loss: 0.5032\n",
      "Epoch 2206/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 4.5135e-09 - val_loss: 0.5029\n",
      "Epoch 2207/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 5.1330e-09 - val_loss: 0.5033\n",
      "Epoch 2208/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 4.8675e-09 - val_loss: 0.5027\n",
      "Epoch 2209/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 4.7790e-09 - val_loss: 0.5028\n",
      "Epoch 2210/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 4.9560e-09 - val_loss: 0.5031\n",
      "Epoch 2211/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.8675e-09 - val_loss: 0.5033\n",
      "Epoch 2212/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.7790e-09 - val_loss: 0.5033\n",
      "Epoch 2213/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 4.7790e-09 - val_loss: 0.5038\n",
      "Epoch 2214/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 4.4250e-09 - val_loss: 0.5033\n",
      "Epoch 2215/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 4.3365e-09 - val_loss: 0.5031\n",
      "Epoch 2216/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.2480e-09 - val_loss: 0.5030\n",
      "Epoch 2217/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 4.9560e-09 - val_loss: 0.5036\n",
      "Epoch 2218/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 4.6020e-09 - val_loss: 0.5035\n",
      "Epoch 2219/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.3365e-09 - val_loss: 0.5035\n",
      "Epoch 2220/5000\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 4.6020e-09 - val_loss: 0.5022\n",
      "Epoch 2221/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.9560e-09 - val_loss: 0.5028\n",
      "Epoch 2222/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 4.2480e-09 - val_loss: 0.5029\n",
      "Epoch 2223/5000\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 5.2215e-09 - val_loss: 0.5033\n",
      "Epoch 2224/5000\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 4.3365e-09 - val_loss: 0.5028\n",
      "Epoch 2225/5000\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 4.5135e-09 - val_loss: 0.5025\n",
      "Epoch 2226/5000\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 4.9560e-09 - val_loss: 0.5031\n",
      "Epoch 2227/5000\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 4.9560e-09 - val_loss: 0.5025\n",
      "Epoch 2228/5000\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 4.4250e-09 - val_loss: 0.5024\n",
      "Epoch 2229/5000\n",
      "1347/1347 [==============================] - 0s 38us/sample - loss: 4.5135e-09\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-110-e0c092e372ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# This builds the model for the first time:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_tr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    335\u001b[0m                 \u001b[0meval_data_iter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initializer\u001b[0m  \u001b[1;31m# pylint: disable=pointless-statement\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m               \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 337\u001b[1;33m                 \u001b[0meval_data_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m               validation_callbacks = cbks.configure_callbacks(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    330\u001b[0m     if (context.executing_eagerly()\n\u001b[0;32m    331\u001b[0m         or ops.get_default_graph()._building_function):  # pylint: disable=protected-access\n\u001b[1;32m--> 332\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIteratorV2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    591\u001b[0m           context.context().device_spec.device_type != \"CPU\"):\n\u001b[0;32m    592\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/cpu:0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    594\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    609\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[1;32m--> 611\u001b[1;33m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    612\u001b[0m       \u001b[1;31m# Delete the resource when this object is deleted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m       self._resource_deleter = IteratorResourceDeleter(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   2913\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2914\u001b[0m         \u001b[1;34m\"MakeIterator\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2915\u001b[1;33m         iterator)\n\u001b[0m\u001b[0;32m   2916\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2917\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "activation = \"relu\"\n",
    "# activation = \"sigmoid\"\n",
    "\n",
    "print('Model with {} activation'.format(activation))\n",
    "\n",
    "inputs = Input(shape=(n_features,))\n",
    "# TODO:\n",
    "x = layers.Dense(n_hidden,activation = activation)(inputs)\n",
    "outputs = layers.Dense(n_classes,activation = \"softmax\")(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs, name='keras_model')\n",
    "\n",
    "model.compile(optimizer= 'Adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# This builds the model for the first time:\n",
    "model.fit(X_tr, Y_tr,validation_data=(X_val,Y_val), epochs=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that you know if the model is underfitting or overfitting:\n",
    "#### - In case of underfitting, can you explain why ? Also change the structure of the 2 previous networks to cancell underfitting\n",
    "#### - In case of overfitting, explain why and change the structure of the 2 previous networks to cancell the overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our model does not overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
