{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "EPSILON = 1e-8 # small constant to avoid underflow or divide per 0\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I - Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This time, the data will correspond to greyscale images. <br> Two different datasets can be used here:\n",
    "- The MNIST dataset, small 8*8 images, corresponding to handwritten digits &rightarrow; 10 classes\n",
    "- The Fashion MNIST dataset, medium 28*28 images, corresponding to clothes pictures &rightarrow; 10 classes\n",
    "\n",
    "#### Starting with the simple MNIST is recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"MNIST\"\n",
    "# dataset = \"FASHION_MNIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset='MNIST'):\n",
    "    if dataset == 'MNIST':\n",
    "        digits = load_digits()\n",
    "        X, Y = np.asarray(digits['data'], dtype='float32'), np.asarray(digits['target'], dtype='int32')\n",
    "        return X, Y\n",
    "    elif dataset == 'FASHION_MNIST':\n",
    "        import tensorflow as tf\n",
    "        fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "        (X, Y), (_, _) = fashion_mnist.load_data()\n",
    "        X = X.reshape((X.shape[0], X.shape[1] * X.shape[2]))\n",
    "        X, Y = np.asarray(X, dtype='float32'), np.asarray(Y, dtype='int32')\n",
    "        return X, Y\n",
    "X, Y = load_data(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(np.unique(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 1797\n",
      "Input dimension: 64\n",
      "Number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples: {:d}'.format(X.shape[0]))\n",
    "print('Input dimension: {:d}'.format(X.shape[1]))  # images 8x8 or 28*28 actually\n",
    "print('Number of classes: {:d}'.format(n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range max-min of greyscale pixel values: (16.0, 0.0)\n",
      "First image sample:\n",
      "[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.\n",
      " 15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.\n",
      "  0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.\n",
      "  0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]\n",
      "First image label: 0\n",
      "Input design matrix shape: (1797, 64)\n"
     ]
    }
   ],
   "source": [
    "print(\"Range max-min of greyscale pixel values: ({0:.1f}, {1:.1f})\".format(np.max(X), np.min(X)))\n",
    "print(\"First image sample:\\n{0}\".format(X[0]))\n",
    "print(\"First image label: {0}\".format(Y[0]))\n",
    "print(\"Input design matrix shape: {0}\".format(X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does the data look like?\n",
    "Each image in the dataset consists of a 8 x 8 (or 28 x 28) matrix, of greyscale pixels. For the MNIST dataset, the values are between 0 and 16 where 0 represents white, 16 represents black and there are many shades of grey in-between. For the Fashion MNIST dataset, the values are between 0 and 255.<br>Each image is assigned a corresponding numerical label, so the image in ```X[i]``` has its corresponding label stored in ```Y[i]```.\n",
    "\n",
    "The next cells below demonstrate how to visualise the input data. Make sure you understand what's happening, particularly how the indices correspond to individual items in the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data_sample(X, Y, nrows=2, ncols=2):\n",
    "    fig, ax = plt.subplots(nrows, ncols)\n",
    "    for row in ax:\n",
    "        for col in row:\n",
    "            index = random.randint(0, X.shape[0])\n",
    "            dim = np.sqrt(X.shape[1]).astype(int)\n",
    "            col.imshow(X[index].reshape((dim, dim)), cmap=plt.cm.gray_r)\n",
    "            col.set_title(\"image label: %d\" % Y[index])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_data_sample(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II - Multiclass classification MLP with Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II a) - Problem definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/mlp_mnist.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task here will be to implement \"from scratch\" a Multilayer Perceptron for classification.\n",
    "\n",
    "We will define the formal categorical cross entropy loss as follows:\n",
    "$$\n",
    "l(\\mathbf{\\Theta}, \\mathbf{X}, \\mathbf{Y}) = - \\frac{1}{n} \\sum_{i=1}^n \\log \\mathbf{f}(\\mathbf{x}_i ; \\mathbf{\\Theta})^\\top y_i\n",
    "$$\n",
    "<center>with $y_i$ being the one-hot encoded true label for the sample $i$, and $\\Theta = (\\mathbf{W}^h; \\mathbf{b}^h; \\mathbf{W}^o; \\mathbf{b}^o)$</center>\n",
    "<center>In addition, $\\mathbf{f}(\\mathbf{x}) = softmax(\\mathbf{z^o}(\\mathbf{x})) = softmax(\\mathbf{W}^o\\mathbf{h}(\\mathbf{x}) + \\mathbf{b}^o)$</center>\n",
    "<center>and $\\mathbf{h}(\\mathbf{x}) = g(\\mathbf{z^h}(\\mathbf{x})) = g(\\mathbf{W}^h\\mathbf{x} + \\mathbf{b}^h)$, $g$ being the activation function and could be implemented with $sigmoid$ or $relu$</center>\n",
    "\n",
    "## Objectives:\n",
    "- Write the categorical cross entropy loss function\n",
    "- Write the activation functions with their associated gradient\n",
    "- Write the softmax function that is going to be used to output the predicted probabilities\n",
    "- Implement the forward pass through the neural network\n",
    "- Implement the backpropagation according to the used loss: progagate the gradients using the chain rule and return $(\\mathbf{\\nabla_{W^h}}l ; \\mathbf{\\nabla_{b^h}}l ; \\mathbf{\\nabla_{W^o}}l ; \\mathbf{\\nabla_{b^o}}l)$\n",
    "- Implement dropout regularization in the forward pass: be careful to consider both training and prediction cases\n",
    "- Implement the SGD optimization algorithm, and improve it with simple momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple graph function to let you have a global overview:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/function_graph.png\" style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) You may find numpy outer products useful: <br>\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.outer.html <br>\n",
    "We have: $outer(u, v) = u \\cdot v^T$, with $u, v$ two vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, v = np.random.normal(size=(5,)), np.random.normal(size=(10,))\n",
    "assert np.array_equal(\n",
    "    np.outer(u, v),\n",
    "    np.dot(np.reshape(u, (u.size, 1)), np.reshape(v, (1, v.size)))\n",
    ")\n",
    "assert np.outer(u, v).shape == (5, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) You also may find numpy matmul function useful: <br>\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.matmul.html <br>\n",
    "It can be used to perform matrix products along one fixed dimension (i.e. the batch size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, B = np.random.randint(0, 100, size=(64, 5, 10)), np.random.randint(0, 100, size=(64, 10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.array_equal(\n",
    "    np.stack([np.dot(A_i, B_i) for A_i, B_i in zip(A, B)]),\n",
    "    np.matmul(A, B)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II b) - Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron():\n",
    "    \"\"\"MLP with one hidden layer having a hidden activation,\n",
    "    and one output layer having a softmax activation\"\"\"\n",
    "    def __init__(self, X, Y, hidden_size, activation='relu',\n",
    "                 initialization='uniform', dropout=False, dropout_rate=1):\n",
    "        # input, hidden, and output dimensions on the MLP based on X, Y\n",
    "        self.input_size, self.output_size = X.shape[1], len(np.unique(Y))\n",
    "        self.hidden_size = hidden_size\n",
    "        # initialization strategies: avoid a full-0 initialization of the weight matricest\n",
    "        if initialization == 'uniform':\n",
    "            self.W_h = np.random.uniform(size=(self.hidden_size, self.input_size), high=0.01, low=-0.01)\n",
    "            self.W_o = np.random.uniform(size=(self.output_size, self.hidden_size), high=0.01, low=-0.01)\n",
    "        elif initialization == 'normal':\n",
    "            self.W_h = np.random.normal(size=(self.hidden_size, self.input_size), loc=0, scale=0.01)\n",
    "            self.W_o = np.random.normal(size=(self.output_size, self.hidden_size), loc=0, scale=0.01)\n",
    "        # the bias could be initializated to 0 or a random low constant\n",
    "        self.b_h = np.zeros(self.hidden_size)\n",
    "        self.b_o = np.zeros(self.output_size)\n",
    "        # our namedtuple structure of gradients\n",
    "        self.Grads = namedtuple('Grads', ['W_h', 'b_h', 'W_o', 'b_o'])\n",
    "        # and the velocities associated which are going to be useful for the momentum\n",
    "        self.velocities = {'W_h': 0., 'b_h': 0., 'W_o': 0., 'b_o': 0.}\n",
    "        # the hidden activation function used\n",
    "        self.activation = activation\n",
    "        # arrays to track back the losses and accuracies evolution\n",
    "        self.training_losses_history = []\n",
    "        self.validation_losses_history = []\n",
    "        self.training_acc_history = []\n",
    "        self.validation_acc_history = []\n",
    "        # train val split and normalization of the features\n",
    "        self.X_tr, self.X_val, self.Y_tr, self.Y_val = self.split_train_validation(X, Y)\n",
    "        self.scaler = MinMaxScaler(feature_range=(0, 1), copy=False)\n",
    "        self.X_tr = self.scaler.fit_transform(self.X_tr)\n",
    "        self.X_val = self.scaler.transform(self.X_val)\n",
    "        # dropout parameters\n",
    "        self.dropout = dropout\n",
    "        self.dropout_rate = dropout_rate\n",
    "        # step used for the optimization algorithm and setted later\n",
    "    \n",
    "    # One-hot encoding of the target\n",
    "    # Transform the integer represensation to a sparse one\n",
    "    @staticmethod\n",
    "    def one_hot(n_classes, Y):\n",
    "        return np.eye(n_classes)[Y]\n",
    "    \n",
    "    # Reverse one-hot encoding of the target\n",
    "    # Recover the former integer representation\n",
    "    # ex: from (0,0,1,0) to 2\n",
    "    @staticmethod\n",
    "    def reverse_one_hot(Y_one_hot):\n",
    "        return np.asarray(np.where(Y_one_hot==1)[1], dtype='int32')\n",
    "    \n",
    "    \"\"\"\n",
    "    Activation functions and their gradient\n",
    "    \"\"\"\n",
    "    # In implementations below X is a matrix of shape (n_samples, p)\n",
    "    \n",
    "    # A max_value value is indicated for the relu and grad_relu functions\n",
    "    # Make sure to clip the output to it to prevent numerical overflow (exploding gradient)\n",
    "    # Make it so the max value reachable is max_value\n",
    "    @staticmethod\n",
    "    def relu(X, max_value=20):\n",
    "        assert max_value > 0\n",
    "        # TODO:\n",
    "        return np.clip(X,0,max_value)\n",
    "    \n",
    "    # Make it so the gradient becomes 0 when X becomes greater than max_value\n",
    "    @staticmethod\n",
    "    def grad_relu(X, max_value=20):\n",
    "        assert max_value > 0\n",
    "        # TODO:\n",
    "        grad_logic = (X >= 0).all() and (X < max_value).all()\n",
    "        return grad_logic.astype(int)\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(X):\n",
    "        # TODO:\n",
    "        return 1/(1+np.exp(-X))\n",
    "    \n",
    "    @staticmethod\n",
    "    def grad_sigmoid(X):\n",
    "        # TODO:\n",
    "        return self.sigmoid(X)*(1-self.sigmoid(X))\n",
    "    \n",
    "    # Softmax function to output probabilities\n",
    "    @staticmethod\n",
    "    def softmax(X):\n",
    "        # TODO:\n",
    "        return np.exp(X)/np.sum(np.exp(X))\n",
    "    \n",
    "    # Loss function\n",
    "    # Consider using EPSILON to prevent numerical issues (log(0) is undefined)\n",
    "    # Y_true and Y_pred are of shape (n_samples,n_classes)\n",
    "    @staticmethod\n",
    "    def categorical_cross_entropy(Y_true, Y_pred):\n",
    "        # TODO:\n",
    "        n = Y_true.shape[0]\n",
    "        a = -1/n        \n",
    "        return a*np.sum(log(Y_pred.T)*Y_true)\n",
    "    \n",
    "    @staticmethod\n",
    "    def split_train_validation(X, Y, test_size=0.25, seed=False):\n",
    "        random_state = 42 if seed else np.random.randint(1e3)\n",
    "        X_tr, X_val, Y_tr, Y_val = train_test_split(X, Y, test_size=test_size, random_state=random_state)\n",
    "        return X_tr, X_val, Y_tr, Y_val\n",
    "    \n",
    "    # Sample random batch in (X, Y) with a given batch_size for SGD\n",
    "    @staticmethod\n",
    "    def get_random_batch(X, Y, batch_size):\n",
    "        indexes = np.random.choice(X.shape[0], size=batch_size, replace=False)\n",
    "        return X[indexes], Y[indexes]\n",
    "        \n",
    "    # Forward pass: compute f(x) as y, and return optionally the hidden states h(x) and z_h(x) for compute_grads\n",
    "    def forward(self, X, return_activation=False, training=False):\n",
    "        if self.activation == 'relu':\n",
    "            g_activation = self.relu\n",
    "        elif self.activation == 'sigmoid':\n",
    "            g_activation = self.sigmoid\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "            \n",
    "        #z_h = np.zeros((X.shape[0], self.hidden_size)) if len(X.shape) > 1 else np.zeros(self.hidden_size)\n",
    "        #h = np.zeros((X.shape[0], self.hidden_size)) if len(X.shape) > 1 else np.zeros(self.hidden_size)\n",
    "        # TODO:\n",
    "        z_h = np.sum(np.dot(X.T,self.W_h)+self.b_h)\n",
    "        h = g_activation(z_h)\n",
    "        \n",
    "        if self.dropout:\n",
    "            if training:\n",
    "                # TODO:\n",
    "                h_size = h.shape[0]\n",
    "                dropout_vec = np.binomial(1,self.dropout_rate,h_size)\n",
    "                h = h*dropout_vec\n",
    "            else:\n",
    "                # TODO:\n",
    "                h = h*self.dropout_rate\n",
    "        # TODO:\n",
    "        #y = np.zeros((X.shape[0], self.output_size)) if len(X.shape) > 1 else np.zeros(self.output_size)        \n",
    "        z_o=np.sum(np.dot(h.T,self.W_o)+self.b_o)\n",
    "            \n",
    "        y = self.softmax(z_o)\n",
    "        if return_activation:\n",
    "            return y, h, z_h\n",
    "        else:\n",
    "            return y\n",
    "    \n",
    "    # Backpropagation: return an instantiation of self.Grads that contains the average gradients for the given batch\n",
    "    def compute_grads(self, X, Y_true, vectorized=False):\n",
    "        if self.activation == 'relu':\n",
    "            g_grad = self.grad_relu\n",
    "        elif self.activation == 'sigmoid':\n",
    "            g_grad = self.grad_sigmoid\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        if len(X.shape) == 1:\n",
    "            X = X.reshape((1,) + X.shape)\n",
    "        \n",
    "        if not vectorized:\n",
    "            n = X.shape[0]\n",
    "            #grad_W_h = np.zeros((self.hidden_size, self.input_size))\n",
    "            #grad_b_h = np.zeros((self.hidden_size, )) \n",
    "            #grad_W_o = np.zeros((self.output_size, self.hidden_size))\n",
    "            #grad_b_o = np.zeros((self.output_size, ))\n",
    "            grad_W_h = \n",
    "            for x, y_true in zip(X, Y_true):\n",
    "                y_pred, h, z_h = self.forward(x, return_activation=True, training=True)\n",
    "                \n",
    "                # TODO:\n",
    "                \n",
    "            grads = self.Grads(grad_W_h/n, grad_b_h/n, grad_W_o/n, grad_b_o/n)\n",
    "            \n",
    "        else: \n",
    "            Y_pred, h, z_h = self.forward(X, return_activation=True, training=True)\n",
    "\n",
    "            # TODO (optional), try to do the backprop without Python loops in a vectorized way:\n",
    "            \n",
    "            grad_W_h = np.zeros((X.shape[0], self.hidden_size, self.input_size))\n",
    "            grad_b_h = np.zeros((X.shape[0], self.hidden_size, )) \n",
    "            grad_W_o = np.zeros((X.shape[0], self.output_size, self.hidden_size))\n",
    "            grad_b_o = np.zeros((X.shape[0], self.output_size, ))\n",
    "            \n",
    "            grads = self.Grads(\n",
    "                np.mean(grad_W_h, axis=0), \n",
    "                np.mean(grad_b_h, axis=0), \n",
    "                np.mean(grad_W_o, axis=0), \n",
    "                np.mean(grad_b_o, axis=0)\n",
    "            )\n",
    "            \n",
    "        return grads\n",
    "    \n",
    "    # Perform the update of the parameters (W_h, b_h, W_o, b_o) based of their gradient\n",
    "    def optimizer_step(self, optimizer='gd', momentum=False, momentum_alpha=0.9, \n",
    "                       batch_size=None, vectorized=True):\n",
    "        if optimizer == 'gd':\n",
    "            grads = self.compute_grads(self.X_tr, self.Y_tr, vectorized=vectorized)\n",
    "        elif optimizer == 'sgd':\n",
    "            batch_X_tr, batch_Y_tr = self.get_random_batch(self.X_tr, self.Y_tr, batch_size)\n",
    "            grads = self.compute_grads(batch_X_tr, batch_Y_tr, vectorized=vectorized)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        if not momentum:\n",
    "            # TODO:\n",
    "            pass\n",
    "        else:\n",
    "            # remember: use the stored velocities\n",
    "            # TODO:\n",
    "            pass\n",
    "    \n",
    "    # Loss wrapper\n",
    "    def loss(self, Y_true, Y_pred):\n",
    "        return self.categorical_cross_entropy(self.one_hot(self.output_size, Y_true), Y_pred)\n",
    "    \n",
    "    def loss_history_flush(self):\n",
    "        self.training_losses_history = []\n",
    "        self.validation_losses_history = []\n",
    "        \n",
    "    # Main function that trains the MLP with a design matrix X and a target vector Y\n",
    "    def train(self, optimizer='sgd', momentum=False, min_iterations=500, max_iterations=5000, initial_step=1e-1,\n",
    "              batch_size=64, early_stopping=True, early_stopping_lookbehind=100, early_stopping_delta=1e-4, \n",
    "              vectorized=False, flush_history=True, verbose=True):\n",
    "        if flush_history:\n",
    "            self.loss_history_flush()\n",
    "        cpt_patience, best_validation_loss = 0, np.inf\n",
    "        iteration_number = 0\n",
    "        self.step = initial_step\n",
    "        while len(self.training_losses_history) < max_iterations:\n",
    "            iteration_number += 1\n",
    "            self.optimizer_step(\n",
    "                optimizer=optimizer, momentum=momentum, batch_size=batch_size, vectorized=vectorized\n",
    "            )\n",
    "            \n",
    "            training_loss = self.loss(self.Y_tr, self.forward(self.X_tr))\n",
    "            self.training_losses_history.append(training_loss)\n",
    "            training_accuracy = self.accuracy_on_train()\n",
    "            self.training_acc_history.append(training_accuracy)\n",
    "            validation_loss = self.loss(self.Y_val, self.forward(self.X_val))\n",
    "            self.validation_losses_history.append(validation_loss)\n",
    "            validation_accuracy = self.accuracy_on_validation()\n",
    "            self.validation_acc_history.append(validation_accuracy)\n",
    "            if iteration_number > min_iterations and early_stopping:\n",
    "                if validation_loss + early_stopping_delta < best_validation_loss:\n",
    "                    best_validation_loss = validation_loss\n",
    "                    cpt_patience = 0\n",
    "                else:\n",
    "                    cpt_patience += 1\n",
    "            if verbose:\n",
    "                msg = \"iteration number: {0}\\t training loss: {1:.4f}\\t\" + \\\n",
    "                \"validation loss: {2:.4f}\\t validation accuracy: {3:.4f}\"\n",
    "                print(msg.format(iteration_number, \n",
    "                                 training_loss, \n",
    "                                 validation_loss,\n",
    "                                 validation_accuracy))\n",
    "            if cpt_patience >= early_stopping_lookbehind:\n",
    "                break\n",
    "    \n",
    "    # Return the predicted class once the MLP has been trained\n",
    "    def predict(self, X, normalize=True):\n",
    "        if normalize:\n",
    "            X = self.scaler.transform(X)\n",
    "        if len(X.shape) == 1:\n",
    "            return np.argmax(self.forward(X))\n",
    "        else:\n",
    "            return np.argmax(self.forward(X), axis=1)\n",
    "        \n",
    "    \"\"\"\n",
    "    Metrics and plots\n",
    "    \"\"\"\n",
    "    def accuracy_on_train(self):\n",
    "        return (self.predict(self.X_tr, normalize=False) == self.Y_tr).mean()\n",
    "\n",
    "    def accuracy_on_validation(self):\n",
    "        return (self.predict(self.X_val, normalize=False) == self.Y_val).mean()\n",
    "\n",
    "    def plot_loss_history(self, add_to_title=None):\n",
    "        import warnings\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.plot(range(len(self.training_losses_history)), \n",
    "                 self.training_losses_history, label='Training loss evolution')\n",
    "        plt.plot(range(len(self.validation_losses_history)), \n",
    "                 self.validation_losses_history, label='Validation loss evolution')\n",
    "        plt.legend(fontsize=15)\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel(\"iteration number\", fontsize=15)\n",
    "        plt.ylabel(\"Cross entropy loss\", fontsize=15)\n",
    "        base_title = \"Cross entropy loss evolution during training\"\n",
    "        if not self.dropout:\n",
    "            base_title += \", no dropout penalization\"\n",
    "        else:\n",
    "            base_title += \", {:.1f} dropout penalization\"\n",
    "            base_title = base_title.format(self.dropout_rate)\n",
    "        title = base_title + \", \" + add_to_title if add_to_title else base_title\n",
    "        plt.title(title, fontsize=20)\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_validation_prediction(self, sample_id):\n",
    "        fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "        classes = np.unique(self.Y_tr)\n",
    "        dim = np.sqrt(self.X_val.shape[1]).astype(int)\n",
    "        ax0.imshow(self.scaler.inverse_transform([self.X_val[sample_id]]).reshape(dim, dim), cmap=plt.cm.gray_r,\n",
    "                   interpolation='nearest')\n",
    "        ax0.set_title(\"True image label: %d\" % self.Y_val[sample_id]);\n",
    "\n",
    "        ax1.bar(classes, self.one_hot(len(classes), self.Y_val[sample_id]), label='true')\n",
    "        ax1.bar(classes, self.forward(self.X_val[sample_id]), label='prediction', color=\"red\")\n",
    "        ax1.set_xticks(classes)\n",
    "        prediction = self.predict(self.X_val[sample_id], normalize=False)\n",
    "        ax1.set_title('Output probabilities (prediction: %d)' % prediction)\n",
    "        ax1.set_xlabel('Digit class')\n",
    "        ax1.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MultiLayerPerceptron(X, Y, hidden_size=50, activation='relu')\n",
    "mlp.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "#### - Did you succeed to train the MLP and get a high validation accuracy? <br> Display available metrics (training and validation accuracies, training and validation losses)\n",
    "#### - Plot the prediction for a given validation sample. Is it accurate?\n",
    "#### - Compare the full gradient descent with the SGD.\n",
    "#### - Play with the hyper parameters you have: the hidden size, the activation function, the initial step and the batch size. <br> Comment. Don't hesitate to visualize results.\n",
    "#### - Once properly implemented, compare the training using early stopping, dropout, or both of them. <br> Why are these methods useful here?\n",
    "#### - Once properly implemented, compare the training using momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.plot_validation_prediction(sample_id=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.plot_loss_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III - Multiclass classification MLP with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Implement the same network architecture with Keras;\n",
    "    - First using the Sequential API\n",
    "    - Secondly using the functional API\n",
    "\n",
    "#### - Check that the Keras model can approximately reproduce the behavior of the Numpy model.\n",
    "\n",
    "#### - Compute the negative log likelihood of a sample 42 in the test set (can use `model.predict_proba`).\n",
    "\n",
    "#### - Compute the average negative log-likelihood on the full test set.\n",
    "\n",
    "#### - Compute the average negative log-likelihood  on the full training set and check that you can get the value of the loss reported by Keras.\n",
    "\n",
    "#### - Is the model overfitting or underfitting? (ensure that the model has fully converged by increasing the number of epochs to 500 or more if necessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_val, Y_tr, Y_val = train_test_split(X, Y)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1), copy=False)\n",
    "X_tr = scaler.fit_transform(X_tr)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X[0].shape[0]\n",
    "n_classes = len(np.unique(Y_tr))\n",
    "n_hidden = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "activation = \"relu\"\n",
    "# activation = \"sigmoid\"\n",
    "\n",
    "print('Model with {} activation'.format(activation))\n",
    "keras_model = Sequential()\n",
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "activation = \"relu\"\n",
    "# activation = \"sigmoid\"\n",
    "\n",
    "print('Model with {} activation'.format(activation))\n",
    "\n",
    "inputs = Input(shape=(n_features,))\n",
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that you know if the model is underfitting or overfitting:\n",
    "#### - In case of underfitting, can you explain why ? Also change the structure of the 2 previous networks to cancell underfitting\n",
    "#### - In case of overfitting, explain why and change the structure of the 2 previous networks to cancell the overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
