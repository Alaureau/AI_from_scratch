{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "EPSILON = 1e-8 # small constant to avoid underflow or divide per 0\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I - Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This time, the data will correspond to greyscale images. <br> Two different datasets can be used here:\n",
    "- The MNIST dataset, small 8*8 images, corresponding to handwritten digits &rightarrow; 10 classes\n",
    "- The Fashion MNIST dataset, medium 28*28 images, corresponding to clothes pictures &rightarrow; 10 classes\n",
    "\n",
    "#### Starting with the simple MNIST is recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"MNIST\"\n",
    "# dataset = \"FASHION_MNIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset='MNIST'):\n",
    "    if dataset == 'MNIST':\n",
    "        digits = load_digits()\n",
    "        X, Y = np.asarray(digits['data'], dtype='float32'), np.asarray(digits['target'], dtype='int32')\n",
    "        return X, Y\n",
    "    elif dataset == 'FASHION_MNIST':\n",
    "        import tensorflow as tf\n",
    "        fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "        (X, Y), (_, _) = fashion_mnist.load_data()\n",
    "        X = X.reshape((X.shape[0], X.shape[1] * X.shape[2]))\n",
    "        X, Y = np.asarray(X, dtype='float32'), np.asarray(Y, dtype='int32')\n",
    "        return X, Y\n",
    "X, Y = load_data(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(np.unique(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 1797\n",
      "Input dimension: 64\n",
      "Number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples: {:d}'.format(X.shape[0]))\n",
    "print('Input dimension: {:d}'.format(X.shape[1]))  # images 8x8 or 28*28 actually\n",
    "print('Number of classes: {:d}'.format(n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range max-min of greyscale pixel values: (16.0, 0.0)\n",
      "First image sample:\n",
      "[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.\n",
      " 15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.\n",
      "  0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.\n",
      "  0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]\n",
      "First image label: 0\n",
      "Input design matrix shape: (1797, 64)\n"
     ]
    }
   ],
   "source": [
    "print(\"Range max-min of greyscale pixel values: ({0:.1f}, {1:.1f})\".format(np.max(X), np.min(X)))\n",
    "print(\"First image sample:\\n{0}\".format(X[0]))\n",
    "print(\"First image label: {0}\".format(Y[0]))\n",
    "print(\"Input design matrix shape: {0}\".format(X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does the data look like?\n",
    "Each image in the dataset consists of a 8 x 8 (or 28 x 28) matrix, of greyscale pixels. For the MNIST dataset, the values are between 0 and 16 where 0 represents white, 16 represents black and there are many shades of grey in-between. For the Fashion MNIST dataset, the values are between 0 and 255.<br>Each image is assigned a corresponding numerical label, so the image in ```X[i]``` has its corresponding label stored in ```Y[i]```.\n",
    "\n",
    "The next cells below demonstrate how to visualise the input data. Make sure you understand what's happening, particularly how the indices correspond to individual items in the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data_sample(X, Y, nrows=2, ncols=2):\n",
    "    fig, ax = plt.subplots(nrows, ncols)\n",
    "    for row in ax:\n",
    "        for col in row:\n",
    "            index = random.randint(0, X.shape[0])\n",
    "            dim = np.sqrt(X.shape[1]).astype(int)\n",
    "            col.imshow(X[index].reshape((dim, dim)), cmap=plt.cm.gray_r)\n",
    "            col.set_title(\"image label: %d\" % Y[index])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_data_sample(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â II - Multiclass classification MLP with Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II a) - Problem definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/mlp_mnist.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task here will be to implement \"from scratch\" a Multilayer Perceptron for classification.\n",
    "\n",
    "We will define the formal categorical cross entropy loss as follows:\n",
    "$$\n",
    "l(\\mathbf{\\Theta}, \\mathbf{X}, \\mathbf{Y}) = - \\frac{1}{n} \\sum_{i=1}^n \\log \\mathbf{f}(\\mathbf{x}_i ; \\mathbf{\\Theta})^\\top y_i\n",
    "$$\n",
    "<center>with $y_i$ being the one-hot encoded true label for the sample $i$, and $\\Theta = (\\mathbf{W}^h; \\mathbf{b}^h; \\mathbf{W}^o; \\mathbf{b}^o)$</center>\n",
    "<center>In addition, $\\mathbf{f}(\\mathbf{x}) = softmax(\\mathbf{z^o}(\\mathbf{x})) = softmax(\\mathbf{W}^o\\mathbf{h}(\\mathbf{x}) + \\mathbf{b}^o)$</center>\n",
    "<center>and $\\mathbf{h}(\\mathbf{x}) = g(\\mathbf{z^h}(\\mathbf{x})) = g(\\mathbf{W}^h\\mathbf{x} + \\mathbf{b}^h)$, $g$ being the activation function and could be implemented with $sigmoid$ or $relu$</center>\n",
    "\n",
    "## Objectives:\n",
    "- Write the categorical cross entropy loss function\n",
    "- Write the activation functions with their associated gradient\n",
    "- Write the softmax function that is going to be used to output the predicted probabilities\n",
    "- Implement the forward pass through the neural network\n",
    "- Implement the backpropagation according to the used loss: progagate the gradients using the chain rule and return $(\\mathbf{\\nabla_{W^h}}l ; \\mathbf{\\nabla_{b^h}}l ; \\mathbf{\\nabla_{W^o}}l ; \\mathbf{\\nabla_{b^o}}l)$\n",
    "- Implement dropout regularization in the forward pass: be careful to consider both training and prediction cases\n",
    "- Implement the SGD optimization algorithm, and improve it with simple momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple graph function to let you have a global overview:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/function_graph.png\" style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) You may find numpy outer products useful: <br>\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.outer.html <br>\n",
    "We have: $outer(u, v) = u \\cdot v^T$, with $u, v$ two vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, v = np.random.normal(size=(5,)), np.random.normal(size=(10,))\n",
    "assert np.array_equal(\n",
    "    np.outer(u, v),\n",
    "    np.dot(np.reshape(u, (u.size, 1)), np.reshape(v, (1, v.size)))\n",
    ")\n",
    "assert np.outer(u, v).shape == (5, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) You also may find numpy matmul function useful: <br>\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.matmul.html <br>\n",
    "It can be used to perform matrix products along one fixed dimension (i.e. the batch size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, B = np.random.randint(0, 100, size=(64, 5, 10)), np.random.randint(0, 100, size=(64, 10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.array_equal(\n",
    "    np.stack([np.dot(A_i, B_i) for A_i, B_i in zip(A, B)]),\n",
    "    np.matmul(A, B)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-28719bea9ef9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mvelocities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'W_h'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'b_h'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'W_o'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'b_o'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mvelocities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.9\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mvelocities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvelocities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W_h'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    }
   ],
   "source": [
    "Grads = namedtuple('Grads', ['W_h', 'b_h', 'W_o', 'b_o'])\n",
    "test = Grads(1,2,3,4)\n",
    "print(test.W_h)\n",
    "\n",
    "velocities = {'W_h': 1., 'b_h': 1., 'W_o': 0., 'b_o': 0.}\n",
    "velocities[:,] = 0.9*velocities[:,]\n",
    "print(velocities['W_h'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â II b) - Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron():\n",
    "    \"\"\"MLP with one hidden layer having a hidden activation,\n",
    "    and one output layer having a softmax activation\"\"\"\n",
    "    def __init__(self, X, Y, hidden_size, activation='relu',\n",
    "                 initialization='uniform', dropout=False, dropout_rate=1):\n",
    "        # input, hidden, and output dimensions on the MLP based on X, Y\n",
    "        self.input_size, self.output_size = X.shape[1], len(np.unique(Y))\n",
    "        self.hidden_size = hidden_size\n",
    "        # initialization strategies: avoid a full-0 initialization of the weight matricest\n",
    "        if initialization == 'uniform':\n",
    "            self.W_h = np.random.uniform(size=(self.hidden_size, self.input_size), high=0.01, low=-0.01)\n",
    "            self.W_o = np.random.uniform(size=(self.output_size, self.hidden_size), high=0.01, low=-0.01)\n",
    "        elif initialization == 'normal':\n",
    "            self.W_h = np.random.normal(size=(self.hidden_size, self.input_size), loc=0, scale=0.01)\n",
    "            self.W_o = np.random.normal(size=(self.output_size, self.hidden_size), loc=0, scale=0.01)\n",
    "        # the bias could be initializated to 0 or a random low constant\n",
    "        self.b_h = np.zeros(self.hidden_size)\n",
    "        self.b_o = np.zeros(self.output_size)\n",
    "        # our namedtuple structure of gradients\n",
    "        self.Grads = namedtuple('Grads', ['W_h', 'b_h', 'W_o', 'b_o'])\n",
    "        # and the velocities associated which are going to be useful for the momentum\n",
    "        self.velocities = {'W_h': 0., 'b_h': 0., 'W_o': 0., 'b_o': 0.}\n",
    "        #Â the hidden activation function used\n",
    "        self.activation = activation\n",
    "        #Â arrays to track back the losses and accuracies evolution\n",
    "        self.training_losses_history = []\n",
    "        self.validation_losses_history = []\n",
    "        self.training_acc_history = []\n",
    "        self.validation_acc_history = []\n",
    "        #Â train val split and normalization of the features\n",
    "        self.X_tr, self.X_val, self.Y_tr, self.Y_val = self.split_train_validation(X, Y)\n",
    "        self.scaler = MinMaxScaler(feature_range=(0, 1), copy=False)\n",
    "        self.X_tr = self.scaler.fit_transform(self.X_tr)\n",
    "        self.X_val = self.scaler.transform(self.X_val)\n",
    "        #Â dropout parameters\n",
    "        self.dropout = dropout\n",
    "        self.dropout_rate = dropout_rate\n",
    "        # step used for the optimization algorithm and setted later (learning rate)\n",
    "        self.step = 0\n",
    "    \n",
    "    # One-hot encoding of the target\n",
    "    # Transform the integer represensation to a sparse one\n",
    "    @staticmethod\n",
    "    def one_hot(n_classes, Y):\n",
    "        return np.eye(n_classes)[Y]\n",
    "    \n",
    "    # Reverse one-hot encoding of the target\n",
    "    # Recover the former integer representation\n",
    "    # ex: from (0,0,1,0) to 2\n",
    "    @staticmethod\n",
    "    def reverse_one_hot(Y_one_hot):\n",
    "        return np.asarray(np.where(Y_one_hot==1)[1], dtype='int32')\n",
    "    \n",
    "    \"\"\"\n",
    "    Activation functions and their gradient\n",
    "    \"\"\"\n",
    "    # In implementations below X is a matrix of shape (n_samples, p)\n",
    "    \n",
    "    #Â A max_value value is indicated for the relu and grad_relu functions\n",
    "    # Make sure to clip the output to it to prevent numerical overflow (exploding gradient)\n",
    "    # Make it so the max value reachable is max_value\n",
    "    @staticmethod\n",
    "    def relu(X, max_value=20):\n",
    "        assert max_value > 0\n",
    "        # TODO:\n",
    "        return np.clip(X,0,max_value)\n",
    "    \n",
    "    # Make it so the gradient becomes 0 when X becomes greater than max_value\n",
    "    @staticmethod\n",
    "    def grad_relu(X, max_value=20):\n",
    "        assert max_value > 0\n",
    "        # TODO:\n",
    "        grad_logic = (X >= 0).all() and (X < max_value).all()\n",
    "        return grad_logic.astype(int)\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(X):\n",
    "        # TODO:\n",
    "        return 1/(1+np.exp(-X))\n",
    "    \n",
    "    @staticmethod\n",
    "    def grad_sigmoid(X):\n",
    "        # TODO:\n",
    "        return self.sigmoid(X)*(1-self.sigmoid(X))\n",
    "    \n",
    "    # Softmax function to output probabilities\n",
    "    @staticmethod\n",
    "    def softmax(X):\n",
    "        # TODO:\n",
    "        return np.exp(X)/np.sum(np.exp(X))\n",
    "    \n",
    "    #Â Loss function\n",
    "    #Â Consider using EPSILON to prevent numerical issues (log(0) is undefined)\n",
    "    # Y_true and Y_pred are of shape (n_samples,n_classes)\n",
    "    @staticmethod\n",
    "    def categorical_cross_entropy(Y_true, Y_pred):\n",
    "        # TODO:\n",
    "        #n = Y_true.shape[0]\n",
    "        return -np.mean(np.dot(np.log(Y_pred.T),Y_true))\n",
    "    \n",
    "    @staticmethod\n",
    "    def split_train_validation(X, Y, test_size=0.25, seed=False):\n",
    "        random_state = 42 if seed else np.random.randint(1e3)\n",
    "        X_tr, X_val, Y_tr, Y_val = train_test_split(X, Y, test_size=test_size, random_state=random_state)\n",
    "        return X_tr, X_val, Y_tr, Y_val\n",
    "    \n",
    "    # Sample random batch in (X, Y) with a given batch_size for SGD\n",
    "    @staticmethod\n",
    "    def get_random_batch(X, Y, batch_size):\n",
    "        indexes = np.random.choice(X.shape[0], size=batch_size, replace=False)\n",
    "        return X[indexes], Y[indexes]\n",
    "        \n",
    "    #Â Forward pass: compute f(x) as y, and return optionally the hidden states h(x) and z_h(x) for compute_grads\n",
    "    def forward(self, X, return_activation=False, training=False):\n",
    "        if self.activation == 'relu':\n",
    "            g_activation = self.relu\n",
    "        elif self.activation == 'sigmoid':\n",
    "            g_activation = self.sigmoid\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "            \n",
    "        #z_h = np.zeros((X.shape[0], self.hidden_size)) if len(X.shape) > 1 else np.zeros(self.hidden_size)\n",
    "        #h = np.zeros((X.shape[0], self.hidden_size)) if len(X.shape) > 1 else np.zeros(self.hidden_size)\n",
    "        # TODO:\n",
    "        #print(\"Parameters shape : \", self.W_h.shape, X.shape, self.b_h.shape)\n",
    "        \n",
    "        \n",
    "        z_h = np.dot(self.W_h,X.T).T + self.b_h\n",
    "        #print(\"z_h forward shape : \",z_h.shape)\n",
    "        h = g_activation(z_h)\n",
    "        #print(\"H forward shape : \", h.shape)\n",
    "        if self.dropout:\n",
    "            if training:\n",
    "                # TODO:\n",
    "                pass\n",
    "                #h_size = h.shape[0]\n",
    "                #dropout_vec = np.binomial(1,self.dropout_rate,h_size)\n",
    "                #h = h*dropout_vec\n",
    "            else:\n",
    "                # TODO:\n",
    "                pass\n",
    "                #h = h*self.dropout_rate\n",
    "        # TODO:\n",
    "        #y = np.zeros((X.shape[0], self.output_size)) if len(X.shape) > 1 else np.zeros(self.output_size)        \n",
    "        z_o = np.dot(self.W_o,h.T).T + self.b_o\n",
    "        #print(\"Z_o forward shape\",z_o.shape)\n",
    "        y = self.softmax(z_o)\n",
    "        #print(\"f(x) forward shape : \",y.shape)\n",
    "        if return_activation:\n",
    "            return y, h, z_h\n",
    "        else:\n",
    "            return y\n",
    "    \n",
    "    #Â Backpropagation: return an instantiation of self.Grads that contains the average gradients for the given batch\n",
    "    def compute_grads(self, X, Y_true, vectorized=False):\n",
    "        if self.activation == 'relu':\n",
    "            g_grad = self.grad_relu\n",
    "        elif self.activation == 'sigmoid':\n",
    "            g_grad = self.grad_sigmoid\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        if len(X.shape) == 1:\n",
    "            X = X.reshape((1,) + X.shape)\n",
    "        \n",
    "        if not vectorized:\n",
    "            n = X.shape[0]\n",
    "            grad_W_h = np.zeros((self.hidden_size, self.input_size))\n",
    "            grad_b_h = np.zeros((self.hidden_size, )) \n",
    "            grad_W_o = np.zeros((self.output_size, self.hidden_size))\n",
    "            grad_b_o = np.zeros((self.output_size, ))\n",
    "           \n",
    "            for x, y_true in zip(X, Y_true):\n",
    "                y_pred, h, z_h = self.forward(x, return_activation=True, training=True)\n",
    "                #compute activation gradients\n",
    "                grad_z_o = y_pred - self.one_hot(self.output_size, y_true)\n",
    "                #print(\"Grad z_o shape : \", grad_z_o.shape)\n",
    "                #computer last layer parameters gradients \n",
    "                grad_W_o += np.outer(grad_z_o, h.T)\n",
    "              \n",
    "                #print(\"Grad W_o shape : \",grad_W_o.shape)\n",
    "                grad_b_o += grad_z_o\n",
    "                #print(\"Grad b_o shape : \",grad_b_o.shape)\n",
    "                #compute first layer activation gradients\n",
    "                grad_h = np.dot(self.W_o.T,grad_z_o)\n",
    "                #print(\"Grad h shape : \",grad_h.shape)\n",
    "\n",
    "                grad_z_h = np.multiply(grad_h,g_grad(z_h))\n",
    "                #print(\"Grad z_h shape : \",grad_z_h.shape)\n",
    "\n",
    "                #compute first layer parameters gradients\n",
    "                grad_W_h += np.outer(grad_z_h,x.T)\n",
    "                #print(\"Grad W_h shape : \",grad_W_h.shape)\n",
    "                grad_b_h += grad_z_h\n",
    "                #print(\"Grad b_h shape : \",grad_b_h.shape)\n",
    "                # TODO:\n",
    "                \n",
    "            grads = self.Grads(grad_W_h/n, grad_b_h/n, grad_W_o/n, grad_b_o/n)\n",
    "            \n",
    "        else: \n",
    "            Y_pred, h, z_h = self.forward(X, return_activation=True, training=True)\n",
    "\n",
    "            # TODO (optional), try to do the backprop without Python loops in a vectorized way:\n",
    "            \n",
    "            grad_W_h = np.zeros((X.shape[0], self.hidden_size, self.input_size))\n",
    "            grad_b_h = np.zeros((X.shape[0], self.hidden_size, )) \n",
    "            grad_W_o = np.zeros((X.shape[0], self.output_size, self.hidden_size))\n",
    "            grad_b_o = np.zeros((X.shape[0], self.output_size, ))\n",
    "            \n",
    "            grads = self.Grads(\n",
    "                np.mean(grad_W_h, axis=0), \n",
    "                np.mean(grad_b_h, axis=0), \n",
    "                np.mean(grad_W_o, axis=0), \n",
    "                np.mean(grad_b_o, axis=0)\n",
    "            )\n",
    "            \n",
    "        return grads\n",
    "    \n",
    "    # Perform the update of the parameters (W_h, b_h, W_o, b_o) based of their gradient\n",
    "    def optimizer_step(self, optimizer='gd', momentum=False, momentum_alpha=0.9, \n",
    "                       batch_size=None, vectorized=True):\n",
    "        if optimizer == 'gd':\n",
    "            grads = self.compute_grads(self.X_tr, self.Y_tr, vectorized=vectorized)\n",
    "        elif optimizer == 'sgd':\n",
    "            batch_X_tr, batch_Y_tr = self.get_random_batch(self.X_tr, self.Y_tr, batch_size)\n",
    "            grads = self.compute_grads(batch_X_tr, batch_Y_tr, vectorized=vectorized)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        if not momentum:\n",
    "            # TODO:\n",
    "            #print(\"Shape step and shape grad W_h\",self.step,grads.W_h.shape)\n",
    "            self.W_h = self.W_h - self.step * grads.W_h\n",
    "            self.b_h = self.b_h - self.step * grads.b_h\n",
    "            self.W_o = self.W_o - self.step * grads.W_o\n",
    "            self.b_o = self.b_o - self.step * grads.b_o\n",
    "        else:\n",
    "            # remember: use the stored velocities\n",
    "            # TODO:\n",
    "            # compute velocities update for momentum\n",
    "            self.velocities['W_h'] = momentum_alpha * self.velocities['W_h'] - self.step * grads.W_h\n",
    "            self.velocities['W_o'] = momentum_alpha * self.velocities['W_o'] - self.step * grads.W_o\n",
    "            self.velocities['b_h'] = momentum_alpha * self.velocities['b_h'] - self.step * grads.b_h\n",
    "            self.velocities['b_o'] = momentum_alpha * self.velocities['b_o'] - self.step * grads.b_o\n",
    "            # update parameters\n",
    "            self.W_h += self.velocities['W_h']\n",
    "            self.b_h += self.velocities['b_h']\n",
    "            self.W_o += self.velocities['W_o']\n",
    "            self.b_o += self.velocities['b_o']\n",
    "            \n",
    "            \n",
    "            pass\n",
    "    \n",
    "    # Loss wrapper\n",
    "    def loss(self, Y_true, Y_pred):\n",
    "        return self.categorical_cross_entropy(self.one_hot(self.output_size, Y_true), Y_pred)\n",
    "    \n",
    "    def loss_history_flush(self):\n",
    "        self.training_losses_history = []\n",
    "        self.validation_losses_history = []\n",
    "        \n",
    "    # Main function that trains the MLP with a design matrix X and a target vector Y\n",
    "    def train(self, optimizer='sgd', momentum=False, min_iterations=500, max_iterations=5000, initial_step=1e-1,\n",
    "              batch_size=64, early_stopping=True, early_stopping_lookbehind=100, early_stopping_delta=1e-4, \n",
    "              vectorized=False, flush_history=True, verbose=True):\n",
    "        if flush_history:\n",
    "            self.loss_history_flush()\n",
    "        cpt_patience, best_validation_loss = 0, np.inf\n",
    "        iteration_number = 0\n",
    "        self.step = initial_step\n",
    "        while len(self.training_losses_history) < max_iterations:\n",
    "            iteration_number += 1\n",
    "            self.optimizer_step(\n",
    "                optimizer=optimizer, momentum=momentum, batch_size=batch_size, vectorized=vectorized\n",
    "            )\n",
    "            \n",
    "            training_loss = self.loss(self.Y_tr, self.forward(self.X_tr))\n",
    "            self.training_losses_history.append(training_loss)\n",
    "            training_accuracy = self.accuracy_on_train()\n",
    "            self.training_acc_history.append(training_accuracy)\n",
    "            validation_loss = self.loss(self.Y_val, self.forward(self.X_val))\n",
    "            self.validation_losses_history.append(validation_loss)\n",
    "            validation_accuracy = self.accuracy_on_validation()\n",
    "            self.validation_acc_history.append(validation_accuracy)\n",
    "            if iteration_number > min_iterations and early_stopping:\n",
    "                if validation_loss + early_stopping_delta < best_validation_loss:\n",
    "                    best_validation_loss = validation_loss\n",
    "                    cpt_patience = 0\n",
    "                else:\n",
    "                    cpt_patience += 1\n",
    "            if verbose:\n",
    "                msg = \"iteration number: {0}\\t training loss: {1:.4f}\\t\" + \\\n",
    "                \"validation loss: {2:.4f}\\t validation accuracy: {3:.4f}\"\n",
    "                print(msg.format(iteration_number, \n",
    "                                 training_loss, \n",
    "                                 validation_loss,\n",
    "                                 validation_accuracy))\n",
    "            if cpt_patience >= early_stopping_lookbehind:\n",
    "                break\n",
    "    \n",
    "    # Return the predicted class once the MLP has been trained\n",
    "    def predict(self, X, normalize=True):\n",
    "        if normalize:\n",
    "            X = self.scaler.transform(X)\n",
    "        if len(X.shape) == 1:\n",
    "            return np.argmax(self.forward(X))\n",
    "        else:\n",
    "            return np.argmax(self.forward(X), axis=1)\n",
    "        \n",
    "    \"\"\"\n",
    "    Metrics and plots\n",
    "    \"\"\"\n",
    "    def accuracy_on_train(self):\n",
    "        return (self.predict(self.X_tr, normalize=False) == self.Y_tr).mean()\n",
    "\n",
    "    def accuracy_on_validation(self):\n",
    "        return (self.predict(self.X_val, normalize=False) == self.Y_val).mean()\n",
    "\n",
    "    def plot_loss_history(self, add_to_title=None):\n",
    "        import warnings\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.plot(range(len(self.training_losses_history)), \n",
    "                 self.training_losses_history, label='Training loss evolution')\n",
    "        plt.plot(range(len(self.validation_losses_history)), \n",
    "                 self.validation_losses_history, label='Validation loss evolution')\n",
    "        plt.legend(fontsize=15)\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel(\"iteration number\", fontsize=15)\n",
    "        plt.ylabel(\"Cross entropy loss\", fontsize=15)\n",
    "        base_title = \"Cross entropy loss evolution during training\"\n",
    "        if not self.dropout:\n",
    "            base_title += \", no dropout penalization\"\n",
    "        else:\n",
    "            base_title += \", {:.1f} dropout penalization\"\n",
    "            base_title = base_title.format(self.dropout_rate)\n",
    "        title = base_title + \", \" + add_to_title if add_to_title else base_title\n",
    "        plt.title(title, fontsize=20)\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_validation_prediction(self, sample_id):\n",
    "        fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "        classes = np.unique(self.Y_tr)\n",
    "        dim = np.sqrt(self.X_val.shape[1]).astype(int)\n",
    "        ax0.imshow(self.scaler.inverse_transform([self.X_val[sample_id]]).reshape(dim, dim), cmap=plt.cm.gray_r,\n",
    "                   interpolation='nearest')\n",
    "        ax0.set_title(\"True image label: %d\" % self.Y_val[sample_id]);\n",
    "\n",
    "        ax1.bar(classes, self.one_hot(len(classes), self.Y_val[sample_id]), label='true')\n",
    "        ax1.bar(classes, self.forward(self.X_val[sample_id]), label='prediction', color=\"red\")\n",
    "        ax1.set_xticks(classes)\n",
    "        prediction = self.predict(self.X_val[sample_id], normalize=False)\n",
    "        ax1.set_title('Output probabilities (prediction: %d)' % prediction)\n",
    "        ax1.set_xlabel('Digit class')\n",
    "        ax1.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1\t training loss: 1280.7582\tvalidation loss: 378.5328\t validation accuracy: 0.2200\n",
      "iteration number: 2\t training loss: 1280.7605\tvalidation loss: 378.5335\t validation accuracy: 0.1044\n",
      "iteration number: 3\t training loss: 1280.7604\tvalidation loss: 378.5335\t validation accuracy: 0.1267\n",
      "iteration number: 4\t training loss: 1280.7612\tvalidation loss: 378.5338\t validation accuracy: 0.1289\n",
      "iteration number: 5\t training loss: 1280.7622\tvalidation loss: 378.5341\t validation accuracy: 0.1044\n",
      "iteration number: 6\t training loss: 1280.7629\tvalidation loss: 378.5344\t validation accuracy: 0.0689\n",
      "iteration number: 7\t training loss: 1280.7643\tvalidation loss: 378.5348\t validation accuracy: 0.1044\n",
      "iteration number: 8\t training loss: 1280.7627\tvalidation loss: 378.5343\t validation accuracy: 0.1044\n",
      "iteration number: 9\t training loss: 1280.7631\tvalidation loss: 378.5344\t validation accuracy: 0.0733\n",
      "iteration number: 10\t training loss: 1280.7642\tvalidation loss: 378.5348\t validation accuracy: 0.0933\n",
      "iteration number: 11\t training loss: 1280.7663\tvalidation loss: 378.5355\t validation accuracy: 0.0933\n",
      "iteration number: 12\t training loss: 1280.7652\tvalidation loss: 378.5351\t validation accuracy: 0.0933\n",
      "iteration number: 13\t training loss: 1280.7661\tvalidation loss: 378.5354\t validation accuracy: 0.0933\n",
      "iteration number: 14\t training loss: 1280.7676\tvalidation loss: 378.5359\t validation accuracy: 0.0933\n",
      "iteration number: 15\t training loss: 1280.7652\tvalidation loss: 378.5351\t validation accuracy: 0.0933\n",
      "iteration number: 16\t training loss: 1280.7670\tvalidation loss: 378.5357\t validation accuracy: 0.0933\n",
      "iteration number: 17\t training loss: 1280.7687\tvalidation loss: 378.5363\t validation accuracy: 0.0933\n",
      "iteration number: 18\t training loss: 1280.7679\tvalidation loss: 378.5360\t validation accuracy: 0.0933\n",
      "iteration number: 19\t training loss: 1280.7715\tvalidation loss: 378.5372\t validation accuracy: 0.0933\n",
      "iteration number: 20\t training loss: 1280.7707\tvalidation loss: 378.5369\t validation accuracy: 0.0933\n",
      "iteration number: 21\t training loss: 1280.7716\tvalidation loss: 378.5373\t validation accuracy: 0.0933\n",
      "iteration number: 22\t training loss: 1280.7700\tvalidation loss: 378.5367\t validation accuracy: 0.0933\n",
      "iteration number: 23\t training loss: 1280.7736\tvalidation loss: 378.5379\t validation accuracy: 0.1578\n",
      "iteration number: 24\t training loss: 1280.7807\tvalidation loss: 378.5403\t validation accuracy: 0.1578\n",
      "iteration number: 25\t training loss: 1280.7839\tvalidation loss: 378.5414\t validation accuracy: 0.0733\n",
      "iteration number: 26\t training loss: 1280.7907\tvalidation loss: 378.5436\t validation accuracy: 0.0689\n",
      "iteration number: 27\t training loss: 1280.7891\tvalidation loss: 378.5431\t validation accuracy: 0.0689\n",
      "iteration number: 28\t training loss: 1280.7892\tvalidation loss: 378.5431\t validation accuracy: 0.1578\n",
      "iteration number: 29\t training loss: 1280.7882\tvalidation loss: 378.5428\t validation accuracy: 0.1578\n",
      "iteration number: 30\t training loss: 1280.7917\tvalidation loss: 378.5440\t validation accuracy: 0.0689\n",
      "iteration number: 31\t training loss: 1280.7955\tvalidation loss: 378.5452\t validation accuracy: 0.0689\n",
      "iteration number: 32\t training loss: 1280.8051\tvalidation loss: 378.5484\t validation accuracy: 0.0689\n",
      "iteration number: 33\t training loss: 1280.8038\tvalidation loss: 378.5480\t validation accuracy: 0.0689\n",
      "iteration number: 34\t training loss: 1280.7983\tvalidation loss: 378.5462\t validation accuracy: 0.0689\n",
      "iteration number: 35\t training loss: 1280.7995\tvalidation loss: 378.5466\t validation accuracy: 0.0689\n",
      "iteration number: 36\t training loss: 1280.8013\tvalidation loss: 378.5472\t validation accuracy: 0.0689\n",
      "iteration number: 37\t training loss: 1280.8028\tvalidation loss: 378.5477\t validation accuracy: 0.0689\n",
      "iteration number: 38\t training loss: 1280.8011\tvalidation loss: 378.5471\t validation accuracy: 0.0689\n",
      "iteration number: 39\t training loss: 1280.8064\tvalidation loss: 378.5489\t validation accuracy: 0.0689\n",
      "iteration number: 40\t training loss: 1280.8109\tvalidation loss: 378.5504\t validation accuracy: 0.0689\n",
      "iteration number: 41\t training loss: 1280.8122\tvalidation loss: 378.5508\t validation accuracy: 0.0689\n",
      "iteration number: 42\t training loss: 1280.8123\tvalidation loss: 378.5509\t validation accuracy: 0.0689\n",
      "iteration number: 43\t training loss: 1280.8246\tvalidation loss: 378.5549\t validation accuracy: 0.0689\n",
      "iteration number: 44\t training loss: 1280.8187\tvalidation loss: 378.5530\t validation accuracy: 0.0689\n",
      "iteration number: 45\t training loss: 1280.8178\tvalidation loss: 378.5527\t validation accuracy: 0.0689\n",
      "iteration number: 46\t training loss: 1280.8221\tvalidation loss: 378.5541\t validation accuracy: 0.0689\n",
      "iteration number: 47\t training loss: 1280.8285\tvalidation loss: 378.5563\t validation accuracy: 0.0689\n",
      "iteration number: 48\t training loss: 1280.8228\tvalidation loss: 378.5544\t validation accuracy: 0.0689\n",
      "iteration number: 49\t training loss: 1280.8222\tvalidation loss: 378.5541\t validation accuracy: 0.0689\n",
      "iteration number: 50\t training loss: 1280.8154\tvalidation loss: 378.5519\t validation accuracy: 0.0689\n",
      "iteration number: 51\t training loss: 1280.8147\tvalidation loss: 378.5516\t validation accuracy: 0.0689\n",
      "iteration number: 52\t training loss: 1280.8234\tvalidation loss: 378.5546\t validation accuracy: 0.0689\n",
      "iteration number: 53\t training loss: 1280.8218\tvalidation loss: 378.5540\t validation accuracy: 0.0689\n",
      "iteration number: 54\t training loss: 1280.8177\tvalidation loss: 378.5527\t validation accuracy: 0.0689\n",
      "iteration number: 55\t training loss: 1280.8074\tvalidation loss: 378.5492\t validation accuracy: 0.0689\n",
      "iteration number: 56\t training loss: 1280.8095\tvalidation loss: 378.5499\t validation accuracy: 0.0689\n",
      "iteration number: 57\t training loss: 1280.8152\tvalidation loss: 378.5518\t validation accuracy: 0.0689\n",
      "iteration number: 58\t training loss: 1280.8126\tvalidation loss: 378.5510\t validation accuracy: 0.0689\n",
      "iteration number: 59\t training loss: 1280.8069\tvalidation loss: 378.5491\t validation accuracy: 0.0689\n",
      "iteration number: 60\t training loss: 1280.8086\tvalidation loss: 378.5496\t validation accuracy: 0.0689\n",
      "iteration number: 61\t training loss: 1280.8126\tvalidation loss: 378.5510\t validation accuracy: 0.0689\n",
      "iteration number: 62\t training loss: 1280.8164\tvalidation loss: 378.5522\t validation accuracy: 0.0689\n",
      "iteration number: 63\t training loss: 1280.8187\tvalidation loss: 378.5530\t validation accuracy: 0.0689\n",
      "iteration number: 64\t training loss: 1280.8214\tvalidation loss: 378.5539\t validation accuracy: 0.0689\n",
      "iteration number: 65\t training loss: 1280.8229\tvalidation loss: 378.5544\t validation accuracy: 0.0689\n",
      "iteration number: 66\t training loss: 1280.8202\tvalidation loss: 378.5535\t validation accuracy: 0.0689\n",
      "iteration number: 67\t training loss: 1280.8251\tvalidation loss: 378.5551\t validation accuracy: 0.0689\n",
      "iteration number: 68\t training loss: 1280.8253\tvalidation loss: 378.5552\t validation accuracy: 0.0689\n",
      "iteration number: 69\t training loss: 1280.8280\tvalidation loss: 378.5561\t validation accuracy: 0.0689\n",
      "iteration number: 70\t training loss: 1280.8299\tvalidation loss: 378.5567\t validation accuracy: 0.0689\n",
      "iteration number: 71\t training loss: 1280.8311\tvalidation loss: 378.5571\t validation accuracy: 0.0689\n",
      "iteration number: 72\t training loss: 1280.8374\tvalidation loss: 378.5592\t validation accuracy: 0.0689\n",
      "iteration number: 73\t training loss: 1280.8470\tvalidation loss: 378.5624\t validation accuracy: 0.0689\n",
      "iteration number: 74\t training loss: 1280.8402\tvalidation loss: 378.5602\t validation accuracy: 0.0689\n",
      "iteration number: 75\t training loss: 1280.8379\tvalidation loss: 378.5594\t validation accuracy: 0.0689\n",
      "iteration number: 76\t training loss: 1280.8339\tvalidation loss: 378.5581\t validation accuracy: 0.0689\n",
      "iteration number: 77\t training loss: 1280.8427\tvalidation loss: 378.5610\t validation accuracy: 0.0689\n",
      "iteration number: 78\t training loss: 1280.8450\tvalidation loss: 378.5618\t validation accuracy: 0.0689\n",
      "iteration number: 79\t training loss: 1280.8434\tvalidation loss: 378.5612\t validation accuracy: 0.0689\n",
      "iteration number: 80\t training loss: 1280.8380\tvalidation loss: 378.5594\t validation accuracy: 0.0689\n",
      "iteration number: 81\t training loss: 1280.8341\tvalidation loss: 378.5581\t validation accuracy: 0.0689\n",
      "iteration number: 82\t training loss: 1280.8456\tvalidation loss: 378.5619\t validation accuracy: 0.0689\n",
      "iteration number: 83\t training loss: 1280.8517\tvalidation loss: 378.5640\t validation accuracy: 0.0689\n",
      "iteration number: 84\t training loss: 1280.8469\tvalidation loss: 378.5624\t validation accuracy: 0.0689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 85\t training loss: 1280.8462\tvalidation loss: 378.5621\t validation accuracy: 0.0689\n",
      "iteration number: 86\t training loss: 1280.8520\tvalidation loss: 378.5641\t validation accuracy: 0.0689\n",
      "iteration number: 87\t training loss: 1280.8509\tvalidation loss: 378.5637\t validation accuracy: 0.0689\n",
      "iteration number: 88\t training loss: 1280.8681\tvalidation loss: 378.5695\t validation accuracy: 0.0689\n",
      "iteration number: 89\t training loss: 1280.8576\tvalidation loss: 378.5660\t validation accuracy: 0.0689\n",
      "iteration number: 90\t training loss: 1280.8604\tvalidation loss: 378.5669\t validation accuracy: 0.0689\n",
      "iteration number: 91\t training loss: 1280.8657\tvalidation loss: 378.5686\t validation accuracy: 0.0689\n",
      "iteration number: 92\t training loss: 1280.8582\tvalidation loss: 378.5661\t validation accuracy: 0.0689\n",
      "iteration number: 93\t training loss: 1280.8698\tvalidation loss: 378.5700\t validation accuracy: 0.0689\n",
      "iteration number: 94\t training loss: 1280.8703\tvalidation loss: 378.5702\t validation accuracy: 0.0689\n",
      "iteration number: 95\t training loss: 1280.8759\tvalidation loss: 378.5721\t validation accuracy: 0.0689\n",
      "iteration number: 96\t training loss: 1280.8762\tvalidation loss: 378.5722\t validation accuracy: 0.0689\n",
      "iteration number: 97\t training loss: 1280.8644\tvalidation loss: 378.5682\t validation accuracy: 0.0689\n",
      "iteration number: 98\t training loss: 1280.8700\tvalidation loss: 378.5701\t validation accuracy: 0.0689\n",
      "iteration number: 99\t training loss: 1280.8645\tvalidation loss: 378.5682\t validation accuracy: 0.0689\n",
      "iteration number: 100\t training loss: 1280.8598\tvalidation loss: 378.5667\t validation accuracy: 0.0689\n",
      "iteration number: 101\t training loss: 1280.8524\tvalidation loss: 378.5642\t validation accuracy: 0.0689\n",
      "iteration number: 102\t training loss: 1280.8368\tvalidation loss: 378.5590\t validation accuracy: 0.0689\n",
      "iteration number: 103\t training loss: 1280.8306\tvalidation loss: 378.5569\t validation accuracy: 0.0689\n",
      "iteration number: 104\t training loss: 1280.8339\tvalidation loss: 378.5580\t validation accuracy: 0.0689\n",
      "iteration number: 105\t training loss: 1280.8372\tvalidation loss: 378.5591\t validation accuracy: 0.0689\n",
      "iteration number: 106\t training loss: 1280.8398\tvalidation loss: 378.5600\t validation accuracy: 0.0689\n",
      "iteration number: 107\t training loss: 1280.8447\tvalidation loss: 378.5616\t validation accuracy: 0.0689\n",
      "iteration number: 108\t training loss: 1280.8464\tvalidation loss: 378.5622\t validation accuracy: 0.0689\n",
      "iteration number: 109\t training loss: 1280.8480\tvalidation loss: 378.5628\t validation accuracy: 0.0689\n",
      "iteration number: 110\t training loss: 1280.8454\tvalidation loss: 378.5619\t validation accuracy: 0.0689\n",
      "iteration number: 111\t training loss: 1280.8418\tvalidation loss: 378.5607\t validation accuracy: 0.0689\n",
      "iteration number: 112\t training loss: 1280.8483\tvalidation loss: 378.5628\t validation accuracy: 0.0689\n",
      "iteration number: 113\t training loss: 1280.8499\tvalidation loss: 378.5634\t validation accuracy: 0.0689\n",
      "iteration number: 114\t training loss: 1280.8579\tvalidation loss: 378.5661\t validation accuracy: 0.0689\n",
      "iteration number: 115\t training loss: 1280.8485\tvalidation loss: 378.5629\t validation accuracy: 0.0689\n",
      "iteration number: 116\t training loss: 1280.8566\tvalidation loss: 378.5656\t validation accuracy: 0.0689\n",
      "iteration number: 117\t training loss: 1280.8625\tvalidation loss: 378.5676\t validation accuracy: 0.0689\n",
      "iteration number: 118\t training loss: 1280.8564\tvalidation loss: 378.5656\t validation accuracy: 0.0689\n",
      "iteration number: 119\t training loss: 1280.8652\tvalidation loss: 378.5685\t validation accuracy: 0.0689\n",
      "iteration number: 120\t training loss: 1280.8747\tvalidation loss: 378.5717\t validation accuracy: 0.0689\n",
      "iteration number: 121\t training loss: 1280.8702\tvalidation loss: 378.5702\t validation accuracy: 0.0689\n",
      "iteration number: 122\t training loss: 1280.8732\tvalidation loss: 378.5711\t validation accuracy: 0.0689\n",
      "iteration number: 123\t training loss: 1280.8708\tvalidation loss: 378.5703\t validation accuracy: 0.0689\n",
      "iteration number: 124\t training loss: 1280.8684\tvalidation loss: 378.5695\t validation accuracy: 0.0689\n",
      "iteration number: 125\t training loss: 1280.8799\tvalidation loss: 378.5734\t validation accuracy: 0.0689\n",
      "iteration number: 126\t training loss: 1280.8825\tvalidation loss: 378.5742\t validation accuracy: 0.0689\n",
      "iteration number: 127\t training loss: 1280.8875\tvalidation loss: 378.5759\t validation accuracy: 0.0689\n",
      "iteration number: 128\t training loss: 1280.8937\tvalidation loss: 378.5780\t validation accuracy: 0.0689\n",
      "iteration number: 129\t training loss: 1280.8904\tvalidation loss: 378.5769\t validation accuracy: 0.0689\n",
      "iteration number: 130\t training loss: 1280.8845\tvalidation loss: 378.5749\t validation accuracy: 0.0689\n",
      "iteration number: 131\t training loss: 1280.8766\tvalidation loss: 378.5723\t validation accuracy: 0.0689\n",
      "iteration number: 132\t training loss: 1280.8813\tvalidation loss: 378.5738\t validation accuracy: 0.0689\n",
      "iteration number: 133\t training loss: 1280.8731\tvalidation loss: 378.5711\t validation accuracy: 0.0689\n",
      "iteration number: 134\t training loss: 1280.8737\tvalidation loss: 378.5713\t validation accuracy: 0.0689\n",
      "iteration number: 135\t training loss: 1280.8796\tvalidation loss: 378.5733\t validation accuracy: 0.0689\n",
      "iteration number: 136\t training loss: 1280.8703\tvalidation loss: 378.5702\t validation accuracy: 0.0689\n",
      "iteration number: 137\t training loss: 1280.8791\tvalidation loss: 378.5731\t validation accuracy: 0.0689\n",
      "iteration number: 138\t training loss: 1280.8874\tvalidation loss: 378.5759\t validation accuracy: 0.0689\n",
      "iteration number: 139\t training loss: 1280.9046\tvalidation loss: 378.5816\t validation accuracy: 0.0689\n",
      "iteration number: 140\t training loss: 1280.8954\tvalidation loss: 378.5785\t validation accuracy: 0.0689\n",
      "iteration number: 141\t training loss: 1280.9040\tvalidation loss: 378.5814\t validation accuracy: 0.0689\n",
      "iteration number: 142\t training loss: 1280.9094\tvalidation loss: 378.5832\t validation accuracy: 0.0689\n",
      "iteration number: 143\t training loss: 1280.8849\tvalidation loss: 378.5750\t validation accuracy: 0.0689\n",
      "iteration number: 144\t training loss: 1280.8848\tvalidation loss: 378.5750\t validation accuracy: 0.0689\n",
      "iteration number: 145\t training loss: 1280.8715\tvalidation loss: 378.5706\t validation accuracy: 0.0689\n",
      "iteration number: 146\t training loss: 1280.8585\tvalidation loss: 378.5662\t validation accuracy: 0.0689\n",
      "iteration number: 147\t training loss: 1280.8553\tvalidation loss: 378.5652\t validation accuracy: 0.0689\n",
      "iteration number: 148\t training loss: 1280.8484\tvalidation loss: 378.5628\t validation accuracy: 0.0689\n",
      "iteration number: 149\t training loss: 1280.8616\tvalidation loss: 378.5672\t validation accuracy: 0.0689\n",
      "iteration number: 150\t training loss: 1280.8721\tvalidation loss: 378.5708\t validation accuracy: 0.0689\n",
      "iteration number: 151\t training loss: 1280.8735\tvalidation loss: 378.5712\t validation accuracy: 0.0689\n",
      "iteration number: 152\t training loss: 1280.8975\tvalidation loss: 378.5792\t validation accuracy: 0.0689\n",
      "iteration number: 153\t training loss: 1280.8926\tvalidation loss: 378.5776\t validation accuracy: 0.0689\n",
      "iteration number: 154\t training loss: 1280.8984\tvalidation loss: 378.5795\t validation accuracy: 0.0689\n",
      "iteration number: 155\t training loss: 1280.8919\tvalidation loss: 378.5774\t validation accuracy: 0.0689\n",
      "iteration number: 156\t training loss: 1280.8887\tvalidation loss: 378.5763\t validation accuracy: 0.0689\n",
      "iteration number: 157\t training loss: 1280.8814\tvalidation loss: 378.5738\t validation accuracy: 0.0689\n",
      "iteration number: 158\t training loss: 1280.8845\tvalidation loss: 378.5749\t validation accuracy: 0.0689\n",
      "iteration number: 159\t training loss: 1280.8844\tvalidation loss: 378.5748\t validation accuracy: 0.0689\n",
      "iteration number: 160\t training loss: 1280.8881\tvalidation loss: 378.5761\t validation accuracy: 0.0689\n",
      "iteration number: 161\t training loss: 1280.8845\tvalidation loss: 378.5749\t validation accuracy: 0.0689\n",
      "iteration number: 162\t training loss: 1280.8836\tvalidation loss: 378.5746\t validation accuracy: 0.0689\n",
      "iteration number: 163\t training loss: 1280.8776\tvalidation loss: 378.5726\t validation accuracy: 0.0689\n",
      "iteration number: 164\t training loss: 1280.8795\tvalidation loss: 378.5732\t validation accuracy: 0.0689\n",
      "iteration number: 165\t training loss: 1280.8779\tvalidation loss: 378.5727\t validation accuracy: 0.0689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 166\t training loss: 1280.8713\tvalidation loss: 378.5705\t validation accuracy: 0.0689\n",
      "iteration number: 167\t training loss: 1280.8721\tvalidation loss: 378.5707\t validation accuracy: 0.0689\n",
      "iteration number: 168\t training loss: 1280.8772\tvalidation loss: 378.5724\t validation accuracy: 0.0689\n",
      "iteration number: 169\t training loss: 1280.8662\tvalidation loss: 378.5687\t validation accuracy: 0.0689\n",
      "iteration number: 170\t training loss: 1280.8661\tvalidation loss: 378.5687\t validation accuracy: 0.0689\n",
      "iteration number: 171\t training loss: 1280.8697\tvalidation loss: 378.5699\t validation accuracy: 0.0689\n",
      "iteration number: 172\t training loss: 1280.8581\tvalidation loss: 378.5660\t validation accuracy: 0.0689\n",
      "iteration number: 173\t training loss: 1280.8565\tvalidation loss: 378.5655\t validation accuracy: 0.0689\n",
      "iteration number: 174\t training loss: 1280.8497\tvalidation loss: 378.5632\t validation accuracy: 0.0689\n",
      "iteration number: 175\t training loss: 1280.8533\tvalidation loss: 378.5644\t validation accuracy: 0.0689\n",
      "iteration number: 176\t training loss: 1280.8584\tvalidation loss: 378.5661\t validation accuracy: 0.0689\n",
      "iteration number: 177\t training loss: 1280.8589\tvalidation loss: 378.5663\t validation accuracy: 0.0689\n",
      "iteration number: 178\t training loss: 1280.8513\tvalidation loss: 378.5638\t validation accuracy: 0.0689\n",
      "iteration number: 179\t training loss: 1280.8475\tvalidation loss: 378.5625\t validation accuracy: 0.0689\n",
      "iteration number: 180\t training loss: 1280.8419\tvalidation loss: 378.5606\t validation accuracy: 0.0689\n",
      "iteration number: 181\t training loss: 1280.8367\tvalidation loss: 378.5589\t validation accuracy: 0.0689\n",
      "iteration number: 182\t training loss: 1280.8377\tvalidation loss: 378.5592\t validation accuracy: 0.0689\n",
      "iteration number: 183\t training loss: 1280.8341\tvalidation loss: 378.5580\t validation accuracy: 0.1311\n",
      "iteration number: 184\t training loss: 1280.8381\tvalidation loss: 378.5594\t validation accuracy: 0.1133\n",
      "iteration number: 185\t training loss: 1280.8433\tvalidation loss: 378.5611\t validation accuracy: 0.1133\n",
      "iteration number: 186\t training loss: 1280.8458\tvalidation loss: 378.5619\t validation accuracy: 0.1356\n",
      "iteration number: 187\t training loss: 1280.8560\tvalidation loss: 378.5653\t validation accuracy: 0.0733\n",
      "iteration number: 188\t training loss: 1280.8638\tvalidation loss: 378.5679\t validation accuracy: 0.0733\n",
      "iteration number: 189\t training loss: 1280.8585\tvalidation loss: 378.5662\t validation accuracy: 0.0733\n",
      "iteration number: 190\t training loss: 1280.8566\tvalidation loss: 378.5655\t validation accuracy: 0.0756\n",
      "iteration number: 191\t training loss: 1280.8568\tvalidation loss: 378.5656\t validation accuracy: 0.0822\n",
      "iteration number: 192\t training loss: 1280.8584\tvalidation loss: 378.5661\t validation accuracy: 0.0689\n",
      "iteration number: 193\t training loss: 1280.8513\tvalidation loss: 378.5638\t validation accuracy: 0.0689\n",
      "iteration number: 194\t training loss: 1280.8436\tvalidation loss: 378.5612\t validation accuracy: 0.0689\n",
      "iteration number: 195\t training loss: 1280.8414\tvalidation loss: 378.5605\t validation accuracy: 0.0711\n",
      "iteration number: 196\t training loss: 1280.8363\tvalidation loss: 378.5588\t validation accuracy: 0.0689\n",
      "iteration number: 197\t training loss: 1280.8370\tvalidation loss: 378.5590\t validation accuracy: 0.0689\n",
      "iteration number: 198\t training loss: 1280.8356\tvalidation loss: 378.5585\t validation accuracy: 0.0689\n",
      "iteration number: 199\t training loss: 1280.8417\tvalidation loss: 378.5606\t validation accuracy: 0.0689\n",
      "iteration number: 200\t training loss: 1280.8444\tvalidation loss: 378.5615\t validation accuracy: 0.0689\n",
      "iteration number: 201\t training loss: 1280.8425\tvalidation loss: 378.5608\t validation accuracy: 0.0689\n",
      "iteration number: 202\t training loss: 1280.8413\tvalidation loss: 378.5604\t validation accuracy: 0.0689\n",
      "iteration number: 203\t training loss: 1280.8465\tvalidation loss: 378.5622\t validation accuracy: 0.0689\n",
      "iteration number: 204\t training loss: 1280.8583\tvalidation loss: 378.5661\t validation accuracy: 0.0689\n",
      "iteration number: 205\t training loss: 1280.8525\tvalidation loss: 378.5642\t validation accuracy: 0.0689\n",
      "iteration number: 206\t training loss: 1280.8563\tvalidation loss: 378.5654\t validation accuracy: 0.0689\n",
      "iteration number: 207\t training loss: 1280.8540\tvalidation loss: 378.5647\t validation accuracy: 0.0689\n",
      "iteration number: 208\t training loss: 1280.8503\tvalidation loss: 378.5634\t validation accuracy: 0.0689\n",
      "iteration number: 209\t training loss: 1280.8514\tvalidation loss: 378.5638\t validation accuracy: 0.0689\n",
      "iteration number: 210\t training loss: 1280.8546\tvalidation loss: 378.5648\t validation accuracy: 0.0689\n",
      "iteration number: 211\t training loss: 1280.8566\tvalidation loss: 378.5655\t validation accuracy: 0.0689\n",
      "iteration number: 212\t training loss: 1280.8439\tvalidation loss: 378.5613\t validation accuracy: 0.0689\n",
      "iteration number: 213\t training loss: 1280.8392\tvalidation loss: 378.5597\t validation accuracy: 0.0689\n",
      "iteration number: 214\t training loss: 1280.8357\tvalidation loss: 378.5586\t validation accuracy: 0.0689\n",
      "iteration number: 215\t training loss: 1280.8423\tvalidation loss: 378.5607\t validation accuracy: 0.0689\n",
      "iteration number: 216\t training loss: 1280.8505\tvalidation loss: 378.5635\t validation accuracy: 0.0689\n",
      "iteration number: 217\t training loss: 1280.8519\tvalidation loss: 378.5639\t validation accuracy: 0.0689\n",
      "iteration number: 218\t training loss: 1280.8513\tvalidation loss: 378.5637\t validation accuracy: 0.0689\n",
      "iteration number: 219\t training loss: 1280.8657\tvalidation loss: 378.5685\t validation accuracy: 0.0689\n",
      "iteration number: 220\t training loss: 1280.8628\tvalidation loss: 378.5676\t validation accuracy: 0.0689\n",
      "iteration number: 221\t training loss: 1280.8700\tvalidation loss: 378.5700\t validation accuracy: 0.0689\n",
      "iteration number: 222\t training loss: 1280.8715\tvalidation loss: 378.5705\t validation accuracy: 0.0689\n",
      "iteration number: 223\t training loss: 1280.8736\tvalidation loss: 378.5712\t validation accuracy: 0.0689\n",
      "iteration number: 224\t training loss: 1280.8902\tvalidation loss: 378.5767\t validation accuracy: 0.1222\n",
      "iteration number: 225\t training loss: 1280.8821\tvalidation loss: 378.5740\t validation accuracy: 0.1333\n",
      "iteration number: 226\t training loss: 1280.8855\tvalidation loss: 378.5751\t validation accuracy: 0.0867\n",
      "iteration number: 227\t training loss: 1280.8859\tvalidation loss: 378.5753\t validation accuracy: 0.1111\n",
      "iteration number: 228\t training loss: 1280.8900\tvalidation loss: 378.5767\t validation accuracy: 0.1133\n",
      "iteration number: 229\t training loss: 1280.9036\tvalidation loss: 378.5812\t validation accuracy: 0.1378\n",
      "iteration number: 230\t training loss: 1280.8997\tvalidation loss: 378.5799\t validation accuracy: 0.1378\n",
      "iteration number: 231\t training loss: 1280.9012\tvalidation loss: 378.5804\t validation accuracy: 0.1244\n",
      "iteration number: 232\t training loss: 1280.9011\tvalidation loss: 378.5803\t validation accuracy: 0.1244\n",
      "iteration number: 233\t training loss: 1280.9034\tvalidation loss: 378.5811\t validation accuracy: 0.1244\n",
      "iteration number: 234\t training loss: 1280.9068\tvalidation loss: 378.5823\t validation accuracy: 0.1333\n",
      "iteration number: 235\t training loss: 1280.9117\tvalidation loss: 378.5839\t validation accuracy: 0.0689\n",
      "iteration number: 236\t training loss: 1280.9041\tvalidation loss: 378.5813\t validation accuracy: 0.0689\n",
      "iteration number: 237\t training loss: 1280.9073\tvalidation loss: 378.5824\t validation accuracy: 0.1244\n",
      "iteration number: 238\t training loss: 1280.9047\tvalidation loss: 378.5816\t validation accuracy: 0.0911\n",
      "iteration number: 239\t training loss: 1280.9101\tvalidation loss: 378.5834\t validation accuracy: 0.1356\n",
      "iteration number: 240\t training loss: 1280.9050\tvalidation loss: 378.5817\t validation accuracy: 0.0711\n",
      "iteration number: 241\t training loss: 1280.9001\tvalidation loss: 378.5800\t validation accuracy: 0.0800\n",
      "iteration number: 242\t training loss: 1280.9209\tvalidation loss: 378.5869\t validation accuracy: 0.0800\n",
      "iteration number: 243\t training loss: 1280.9159\tvalidation loss: 378.5853\t validation accuracy: 0.1111\n",
      "iteration number: 244\t training loss: 1280.9160\tvalidation loss: 378.5853\t validation accuracy: 0.1044\n",
      "iteration number: 245\t training loss: 1280.9164\tvalidation loss: 378.5855\t validation accuracy: 0.1289\n",
      "iteration number: 246\t training loss: 1280.9144\tvalidation loss: 378.5848\t validation accuracy: 0.0689\n",
      "iteration number: 247\t training loss: 1280.9167\tvalidation loss: 378.5856\t validation accuracy: 0.0689\n",
      "iteration number: 248\t training loss: 1280.9180\tvalidation loss: 378.5860\t validation accuracy: 0.0689\n",
      "iteration number: 249\t training loss: 1280.9240\tvalidation loss: 378.5880\t validation accuracy: 0.0689\n",
      "iteration number: 250\t training loss: 1280.9307\tvalidation loss: 378.5902\t validation accuracy: 0.0689\n",
      "iteration number: 251\t training loss: 1280.9321\tvalidation loss: 378.5907\t validation accuracy: 0.0689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 252\t training loss: 1280.9273\tvalidation loss: 378.5891\t validation accuracy: 0.0689\n",
      "iteration number: 253\t training loss: 1280.9193\tvalidation loss: 378.5864\t validation accuracy: 0.0689\n",
      "iteration number: 254\t training loss: 1280.9379\tvalidation loss: 378.5926\t validation accuracy: 0.0711\n",
      "iteration number: 255\t training loss: 1280.9409\tvalidation loss: 378.5936\t validation accuracy: 0.0689\n",
      "iteration number: 256\t training loss: 1280.9560\tvalidation loss: 378.5987\t validation accuracy: 0.0689\n",
      "iteration number: 257\t training loss: 1280.9608\tvalidation loss: 378.6003\t validation accuracy: 0.0689\n",
      "iteration number: 258\t training loss: 1280.9504\tvalidation loss: 378.5968\t validation accuracy: 0.0689\n",
      "iteration number: 259\t training loss: 1280.9507\tvalidation loss: 378.5969\t validation accuracy: 0.0689\n",
      "iteration number: 260\t training loss: 1280.9467\tvalidation loss: 378.5955\t validation accuracy: 0.0689\n",
      "iteration number: 261\t training loss: 1280.9521\tvalidation loss: 378.5973\t validation accuracy: 0.0689\n",
      "iteration number: 262\t training loss: 1280.9520\tvalidation loss: 378.5973\t validation accuracy: 0.0689\n",
      "iteration number: 263\t training loss: 1280.9586\tvalidation loss: 378.5995\t validation accuracy: 0.0689\n",
      "iteration number: 264\t training loss: 1280.9587\tvalidation loss: 378.5996\t validation accuracy: 0.0689\n",
      "iteration number: 265\t training loss: 1280.9455\tvalidation loss: 378.5951\t validation accuracy: 0.0689\n",
      "iteration number: 266\t training loss: 1280.9314\tvalidation loss: 378.5904\t validation accuracy: 0.0689\n",
      "iteration number: 267\t training loss: 1280.9406\tvalidation loss: 378.5935\t validation accuracy: 0.0689\n",
      "iteration number: 268\t training loss: 1280.9309\tvalidation loss: 378.5903\t validation accuracy: 0.0689\n",
      "iteration number: 269\t training loss: 1280.9253\tvalidation loss: 378.5884\t validation accuracy: 0.0689\n",
      "iteration number: 270\t training loss: 1280.9299\tvalidation loss: 378.5899\t validation accuracy: 0.0689\n",
      "iteration number: 271\t training loss: 1280.9292\tvalidation loss: 378.5897\t validation accuracy: 0.0689\n",
      "iteration number: 272\t training loss: 1280.9338\tvalidation loss: 378.5912\t validation accuracy: 0.0689\n",
      "iteration number: 273\t training loss: 1280.9250\tvalidation loss: 378.5883\t validation accuracy: 0.0689\n",
      "iteration number: 274\t training loss: 1280.9349\tvalidation loss: 378.5916\t validation accuracy: 0.0689\n",
      "iteration number: 275\t training loss: 1280.9273\tvalidation loss: 378.5891\t validation accuracy: 0.0689\n",
      "iteration number: 276\t training loss: 1280.9366\tvalidation loss: 378.5922\t validation accuracy: 0.0689\n",
      "iteration number: 277\t training loss: 1280.9495\tvalidation loss: 378.5965\t validation accuracy: 0.0689\n",
      "iteration number: 278\t training loss: 1280.9500\tvalidation loss: 378.5966\t validation accuracy: 0.0689\n",
      "iteration number: 279\t training loss: 1280.9430\tvalidation loss: 378.5943\t validation accuracy: 0.0689\n",
      "iteration number: 280\t training loss: 1280.9655\tvalidation loss: 378.6018\t validation accuracy: 0.0689\n",
      "iteration number: 281\t training loss: 1280.9755\tvalidation loss: 378.6051\t validation accuracy: 0.0689\n",
      "iteration number: 282\t training loss: 1280.9837\tvalidation loss: 378.6079\t validation accuracy: 0.0689\n",
      "iteration number: 283\t training loss: 1280.9795\tvalidation loss: 378.6065\t validation accuracy: 0.0689\n",
      "iteration number: 284\t training loss: 1280.9738\tvalidation loss: 378.6046\t validation accuracy: 0.0689\n",
      "iteration number: 285\t training loss: 1280.9921\tvalidation loss: 378.6107\t validation accuracy: 0.0689\n",
      "iteration number: 286\t training loss: 1281.0139\tvalidation loss: 378.6179\t validation accuracy: 0.0689\n",
      "iteration number: 287\t training loss: 1281.0205\tvalidation loss: 378.6202\t validation accuracy: 0.0689\n",
      "iteration number: 288\t training loss: 1281.0227\tvalidation loss: 378.6209\t validation accuracy: 0.0689\n",
      "iteration number: 289\t training loss: 1281.0534\tvalidation loss: 378.6311\t validation accuracy: 0.0689\n",
      "iteration number: 290\t training loss: 1281.0700\tvalidation loss: 378.6366\t validation accuracy: 0.0689\n",
      "iteration number: 291\t training loss: 1281.0732\tvalidation loss: 378.6377\t validation accuracy: 0.0689\n",
      "iteration number: 292\t training loss: 1281.0587\tvalidation loss: 378.6329\t validation accuracy: 0.0689\n",
      "iteration number: 293\t training loss: 1281.0325\tvalidation loss: 378.6241\t validation accuracy: 0.0689\n",
      "iteration number: 294\t training loss: 1281.0114\tvalidation loss: 378.6171\t validation accuracy: 0.0689\n",
      "iteration number: 295\t training loss: 1281.0092\tvalidation loss: 378.6164\t validation accuracy: 0.0689\n",
      "iteration number: 296\t training loss: 1281.0138\tvalidation loss: 378.6179\t validation accuracy: 0.0689\n",
      "iteration number: 297\t training loss: 1281.0348\tvalidation loss: 378.6249\t validation accuracy: 0.0689\n",
      "iteration number: 298\t training loss: 1281.0242\tvalidation loss: 378.6214\t validation accuracy: 0.0689\n",
      "iteration number: 299\t training loss: 1280.9962\tvalidation loss: 378.6120\t validation accuracy: 0.0689\n",
      "iteration number: 300\t training loss: 1280.9939\tvalidation loss: 378.6112\t validation accuracy: 0.0689\n",
      "iteration number: 301\t training loss: 1281.0115\tvalidation loss: 378.6171\t validation accuracy: 0.0689\n",
      "iteration number: 302\t training loss: 1281.0204\tvalidation loss: 378.6201\t validation accuracy: 0.0689\n",
      "iteration number: 303\t training loss: 1281.0261\tvalidation loss: 378.6220\t validation accuracy: 0.0689\n",
      "iteration number: 304\t training loss: 1281.0234\tvalidation loss: 378.6211\t validation accuracy: 0.0689\n",
      "iteration number: 305\t training loss: 1281.0506\tvalidation loss: 378.6302\t validation accuracy: 0.0689\n",
      "iteration number: 306\t training loss: 1281.0542\tvalidation loss: 378.6314\t validation accuracy: 0.0689\n",
      "iteration number: 307\t training loss: 1281.0370\tvalidation loss: 378.6256\t validation accuracy: 0.0689\n",
      "iteration number: 308\t training loss: 1281.0538\tvalidation loss: 378.6312\t validation accuracy: 0.0689\n",
      "iteration number: 309\t training loss: 1281.0496\tvalidation loss: 378.6298\t validation accuracy: 0.0689\n",
      "iteration number: 310\t training loss: 1281.0606\tvalidation loss: 378.6335\t validation accuracy: 0.0689\n",
      "iteration number: 311\t training loss: 1281.0660\tvalidation loss: 378.6353\t validation accuracy: 0.0689\n",
      "iteration number: 312\t training loss: 1281.0460\tvalidation loss: 378.6286\t validation accuracy: 0.0689\n",
      "iteration number: 313\t training loss: 1281.0325\tvalidation loss: 378.6241\t validation accuracy: 0.0689\n",
      "iteration number: 314\t training loss: 1281.0277\tvalidation loss: 378.6225\t validation accuracy: 0.0689\n",
      "iteration number: 315\t training loss: 1281.0226\tvalidation loss: 378.6208\t validation accuracy: 0.0689\n",
      "iteration number: 316\t training loss: 1281.0234\tvalidation loss: 378.6211\t validation accuracy: 0.0689\n",
      "iteration number: 317\t training loss: 1281.0179\tvalidation loss: 378.6192\t validation accuracy: 0.0689\n",
      "iteration number: 318\t training loss: 1281.0018\tvalidation loss: 378.6139\t validation accuracy: 0.0689\n",
      "iteration number: 319\t training loss: 1280.9787\tvalidation loss: 378.6062\t validation accuracy: 0.0689\n",
      "iteration number: 320\t training loss: 1280.9882\tvalidation loss: 378.6093\t validation accuracy: 0.0689\n",
      "iteration number: 321\t training loss: 1280.9951\tvalidation loss: 378.6116\t validation accuracy: 0.0689\n",
      "iteration number: 322\t training loss: 1280.9972\tvalidation loss: 378.6123\t validation accuracy: 0.0689\n",
      "iteration number: 323\t training loss: 1280.9953\tvalidation loss: 378.6117\t validation accuracy: 0.0689\n",
      "iteration number: 324\t training loss: 1281.0178\tvalidation loss: 378.6192\t validation accuracy: 0.0689\n",
      "iteration number: 325\t training loss: 1281.0435\tvalidation loss: 378.6278\t validation accuracy: 0.0689\n",
      "iteration number: 326\t training loss: 1281.0648\tvalidation loss: 378.6349\t validation accuracy: 0.0689\n",
      "iteration number: 327\t training loss: 1281.0629\tvalidation loss: 378.6342\t validation accuracy: 0.0689\n",
      "iteration number: 328\t training loss: 1281.0713\tvalidation loss: 378.6370\t validation accuracy: 0.0689\n",
      "iteration number: 329\t training loss: 1281.0574\tvalidation loss: 378.6324\t validation accuracy: 0.0689\n",
      "iteration number: 330\t training loss: 1281.0493\tvalidation loss: 378.6297\t validation accuracy: 0.0689\n",
      "iteration number: 331\t training loss: 1281.0536\tvalidation loss: 378.6311\t validation accuracy: 0.0689\n",
      "iteration number: 332\t training loss: 1281.0629\tvalidation loss: 378.6342\t validation accuracy: 0.0689\n",
      "iteration number: 333\t training loss: 1281.0719\tvalidation loss: 378.6372\t validation accuracy: 0.0689\n",
      "iteration number: 334\t training loss: 1281.0867\tvalidation loss: 378.6422\t validation accuracy: 0.0689\n",
      "iteration number: 335\t training loss: 1281.0757\tvalidation loss: 378.6385\t validation accuracy: 0.0689\n",
      "iteration number: 336\t training loss: 1281.0736\tvalidation loss: 378.6378\t validation accuracy: 0.0689\n",
      "iteration number: 337\t training loss: 1281.0653\tvalidation loss: 378.6350\t validation accuracy: 0.0689\n",
      "iteration number: 338\t training loss: 1281.0965\tvalidation loss: 378.6454\t validation accuracy: 0.0689\n",
      "iteration number: 339\t training loss: 1281.0978\tvalidation loss: 378.6458\t validation accuracy: 0.0689\n",
      "iteration number: 340\t training loss: 1281.0802\tvalidation loss: 378.6400\t validation accuracy: 0.0689\n",
      "iteration number: 341\t training loss: 1281.0725\tvalidation loss: 378.6374\t validation accuracy: 0.0689\n",
      "iteration number: 342\t training loss: 1281.0642\tvalidation loss: 378.6346\t validation accuracy: 0.0689\n",
      "iteration number: 343\t training loss: 1281.0786\tvalidation loss: 378.6394\t validation accuracy: 0.0689\n",
      "iteration number: 344\t training loss: 1281.0647\tvalidation loss: 378.6348\t validation accuracy: 0.0689\n",
      "iteration number: 345\t training loss: 1281.0903\tvalidation loss: 378.6433\t validation accuracy: 0.0689\n",
      "iteration number: 346\t training loss: 1281.1066\tvalidation loss: 378.6488\t validation accuracy: 0.0689\n",
      "iteration number: 347\t training loss: 1281.0949\tvalidation loss: 378.6449\t validation accuracy: 0.0689\n",
      "iteration number: 348\t training loss: 1281.0807\tvalidation loss: 378.6401\t validation accuracy: 0.0689\n",
      "iteration number: 349\t training loss: 1281.0694\tvalidation loss: 378.6364\t validation accuracy: 0.0689\n",
      "iteration number: 350\t training loss: 1281.0744\tvalidation loss: 378.6380\t validation accuracy: 0.0689\n",
      "iteration number: 351\t training loss: 1281.0744\tvalidation loss: 378.6380\t validation accuracy: 0.0689\n",
      "iteration number: 352\t training loss: 1281.0501\tvalidation loss: 378.6299\t validation accuracy: 0.0689\n",
      "iteration number: 353\t training loss: 1281.0713\tvalidation loss: 378.6370\t validation accuracy: 0.0689\n",
      "iteration number: 354\t training loss: 1281.0626\tvalidation loss: 378.6341\t validation accuracy: 0.0689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 355\t training loss: 1281.0463\tvalidation loss: 378.6286\t validation accuracy: 0.0689\n",
      "iteration number: 356\t training loss: 1281.0393\tvalidation loss: 378.6263\t validation accuracy: 0.0689\n",
      "iteration number: 357\t training loss: 1281.0379\tvalidation loss: 378.6259\t validation accuracy: 0.0689\n",
      "iteration number: 358\t training loss: 1281.0315\tvalidation loss: 378.6237\t validation accuracy: 0.0689\n",
      "iteration number: 359\t training loss: 1281.0319\tvalidation loss: 378.6239\t validation accuracy: 0.0689\n",
      "iteration number: 360\t training loss: 1281.0401\tvalidation loss: 378.6266\t validation accuracy: 0.0689\n",
      "iteration number: 361\t training loss: 1281.0676\tvalidation loss: 378.6358\t validation accuracy: 0.0689\n",
      "iteration number: 362\t training loss: 1281.0600\tvalidation loss: 378.6333\t validation accuracy: 0.0689\n",
      "iteration number: 363\t training loss: 1281.0559\tvalidation loss: 378.6319\t validation accuracy: 0.0689\n",
      "iteration number: 364\t training loss: 1281.0565\tvalidation loss: 378.6321\t validation accuracy: 0.0689\n",
      "iteration number: 365\t training loss: 1281.0567\tvalidation loss: 378.6321\t validation accuracy: 0.0689\n",
      "iteration number: 366\t training loss: 1281.0638\tvalidation loss: 378.6345\t validation accuracy: 0.0689\n",
      "iteration number: 367\t training loss: 1281.0819\tvalidation loss: 378.6405\t validation accuracy: 0.0689\n",
      "iteration number: 368\t training loss: 1281.0803\tvalidation loss: 378.6400\t validation accuracy: 0.0689\n",
      "iteration number: 369\t training loss: 1281.0940\tvalidation loss: 378.6446\t validation accuracy: 0.0689\n",
      "iteration number: 370\t training loss: 1281.0901\tvalidation loss: 378.6433\t validation accuracy: 0.0689\n",
      "iteration number: 371\t training loss: 1281.0764\tvalidation loss: 378.6387\t validation accuracy: 0.0689\n",
      "iteration number: 372\t training loss: 1281.0598\tvalidation loss: 378.6332\t validation accuracy: 0.0689\n",
      "iteration number: 373\t training loss: 1281.0369\tvalidation loss: 378.6255\t validation accuracy: 0.0689\n",
      "iteration number: 374\t training loss: 1281.0283\tvalidation loss: 378.6227\t validation accuracy: 0.0689\n",
      "iteration number: 375\t training loss: 1281.0325\tvalidation loss: 378.6240\t validation accuracy: 0.0689\n",
      "iteration number: 376\t training loss: 1281.0285\tvalidation loss: 378.6227\t validation accuracy: 0.0689\n",
      "iteration number: 377\t training loss: 1281.0247\tvalidation loss: 378.6215\t validation accuracy: 0.0689\n",
      "iteration number: 378\t training loss: 1281.0220\tvalidation loss: 378.6205\t validation accuracy: 0.0689\n",
      "iteration number: 379\t training loss: 1281.0227\tvalidation loss: 378.6208\t validation accuracy: 0.0689\n",
      "iteration number: 380\t training loss: 1281.0311\tvalidation loss: 378.6236\t validation accuracy: 0.0689\n",
      "iteration number: 381\t training loss: 1281.0235\tvalidation loss: 378.6210\t validation accuracy: 0.0689\n",
      "iteration number: 382\t training loss: 1281.0421\tvalidation loss: 378.6272\t validation accuracy: 0.0689\n",
      "iteration number: 383\t training loss: 1281.0460\tvalidation loss: 378.6285\t validation accuracy: 0.0689\n",
      "iteration number: 384\t training loss: 1281.0392\tvalidation loss: 378.6262\t validation accuracy: 0.0689\n",
      "iteration number: 385\t training loss: 1281.0252\tvalidation loss: 378.6216\t validation accuracy: 0.0689\n",
      "iteration number: 386\t training loss: 1281.0241\tvalidation loss: 378.6212\t validation accuracy: 0.0689\n",
      "iteration number: 387\t training loss: 1281.0431\tvalidation loss: 378.6276\t validation accuracy: 0.0689\n",
      "iteration number: 388\t training loss: 1281.0562\tvalidation loss: 378.6319\t validation accuracy: 0.0689\n",
      "iteration number: 389\t training loss: 1281.0542\tvalidation loss: 378.6312\t validation accuracy: 0.0689\n",
      "iteration number: 390\t training loss: 1281.0462\tvalidation loss: 378.6286\t validation accuracy: 0.0689\n",
      "iteration number: 391\t training loss: 1281.0312\tvalidation loss: 378.6236\t validation accuracy: 0.0689\n",
      "iteration number: 392\t training loss: 1281.0434\tvalidation loss: 378.6276\t validation accuracy: 0.0689\n",
      "iteration number: 393\t training loss: 1281.0532\tvalidation loss: 378.6309\t validation accuracy: 0.0689\n",
      "iteration number: 394\t training loss: 1281.0369\tvalidation loss: 378.6255\t validation accuracy: 0.0689\n",
      "iteration number: 395\t training loss: 1281.0191\tvalidation loss: 378.6195\t validation accuracy: 0.0689\n",
      "iteration number: 396\t training loss: 1281.0215\tvalidation loss: 378.6203\t validation accuracy: 0.0689\n",
      "iteration number: 397\t training loss: 1281.0215\tvalidation loss: 378.6203\t validation accuracy: 0.0689\n",
      "iteration number: 398\t training loss: 1281.0330\tvalidation loss: 378.6242\t validation accuracy: 0.0689\n",
      "iteration number: 399\t training loss: 1281.0255\tvalidation loss: 378.6217\t validation accuracy: 0.0689\n",
      "iteration number: 400\t training loss: 1281.0272\tvalidation loss: 378.6222\t validation accuracy: 0.0689\n",
      "iteration number: 401\t training loss: 1281.0384\tvalidation loss: 378.6260\t validation accuracy: 0.0689\n",
      "iteration number: 402\t training loss: 1281.0158\tvalidation loss: 378.6184\t validation accuracy: 0.0689\n",
      "iteration number: 403\t training loss: 1281.0220\tvalidation loss: 378.6205\t validation accuracy: 0.0689\n",
      "iteration number: 404\t training loss: 1281.0455\tvalidation loss: 378.6283\t validation accuracy: 0.0689\n",
      "iteration number: 405\t training loss: 1281.0407\tvalidation loss: 378.6267\t validation accuracy: 0.0689\n",
      "iteration number: 406\t training loss: 1281.0437\tvalidation loss: 378.6277\t validation accuracy: 0.0689\n",
      "iteration number: 407\t training loss: 1281.0322\tvalidation loss: 378.6239\t validation accuracy: 0.0689\n",
      "iteration number: 408\t training loss: 1281.0563\tvalidation loss: 378.6319\t validation accuracy: 0.0689\n",
      "iteration number: 409\t training loss: 1281.0547\tvalidation loss: 378.6314\t validation accuracy: 0.0689\n",
      "iteration number: 410\t training loss: 1281.0449\tvalidation loss: 378.6281\t validation accuracy: 0.0689\n",
      "iteration number: 411\t training loss: 1281.0569\tvalidation loss: 378.6321\t validation accuracy: 0.0689\n",
      "iteration number: 412\t training loss: 1281.0714\tvalidation loss: 378.6370\t validation accuracy: 0.0689\n",
      "iteration number: 413\t training loss: 1281.0819\tvalidation loss: 378.6405\t validation accuracy: 0.0689\n",
      "iteration number: 414\t training loss: 1281.0869\tvalidation loss: 378.6421\t validation accuracy: 0.0689\n",
      "iteration number: 415\t training loss: 1281.0949\tvalidation loss: 378.6448\t validation accuracy: 0.0689\n",
      "iteration number: 416\t training loss: 1281.0970\tvalidation loss: 378.6455\t validation accuracy: 0.0689\n",
      "iteration number: 417\t training loss: 1281.0903\tvalidation loss: 378.6432\t validation accuracy: 0.0689\n",
      "iteration number: 418\t training loss: 1281.0786\tvalidation loss: 378.6394\t validation accuracy: 0.0689\n",
      "iteration number: 419\t training loss: 1281.0747\tvalidation loss: 378.6381\t validation accuracy: 0.0689\n",
      "iteration number: 420\t training loss: 1281.0780\tvalidation loss: 378.6391\t validation accuracy: 0.0689\n",
      "iteration number: 421\t training loss: 1281.0875\tvalidation loss: 378.6423\t validation accuracy: 0.0689\n",
      "iteration number: 422\t training loss: 1281.0769\tvalidation loss: 378.6388\t validation accuracy: 0.0689\n",
      "iteration number: 423\t training loss: 1281.0635\tvalidation loss: 378.6343\t validation accuracy: 0.0689\n",
      "iteration number: 424\t training loss: 1281.0647\tvalidation loss: 378.6347\t validation accuracy: 0.0689\n",
      "iteration number: 425\t training loss: 1281.0681\tvalidation loss: 378.6358\t validation accuracy: 0.0689\n",
      "iteration number: 426\t training loss: 1281.0554\tvalidation loss: 378.6316\t validation accuracy: 0.0689\n",
      "iteration number: 427\t training loss: 1281.0603\tvalidation loss: 378.6332\t validation accuracy: 0.0689\n",
      "iteration number: 428\t training loss: 1281.0526\tvalidation loss: 378.6307\t validation accuracy: 0.0689\n",
      "iteration number: 429\t training loss: 1281.0531\tvalidation loss: 378.6308\t validation accuracy: 0.0689\n",
      "iteration number: 430\t training loss: 1281.0397\tvalidation loss: 378.6264\t validation accuracy: 0.0689\n",
      "iteration number: 431\t training loss: 1281.0471\tvalidation loss: 378.6288\t validation accuracy: 0.0689\n",
      "iteration number: 432\t training loss: 1281.0240\tvalidation loss: 378.6211\t validation accuracy: 0.0689\n",
      "iteration number: 433\t training loss: 1281.0139\tvalidation loss: 378.6177\t validation accuracy: 0.0689\n",
      "iteration number: 434\t training loss: 1281.0221\tvalidation loss: 378.6205\t validation accuracy: 0.0689\n",
      "iteration number: 435\t training loss: 1281.0235\tvalidation loss: 378.6210\t validation accuracy: 0.0689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 436\t training loss: 1281.0236\tvalidation loss: 378.6210\t validation accuracy: 0.0689\n",
      "iteration number: 437\t training loss: 1281.0369\tvalidation loss: 378.6254\t validation accuracy: 0.0689\n",
      "iteration number: 438\t training loss: 1281.0660\tvalidation loss: 378.6351\t validation accuracy: 0.0689\n",
      "iteration number: 439\t training loss: 1281.0656\tvalidation loss: 378.6350\t validation accuracy: 0.0689\n",
      "iteration number: 440\t training loss: 1281.0659\tvalidation loss: 378.6351\t validation accuracy: 0.0689\n",
      "iteration number: 441\t training loss: 1281.0544\tvalidation loss: 378.6312\t validation accuracy: 0.0689\n",
      "iteration number: 442\t training loss: 1281.0408\tvalidation loss: 378.6267\t validation accuracy: 0.0689\n",
      "iteration number: 443\t training loss: 1281.0239\tvalidation loss: 378.6211\t validation accuracy: 0.0689\n",
      "iteration number: 444\t training loss: 1281.0040\tvalidation loss: 378.6144\t validation accuracy: 0.0689\n",
      "iteration number: 445\t training loss: 1281.0053\tvalidation loss: 378.6149\t validation accuracy: 0.0689\n",
      "iteration number: 446\t training loss: 1281.0033\tvalidation loss: 378.6142\t validation accuracy: 0.0689\n",
      "iteration number: 447\t training loss: 1280.9886\tvalidation loss: 378.6093\t validation accuracy: 0.0689\n",
      "iteration number: 448\t training loss: 1280.9810\tvalidation loss: 378.6068\t validation accuracy: 0.0689\n",
      "iteration number: 449\t training loss: 1280.9822\tvalidation loss: 378.6072\t validation accuracy: 0.0689\n",
      "iteration number: 450\t training loss: 1280.9926\tvalidation loss: 378.6106\t validation accuracy: 0.0689\n",
      "iteration number: 451\t training loss: 1281.0093\tvalidation loss: 378.6162\t validation accuracy: 0.0689\n",
      "iteration number: 452\t training loss: 1281.0098\tvalidation loss: 378.6164\t validation accuracy: 0.0689\n",
      "iteration number: 453\t training loss: 1281.0045\tvalidation loss: 378.6146\t validation accuracy: 0.0689\n",
      "iteration number: 454\t training loss: 1281.0125\tvalidation loss: 378.6173\t validation accuracy: 0.0689\n",
      "iteration number: 455\t training loss: 1281.0187\tvalidation loss: 378.6193\t validation accuracy: 0.0689\n",
      "iteration number: 456\t training loss: 1281.0266\tvalidation loss: 378.6219\t validation accuracy: 0.0689\n",
      "iteration number: 457\t training loss: 1281.0371\tvalidation loss: 378.6254\t validation accuracy: 0.0689\n",
      "iteration number: 458\t training loss: 1281.0313\tvalidation loss: 378.6235\t validation accuracy: 0.0689\n",
      "iteration number: 459\t training loss: 1281.0469\tvalidation loss: 378.6287\t validation accuracy: 0.0689\n",
      "iteration number: 460\t training loss: 1281.0375\tvalidation loss: 378.6256\t validation accuracy: 0.0689\n",
      "iteration number: 461\t training loss: 1281.0455\tvalidation loss: 378.6282\t validation accuracy: 0.0689\n",
      "iteration number: 462\t training loss: 1281.0474\tvalidation loss: 378.6289\t validation accuracy: 0.0689\n",
      "iteration number: 463\t training loss: 1281.0318\tvalidation loss: 378.6237\t validation accuracy: 0.0689\n",
      "iteration number: 464\t training loss: 1281.0304\tvalidation loss: 378.6232\t validation accuracy: 0.0689\n",
      "iteration number: 465\t training loss: 1281.0162\tvalidation loss: 378.6185\t validation accuracy: 0.0689\n",
      "iteration number: 466\t training loss: 1281.0284\tvalidation loss: 378.6225\t validation accuracy: 0.0689\n",
      "iteration number: 467\t training loss: 1281.0499\tvalidation loss: 378.6297\t validation accuracy: 0.0689\n",
      "iteration number: 468\t training loss: 1281.0769\tvalidation loss: 378.6387\t validation accuracy: 0.0689\n",
      "iteration number: 469\t training loss: 1281.0893\tvalidation loss: 378.6428\t validation accuracy: 0.0689\n",
      "iteration number: 470\t training loss: 1281.0819\tvalidation loss: 378.6404\t validation accuracy: 0.0689\n",
      "iteration number: 471\t training loss: 1281.0908\tvalidation loss: 378.6433\t validation accuracy: 0.0689\n",
      "iteration number: 472\t training loss: 1281.0702\tvalidation loss: 378.6365\t validation accuracy: 0.0689\n",
      "iteration number: 473\t training loss: 1281.0739\tvalidation loss: 378.6377\t validation accuracy: 0.0689\n",
      "iteration number: 474\t training loss: 1281.0607\tvalidation loss: 378.6333\t validation accuracy: 0.0689\n",
      "iteration number: 475\t training loss: 1281.0760\tvalidation loss: 378.6384\t validation accuracy: 0.0689\n",
      "iteration number: 476\t training loss: 1281.0533\tvalidation loss: 378.6308\t validation accuracy: 0.0689\n",
      "iteration number: 477\t training loss: 1281.0459\tvalidation loss: 378.6283\t validation accuracy: 0.0689\n",
      "iteration number: 478\t training loss: 1281.0580\tvalidation loss: 378.6324\t validation accuracy: 0.0689\n",
      "iteration number: 479\t training loss: 1281.0479\tvalidation loss: 378.6290\t validation accuracy: 0.0689\n",
      "iteration number: 480\t training loss: 1281.0429\tvalidation loss: 378.6273\t validation accuracy: 0.0689\n",
      "iteration number: 481\t training loss: 1281.0422\tvalidation loss: 378.6271\t validation accuracy: 0.0689\n",
      "iteration number: 482\t training loss: 1281.0294\tvalidation loss: 378.6228\t validation accuracy: 0.0689\n",
      "iteration number: 483\t training loss: 1281.0467\tvalidation loss: 378.6286\t validation accuracy: 0.0689\n",
      "iteration number: 484\t training loss: 1281.0553\tvalidation loss: 378.6314\t validation accuracy: 0.0689\n",
      "iteration number: 485\t training loss: 1281.0452\tvalidation loss: 378.6281\t validation accuracy: 0.0689\n",
      "iteration number: 486\t training loss: 1281.0695\tvalidation loss: 378.6362\t validation accuracy: 0.0689\n",
      "iteration number: 487\t training loss: 1281.0525\tvalidation loss: 378.6305\t validation accuracy: 0.0689\n",
      "iteration number: 488\t training loss: 1281.0387\tvalidation loss: 378.6259\t validation accuracy: 0.0689\n",
      "iteration number: 489\t training loss: 1281.0343\tvalidation loss: 378.6245\t validation accuracy: 0.0689\n",
      "iteration number: 490\t training loss: 1281.0529\tvalidation loss: 378.6307\t validation accuracy: 0.0689\n",
      "iteration number: 491\t training loss: 1281.0443\tvalidation loss: 378.6278\t validation accuracy: 0.0689\n",
      "iteration number: 492\t training loss: 1281.0598\tvalidation loss: 378.6330\t validation accuracy: 0.0689\n",
      "iteration number: 493\t training loss: 1281.0376\tvalidation loss: 378.6256\t validation accuracy: 0.0689\n",
      "iteration number: 494\t training loss: 1281.0494\tvalidation loss: 378.6295\t validation accuracy: 0.0689\n",
      "iteration number: 495\t training loss: 1281.0368\tvalidation loss: 378.6253\t validation accuracy: 0.0689\n",
      "iteration number: 496\t training loss: 1281.0275\tvalidation loss: 378.6222\t validation accuracy: 0.0689\n",
      "iteration number: 497\t training loss: 1281.0212\tvalidation loss: 378.6201\t validation accuracy: 0.0689\n",
      "iteration number: 498\t training loss: 1281.0222\tvalidation loss: 378.6204\t validation accuracy: 0.0689\n",
      "iteration number: 499\t training loss: 1281.0107\tvalidation loss: 378.6166\t validation accuracy: 0.0689\n",
      "iteration number: 500\t training loss: 1280.9955\tvalidation loss: 378.6115\t validation accuracy: 0.0689\n",
      "iteration number: 501\t training loss: 1281.0084\tvalidation loss: 378.6159\t validation accuracy: 0.0689\n",
      "iteration number: 502\t training loss: 1281.0121\tvalidation loss: 378.6171\t validation accuracy: 0.0689\n",
      "iteration number: 503\t training loss: 1281.0208\tvalidation loss: 378.6200\t validation accuracy: 0.0689\n",
      "iteration number: 504\t training loss: 1281.0362\tvalidation loss: 378.6251\t validation accuracy: 0.0689\n",
      "iteration number: 505\t training loss: 1281.0272\tvalidation loss: 378.6221\t validation accuracy: 0.0689\n",
      "iteration number: 506\t training loss: 1281.0352\tvalidation loss: 378.6248\t validation accuracy: 0.0689\n",
      "iteration number: 507\t training loss: 1281.0284\tvalidation loss: 378.6225\t validation accuracy: 0.0689\n",
      "iteration number: 508\t training loss: 1281.0334\tvalidation loss: 378.6242\t validation accuracy: 0.0689\n",
      "iteration number: 509\t training loss: 1281.0261\tvalidation loss: 378.6218\t validation accuracy: 0.0689\n",
      "iteration number: 510\t training loss: 1281.0250\tvalidation loss: 378.6214\t validation accuracy: 0.0689\n",
      "iteration number: 511\t training loss: 1281.0124\tvalidation loss: 378.6172\t validation accuracy: 0.0689\n",
      "iteration number: 512\t training loss: 1281.0194\tvalidation loss: 378.6195\t validation accuracy: 0.0689\n",
      "iteration number: 513\t training loss: 1281.0087\tvalidation loss: 378.6159\t validation accuracy: 0.0689\n",
      "iteration number: 514\t training loss: 1281.0162\tvalidation loss: 378.6184\t validation accuracy: 0.0689\n",
      "iteration number: 515\t training loss: 1281.0028\tvalidation loss: 378.6140\t validation accuracy: 0.0689\n",
      "iteration number: 516\t training loss: 1280.9906\tvalidation loss: 378.6099\t validation accuracy: 0.0689\n",
      "iteration number: 517\t training loss: 1280.9859\tvalidation loss: 378.6083\t validation accuracy: 0.0689\n",
      "iteration number: 518\t training loss: 1280.9803\tvalidation loss: 378.6064\t validation accuracy: 0.0689\n",
      "iteration number: 519\t training loss: 1280.9761\tvalidation loss: 378.6050\t validation accuracy: 0.0689\n",
      "iteration number: 520\t training loss: 1280.9687\tvalidation loss: 378.6026\t validation accuracy: 0.0689\n",
      "iteration number: 521\t training loss: 1280.9984\tvalidation loss: 378.6125\t validation accuracy: 0.0689\n",
      "iteration number: 522\t training loss: 1280.9895\tvalidation loss: 378.6095\t validation accuracy: 0.0689\n",
      "iteration number: 523\t training loss: 1280.9888\tvalidation loss: 378.6093\t validation accuracy: 0.0689\n",
      "iteration number: 524\t training loss: 1280.9751\tvalidation loss: 378.6047\t validation accuracy: 0.0689\n",
      "iteration number: 525\t training loss: 1280.9666\tvalidation loss: 378.6019\t validation accuracy: 0.0689\n",
      "iteration number: 526\t training loss: 1280.9660\tvalidation loss: 378.6016\t validation accuracy: 0.0689\n",
      "iteration number: 527\t training loss: 1280.9610\tvalidation loss: 378.6000\t validation accuracy: 0.0689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 528\t training loss: 1280.9568\tvalidation loss: 378.5986\t validation accuracy: 0.0689\n",
      "iteration number: 529\t training loss: 1280.9585\tvalidation loss: 378.5992\t validation accuracy: 0.0689\n",
      "iteration number: 530\t training loss: 1280.9488\tvalidation loss: 378.5960\t validation accuracy: 0.0689\n",
      "iteration number: 531\t training loss: 1280.9551\tvalidation loss: 378.5981\t validation accuracy: 0.0689\n",
      "iteration number: 532\t training loss: 1280.9613\tvalidation loss: 378.6001\t validation accuracy: 0.0689\n",
      "iteration number: 533\t training loss: 1280.9692\tvalidation loss: 378.6027\t validation accuracy: 0.0689\n",
      "iteration number: 534\t training loss: 1280.9761\tvalidation loss: 378.6050\t validation accuracy: 0.0689\n",
      "iteration number: 535\t training loss: 1280.9634\tvalidation loss: 378.6008\t validation accuracy: 0.0689\n",
      "iteration number: 536\t training loss: 1280.9350\tvalidation loss: 378.5913\t validation accuracy: 0.0689\n",
      "iteration number: 537\t training loss: 1280.9329\tvalidation loss: 378.5907\t validation accuracy: 0.0689\n",
      "iteration number: 538\t training loss: 1280.9306\tvalidation loss: 378.5899\t validation accuracy: 0.0689\n",
      "iteration number: 539\t training loss: 1280.9255\tvalidation loss: 378.5882\t validation accuracy: 0.0689\n",
      "iteration number: 540\t training loss: 1280.9170\tvalidation loss: 378.5854\t validation accuracy: 0.0689\n",
      "iteration number: 541\t training loss: 1280.9111\tvalidation loss: 378.5834\t validation accuracy: 0.0689\n",
      "iteration number: 542\t training loss: 1280.9136\tvalidation loss: 378.5842\t validation accuracy: 0.0689\n",
      "iteration number: 543\t training loss: 1280.9119\tvalidation loss: 378.5836\t validation accuracy: 0.0689\n",
      "iteration number: 544\t training loss: 1280.9102\tvalidation loss: 378.5831\t validation accuracy: 0.0689\n",
      "iteration number: 545\t training loss: 1280.9166\tvalidation loss: 378.5852\t validation accuracy: 0.0689\n",
      "iteration number: 546\t training loss: 1280.9117\tvalidation loss: 378.5836\t validation accuracy: 0.0689\n",
      "iteration number: 547\t training loss: 1280.9233\tvalidation loss: 378.5874\t validation accuracy: 0.0689\n",
      "iteration number: 548\t training loss: 1280.9269\tvalidation loss: 378.5886\t validation accuracy: 0.0689\n",
      "iteration number: 549\t training loss: 1280.9206\tvalidation loss: 378.5865\t validation accuracy: 0.0689\n",
      "iteration number: 550\t training loss: 1280.9348\tvalidation loss: 378.5912\t validation accuracy: 0.0689\n",
      "iteration number: 551\t training loss: 1280.9386\tvalidation loss: 378.5925\t validation accuracy: 0.0689\n",
      "iteration number: 552\t training loss: 1280.9224\tvalidation loss: 378.5871\t validation accuracy: 0.0689\n",
      "iteration number: 553\t training loss: 1280.9229\tvalidation loss: 378.5873\t validation accuracy: 0.0689\n",
      "iteration number: 554\t training loss: 1280.9230\tvalidation loss: 378.5873\t validation accuracy: 0.0689\n",
      "iteration number: 555\t training loss: 1280.9250\tvalidation loss: 378.5880\t validation accuracy: 0.0689\n",
      "iteration number: 556\t training loss: 1280.9275\tvalidation loss: 378.5888\t validation accuracy: 0.0689\n",
      "iteration number: 557\t training loss: 1280.9343\tvalidation loss: 378.5911\t validation accuracy: 0.0689\n",
      "iteration number: 558\t training loss: 1280.9370\tvalidation loss: 378.5920\t validation accuracy: 0.0689\n",
      "iteration number: 559\t training loss: 1280.9314\tvalidation loss: 378.5901\t validation accuracy: 0.0689\n",
      "iteration number: 560\t training loss: 1280.9256\tvalidation loss: 378.5882\t validation accuracy: 0.0689\n",
      "iteration number: 561\t training loss: 1280.9301\tvalidation loss: 378.5897\t validation accuracy: 0.0689\n",
      "iteration number: 562\t training loss: 1280.9380\tvalidation loss: 378.5923\t validation accuracy: 0.0689\n",
      "iteration number: 563\t training loss: 1280.9477\tvalidation loss: 378.5956\t validation accuracy: 0.0689\n",
      "iteration number: 564\t training loss: 1280.9651\tvalidation loss: 378.6013\t validation accuracy: 0.0689\n",
      "iteration number: 565\t training loss: 1280.9640\tvalidation loss: 378.6010\t validation accuracy: 0.0689\n",
      "iteration number: 566\t training loss: 1280.9623\tvalidation loss: 378.6005\t validation accuracy: 0.0689\n",
      "iteration number: 567\t training loss: 1280.9916\tvalidation loss: 378.6102\t validation accuracy: 0.0689\n",
      "iteration number: 568\t training loss: 1280.9934\tvalidation loss: 378.6108\t validation accuracy: 0.0689\n",
      "iteration number: 569\t training loss: 1280.9748\tvalidation loss: 378.6046\t validation accuracy: 0.0689\n",
      "iteration number: 570\t training loss: 1280.9911\tvalidation loss: 378.6101\t validation accuracy: 0.0689\n",
      "iteration number: 571\t training loss: 1280.9898\tvalidation loss: 378.6096\t validation accuracy: 0.0689\n",
      "iteration number: 572\t training loss: 1281.0055\tvalidation loss: 378.6148\t validation accuracy: 0.0689\n",
      "iteration number: 573\t training loss: 1281.0070\tvalidation loss: 378.6153\t validation accuracy: 0.0689\n",
      "iteration number: 574\t training loss: 1281.0205\tvalidation loss: 378.6198\t validation accuracy: 0.0689\n",
      "iteration number: 575\t training loss: 1281.0240\tvalidation loss: 378.6210\t validation accuracy: 0.0689\n",
      "iteration number: 576\t training loss: 1281.0194\tvalidation loss: 378.6194\t validation accuracy: 0.0689\n",
      "iteration number: 577\t training loss: 1281.0237\tvalidation loss: 378.6209\t validation accuracy: 0.0689\n",
      "iteration number: 578\t training loss: 1281.0134\tvalidation loss: 378.6174\t validation accuracy: 0.0689\n",
      "iteration number: 579\t training loss: 1281.0261\tvalidation loss: 378.6216\t validation accuracy: 0.0689\n",
      "iteration number: 580\t training loss: 1281.0081\tvalidation loss: 378.6157\t validation accuracy: 0.0689\n",
      "iteration number: 581\t training loss: 1281.0053\tvalidation loss: 378.6147\t validation accuracy: 0.0689\n",
      "iteration number: 582\t training loss: 1281.0032\tvalidation loss: 378.6140\t validation accuracy: 0.0689\n",
      "iteration number: 583\t training loss: 1281.0045\tvalidation loss: 378.6144\t validation accuracy: 0.0689\n",
      "iteration number: 584\t training loss: 1281.0159\tvalidation loss: 378.6183\t validation accuracy: 0.0689\n",
      "iteration number: 585\t training loss: 1281.0153\tvalidation loss: 378.6181\t validation accuracy: 0.0689\n",
      "iteration number: 586\t training loss: 1281.0101\tvalidation loss: 378.6163\t validation accuracy: 0.0689\n",
      "iteration number: 587\t training loss: 1281.0253\tvalidation loss: 378.6214\t validation accuracy: 0.0689\n",
      "iteration number: 588\t training loss: 1281.0190\tvalidation loss: 378.6193\t validation accuracy: 0.0689\n",
      "iteration number: 589\t training loss: 1281.0150\tvalidation loss: 378.6179\t validation accuracy: 0.0689\n",
      "iteration number: 590\t training loss: 1280.9984\tvalidation loss: 378.6124\t validation accuracy: 0.0689\n",
      "iteration number: 591\t training loss: 1280.9993\tvalidation loss: 378.6127\t validation accuracy: 0.0689\n",
      "iteration number: 592\t training loss: 1280.9843\tvalidation loss: 378.6077\t validation accuracy: 0.0689\n",
      "iteration number: 593\t training loss: 1280.9856\tvalidation loss: 378.6081\t validation accuracy: 0.0689\n",
      "iteration number: 594\t training loss: 1280.9749\tvalidation loss: 378.6046\t validation accuracy: 0.0689\n",
      "iteration number: 595\t training loss: 1280.9809\tvalidation loss: 378.6065\t validation accuracy: 0.0689\n",
      "iteration number: 596\t training loss: 1280.9917\tvalidation loss: 378.6101\t validation accuracy: 0.0689\n",
      "iteration number: 597\t training loss: 1280.9924\tvalidation loss: 378.6104\t validation accuracy: 0.0689\n",
      "iteration number: 598\t training loss: 1280.9959\tvalidation loss: 378.6115\t validation accuracy: 0.0689\n",
      "iteration number: 599\t training loss: 1280.9816\tvalidation loss: 378.6068\t validation accuracy: 0.0689\n",
      "iteration number: 600\t training loss: 1280.9844\tvalidation loss: 378.6077\t validation accuracy: 0.0689\n",
      "iteration number: 601\t training loss: 1280.9731\tvalidation loss: 378.6040\t validation accuracy: 0.0689\n",
      "iteration number: 602\t training loss: 1280.9606\tvalidation loss: 378.5998\t validation accuracy: 0.0689\n",
      "iteration number: 603\t training loss: 1280.9614\tvalidation loss: 378.6001\t validation accuracy: 0.0689\n",
      "iteration number: 604\t training loss: 1280.9622\tvalidation loss: 378.6003\t validation accuracy: 0.0689\n",
      "iteration number: 605\t training loss: 1280.9575\tvalidation loss: 378.5988\t validation accuracy: 0.0689\n",
      "iteration number: 606\t training loss: 1280.9627\tvalidation loss: 378.6005\t validation accuracy: 0.0689\n",
      "iteration number: 607\t training loss: 1280.9801\tvalidation loss: 378.6063\t validation accuracy: 0.0689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 608\t training loss: 1280.9827\tvalidation loss: 378.6072\t validation accuracy: 0.0689\n",
      "iteration number: 609\t training loss: 1280.9733\tvalidation loss: 378.6041\t validation accuracy: 0.0689\n",
      "iteration number: 610\t training loss: 1280.9892\tvalidation loss: 378.6094\t validation accuracy: 0.0689\n",
      "iteration number: 611\t training loss: 1281.0028\tvalidation loss: 378.6139\t validation accuracy: 0.0689\n",
      "iteration number: 612\t training loss: 1281.0003\tvalidation loss: 378.6131\t validation accuracy: 0.0689\n",
      "iteration number: 613\t training loss: 1281.0000\tvalidation loss: 378.6130\t validation accuracy: 0.0689\n",
      "iteration number: 614\t training loss: 1280.9912\tvalidation loss: 378.6100\t validation accuracy: 0.0689\n",
      "iteration number: 615\t training loss: 1280.9755\tvalidation loss: 378.6048\t validation accuracy: 0.0689\n",
      "iteration number: 616\t training loss: 1280.9651\tvalidation loss: 378.6014\t validation accuracy: 0.0689\n",
      "iteration number: 617\t training loss: 1280.9655\tvalidation loss: 378.6015\t validation accuracy: 0.0689\n",
      "iteration number: 618\t training loss: 1280.9828\tvalidation loss: 378.6073\t validation accuracy: 0.0689\n",
      "iteration number: 619\t training loss: 1280.9755\tvalidation loss: 378.6048\t validation accuracy: 0.0689\n",
      "iteration number: 620\t training loss: 1280.9906\tvalidation loss: 378.6098\t validation accuracy: 0.0689\n",
      "iteration number: 621\t training loss: 1280.9833\tvalidation loss: 378.6074\t validation accuracy: 0.0689\n",
      "iteration number: 622\t training loss: 1280.9807\tvalidation loss: 378.6065\t validation accuracy: 0.0689\n",
      "iteration number: 623\t training loss: 1280.9753\tvalidation loss: 378.6047\t validation accuracy: 0.0689\n",
      "iteration number: 624\t training loss: 1280.9804\tvalidation loss: 378.6064\t validation accuracy: 0.0689\n",
      "iteration number: 625\t training loss: 1280.9753\tvalidation loss: 378.6047\t validation accuracy: 0.0689\n",
      "iteration number: 626\t training loss: 1280.9832\tvalidation loss: 378.6073\t validation accuracy: 0.0689\n",
      "iteration number: 627\t training loss: 1280.9871\tvalidation loss: 378.6086\t validation accuracy: 0.0689\n",
      "iteration number: 628\t training loss: 1280.9742\tvalidation loss: 378.6043\t validation accuracy: 0.0689\n",
      "iteration number: 629\t training loss: 1280.9787\tvalidation loss: 378.6058\t validation accuracy: 0.0689\n",
      "iteration number: 630\t training loss: 1280.9701\tvalidation loss: 378.6030\t validation accuracy: 0.0689\n",
      "iteration number: 631\t training loss: 1280.9633\tvalidation loss: 378.6007\t validation accuracy: 0.0689\n",
      "iteration number: 632\t training loss: 1280.9658\tvalidation loss: 378.6015\t validation accuracy: 0.0689\n",
      "iteration number: 633\t training loss: 1280.9677\tvalidation loss: 378.6022\t validation accuracy: 0.0689\n",
      "iteration number: 634\t training loss: 1280.9555\tvalidation loss: 378.5981\t validation accuracy: 0.0689\n",
      "iteration number: 635\t training loss: 1280.9500\tvalidation loss: 378.5963\t validation accuracy: 0.0689\n",
      "iteration number: 636\t training loss: 1280.9591\tvalidation loss: 378.5993\t validation accuracy: 0.0689\n",
      "iteration number: 637\t training loss: 1280.9528\tvalidation loss: 378.5972\t validation accuracy: 0.0689\n",
      "iteration number: 638\t training loss: 1280.9497\tvalidation loss: 378.5962\t validation accuracy: 0.0689\n",
      "iteration number: 639\t training loss: 1280.9653\tvalidation loss: 378.6013\t validation accuracy: 0.0689\n",
      "iteration number: 640\t training loss: 1280.9410\tvalidation loss: 378.5933\t validation accuracy: 0.0689\n",
      "iteration number: 641\t training loss: 1280.9413\tvalidation loss: 378.5933\t validation accuracy: 0.0689\n",
      "iteration number: 642\t training loss: 1280.9523\tvalidation loss: 378.5970\t validation accuracy: 0.0689\n",
      "iteration number: 643\t training loss: 1280.9382\tvalidation loss: 378.5923\t validation accuracy: 0.0689\n",
      "iteration number: 644\t training loss: 1280.9339\tvalidation loss: 378.5909\t validation accuracy: 0.0689\n"
     ]
    }
   ],
   "source": [
    "mlp = MultiLayerPerceptron(X, Y, hidden_size=50, activation='relu')\n",
    "mlp.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Questions:\n",
    "#### - Did you succeed to train the MLP and get a high validation accuracy? <br> Display available metrics (training and validation accuracies, training and validation losses)\n",
    "#### - Plot the prediction for a given validation sample. Is it accurate?\n",
    "#### - Compare the full gradient descent with the SGD.\n",
    "#### - Play with the hyper parameters you have: the hidden size, the activation function, the initial step and the batch size. <br> Comment. Don't hesitate to visualize results.\n",
    "#### - Once properly implemented, compare the training using early stopping, dropout, or both of them. <br> Why are these methods useful here?\n",
    "#### - Once properly implemented, compare the training using momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.plot_validation_prediction(sample_id=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.plot_loss_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III - Multiclass classification MLP with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Implement the same network architecture with Keras;\n",
    "    - First using the Sequential API\n",
    "    - Secondly using the functional API\n",
    "\n",
    "#### - Check that the Keras model can approximately reproduce the behavior of the Numpy model.\n",
    "\n",
    "#### - Compute the negative log likelihood of a sample 42 in the test set (can use `model.predict_proba`).\n",
    "\n",
    "#### - Compute the average negative log-likelihood on the full test set.\n",
    "\n",
    "#### - Compute the average negative log-likelihood  on the full training set and check that you can get the value of the loss reported by Keras.\n",
    "\n",
    "#### - Is the model overfitting or underfitting? (ensure that the model has fully converged by increasing the number of epochs to 500 or more if necessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_val, Y_tr, Y_val = train_test_split(X, Y)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1), copy=False)\n",
    "X_tr = scaler.fit_transform(X_tr)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X[0].shape[0]\n",
    "n_classes = len(np.unique(Y_tr))\n",
    "n_hidden = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "activation = \"relu\"\n",
    "# activation = \"sigmoid\"\n",
    "\n",
    "print('Model with {} activation'.format(activation))\n",
    "keras_model = Sequential()\n",
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "activation = \"relu\"\n",
    "# activation = \"sigmoid\"\n",
    "\n",
    "print('Model with {} activation'.format(activation))\n",
    "\n",
    "inputs = Input(shape=(n_features,))\n",
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that you know if the model is underfitting or overfitting:\n",
    "#### - In case of underfitting, can you explain why ? Also change the structure of the 2 previous networks to cancell underfitting\n",
    "#### - In case of overfitting, explain why and change the structure of the 2 previous networks to cancell the overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
